<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/66449ed0e802001c.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-1e56b3c3206210e2.js"/><script src="/_next/static/chunks/ec7a6716-6cab3e3f753fb035.js" async=""></script><script src="/_next/static/chunks/5774-3c92e2fd4a5f0665.js" async=""></script><script src="/_next/static/chunks/main-app-669fa76d353b467e.js" async=""></script><script src="/_next/static/chunks/f6ab4fea-4425dae3da7674a3.js" async=""></script><script src="/_next/static/chunks/b27dc69b-e5acb70cd9d66ddf.js" async=""></script><script src="/_next/static/chunks/f3ac7ef3-57e0d20b76b6c0bf.js" async=""></script><script src="/_next/static/chunks/5232-db5923cda78a4960.js" async=""></script><script src="/_next/static/chunks/6465-524ff7a6f6bb774c.js" async=""></script><script src="/_next/static/chunks/9388-d1a745fd53a10850.js" async=""></script><script src="/_next/static/chunks/1508-abbc58115cbb4fd0.js" async=""></script><script src="/_next/static/chunks/236-f8f5e32bce4e096f.js" async=""></script><script src="/_next/static/chunks/7101-9ab34b82066b9cd8.js" async=""></script><script src="/_next/static/chunks/6125-6f7cbc86df4f2503.js" async=""></script><script src="/_next/static/chunks/2919-7652e8907f6a6ad4.js" async=""></script><script src="/_next/static/chunks/7640-74d36ce7728a4200.js" async=""></script><script src="/_next/static/chunks/4091-69e125b8fd863f9f.js" async=""></script><script src="/_next/static/chunks/976-8676cce570a8fe93.js" async=""></script><script src="/_next/static/chunks/4942-51fd33fb9762f202.js" async=""></script><script src="/_next/static/chunks/app/%5Blocale%5D/(home)/layout-e297fbc0fe7ac09d.js" async=""></script><script src="/_next/static/chunks/1866796a-670a712d9364e62c.js" async=""></script><script src="/_next/static/chunks/6541-2f0250d6a0967fe4.js" async=""></script><script src="/_next/static/chunks/app/%5Blocale%5D/(default)/layout-681f37f67bcbff3e.js" async=""></script><script src="/_next/static/chunks/9732-8158987224fdfa35.js" async=""></script><script src="/_next/static/chunks/1358-a345831bbdffdf07.js" async=""></script><script src="/_next/static/chunks/app/%5Blocale%5D/(default)/clients/page-5664c6f855076eab.js" async=""></script><script src="/_next/static/chunks/4b7059ae-76f7529d720584f8.js" async=""></script><script src="/_next/static/chunks/4902-f1919cf5204afe9f.js" async=""></script><script src="/_next/static/chunks/2407-4b5edf4e84c5d717.js" async=""></script><script src="/_next/static/chunks/app/%5Blocale%5D/layout-2c849eca4082e774.js" async=""></script><script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2123767634383915" crossorigin="anonymous"></script><link rel="preload" href="https://www.googletagmanager.com/gtag/js?id=G-9ZWF7FKDR8" as="script"/><link rel="icon" href="/favicon.ico"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"/><link rel="alternate" hrefLang="en" href="https://mcp.so"/><link rel="alternate" hrefLang="zh" href="https://mcp.so/zh/"/><link rel="alternate" hrefLang="ja" href="https://mcp.so/ja/"/><link rel="alternate" hrefLang="x-default" href="https://mcp.so"/><meta name="google-adsense-account" content="ca-pub-2123767634383915"/><title>MCP Clients</title><meta name="description" content="Find the best MCP Clients for your needs."/><meta name="keywords" content="MCP Servers, Awesome MCP Servers, Claude MCP, Model Context Protocol"/><link rel="canonical" href="https://mcp.so/clients"/><meta property="og:title" content="MCP Servers"/><meta property="og:description" content="The largest collection of MCP Servers, including Awesome MCP Servers and Claude MCP integration. Search and discover MCP servers to enhance your AI capabilities."/><meta property="og:url" content="https://mcp.so"/><meta property="og:site_name" content="MCP.so"/><meta property="og:locale" content="en"/><meta property="og:image" content="https://mcp.so/logo.png"/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="630"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary"/><meta name="twitter:site" content="MCP.so"/><meta name="twitter:title" content="MCP Servers"/><meta name="twitter:description" content="The largest collection of MCP Servers, including Awesome MCP Servers and Claude MCP integration. Search and discover MCP servers to enhance your AI capabilities."/><meta name="twitter:image" content="https://mcp.so/logo.png"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="min-h-screen bg-background antialiased overflow-x-hidden"><script>((e2, t2, r5, n2, a3, o4, i3, s3) => {
            let l4 = document.documentElement, u3 = ["light", "dark"];
            function c2(t3) {
              var r6;
              (Array.isArray(e2) ? e2 : [e2]).forEach((e3) => {
                let r7 = "class" === e3, n3 = r7 && o4 ? a3.map((e4) => o4[e4] || e4) : a3;
                r7 ? (l4.classList.remove(...n3), l4.classList.add(t3)) : l4.setAttribute(e3, t3);
              }), r6 = t3, s3 && u3.includes(r6) && (l4.style.colorScheme = r6);
            }
            __name(c2, "c2");
            if (n2) c2(n2);
            else try {
              let e3 = localStorage.getItem(t2) || r5, n3 = i3 && "system" === e3 ? window.matchMedia("(prefers-color-scheme: dark)").matches ? "dark" : "light" : e3;
              c2(n3);
            } catch (e3) {
            }
          })("class","theme","system","light",["light","dark"],null,true,true)</script><div style="--sidebar-width:16rem;--sidebar-width-icon:3rem" class="group/sidebar-wrapper flex min-h-svh w-full has-data-[variant=inset]:bg-sidebar"><div class="group peer hidden text-sidebar-foreground md:block" data-state="expanded" data-collapsible="" data-variant="floating" data-side="left"><div class="relative w-(--sidebar-width) bg-transparent transition-[width] duration-200 ease-linear group-data-[collapsible=offcanvas]:w-0 group-data-[side=right]:rotate-180 group-data-[collapsible=icon]:w-[calc(var(--sidebar-width-icon)+(--spacing(4)))]"></div><div class="fixed inset-y-0 z-10 hidden h-svh w-(--sidebar-width) transition-[left,right,width] duration-200 ease-linear md:flex left-0 group-data-[collapsible=offcanvas]:left-[calc(var(--sidebar-width)*-1)] p-2 group-data-[collapsible=icon]:w-[calc(var(--sidebar-width-icon)+(--spacing(4))+2px)]"><div data-sidebar="sidebar" class="flex h-full w-full flex-col bg-sidebar group-data-[variant=floating]:rounded-lg group-data-[variant=floating]:border group-data-[variant=floating]:border-sidebar-border group-data-[variant=floating]:shadow-sm"><div data-sidebar="header" class="flex flex-col gap-2 p-2"><ul data-sidebar="menu" class="flex w-full min-w-0 flex-col gap-1"><li data-sidebar="menu-item" class="group/menu-item relative"><a class="flex items-center gap-2" href="/"><img alt="MCP.so" loading="lazy" width="28" height="28" decoding="async" data-nimg="1" class="rounded-md w-8 h-8" style="color:transparent" srcSet="/_next/image?url=%2Flogo.png&amp;w=32&amp;q=75 1x, /_next/image?url=%2Flogo.png&amp;w=64&amp;q=75 2x" src="/_next/image?url=%2Flogo.png&amp;w=64&amp;q=75"/><span class="text-base font-semibold flex-1">MCP.so</span></a></li></ul></div><div data-sidebar="content" class="flex min-h-0 flex-1 flex-col gap-2 overflow-auto group-data-[collapsible=icon]:overflow-hidden"><div data-sidebar="group" class="relative flex w-full min-w-0 flex-col p-2"><div data-sidebar="group-content" class="w-full text-sm flex flex-col gap-2 mt-4"><ul data-sidebar="menu" class="flex w-full min-w-0 flex-col gap-1"><li data-sidebar="menu-item" class="group/menu-item relative group/collapsible" data-state="closed"><button data-sidebar="menu-button" data-size="default" data-active="false" class="peer/menu-button flex w-full items-center gap-2 overflow-hidden rounded-md p-2 text-left outline-hidden ring-sidebar-ring transition-[width,height,padding] focus-visible:ring-2 active:bg-sidebar-accent active:text-sidebar-accent-foreground disabled:pointer-events-none disabled:opacity-50 group-has-data-[sidebar=menu-action]/menu-item:pr-8 aria-disabled:pointer-events-none aria-disabled:opacity-50 data-[active=true]:bg-sidebar-accent data-[active=true]:font-medium data-[active=true]:text-sidebar-accent-foreground data-[state=open]:hover:bg-sidebar-accent data-[state=open]:hover:text-sidebar-accent-foreground group-data-[collapsible=icon]:size-8! group-data-[collapsible=icon]:p-2! [&amp;&gt;span:last-child]:truncate [&amp;&gt;svg]:size-4 [&amp;&gt;svg]:shrink-0 hover:bg-sidebar-accent hover:text-sidebar-accent-foreground h-8 text-sm" type="button" aria-controls="radix-«Rmjcqtb»" aria-expanded="false" data-state="closed"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" style="cursor:pointer" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M21 20C21 20.5523 20.5523 21 20 21H4C3.44772 21 3 20.5523 3 20V9.48907C3 9.18048 3.14247 8.88917 3.38606 8.69972L11.3861 2.47749C11.7472 2.19663 12.2528 2.19663 12.6139 2.47749L20.6139 8.69972C20.8575 8.88917 21 9.18048 21 9.48907V20ZM19 19V9.97815L12 4.53371L5 9.97815V19H19Z"></path></svg><span>Home</span></button></li><li data-sidebar="menu-item" class="group/menu-item relative group/collapsible" data-state="closed"><button data-sidebar="menu-button" data-size="default" data-active="false" class="peer/menu-button flex w-full items-center gap-2 overflow-hidden rounded-md p-2 text-left outline-hidden ring-sidebar-ring transition-[width,height,padding] focus-visible:ring-2 active:bg-sidebar-accent active:text-sidebar-accent-foreground disabled:pointer-events-none disabled:opacity-50 group-has-data-[sidebar=menu-action]/menu-item:pr-8 aria-disabled:pointer-events-none aria-disabled:opacity-50 data-[active=true]:bg-sidebar-accent data-[active=true]:font-medium data-[active=true]:text-sidebar-accent-foreground data-[state=open]:hover:bg-sidebar-accent data-[state=open]:hover:text-sidebar-accent-foreground group-data-[collapsible=icon]:size-8! group-data-[collapsible=icon]:p-2! [&amp;&gt;span:last-child]:truncate [&amp;&gt;svg]:size-4 [&amp;&gt;svg]:shrink-0 hover:bg-sidebar-accent hover:text-sidebar-accent-foreground h-8 text-sm" type="button" aria-controls="radix-«R16jcqtb»" aria-expanded="false" data-state="closed"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" style="cursor:pointer" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M5 11H19V5H5V11ZM21 4V20C21 20.5523 20.5523 21 20 21H4C3.44772 21 3 20.5523 3 20V4C3 3.44772 3.44772 3 4 3H20C20.5523 3 21 3.44772 21 4ZM19 13H5V19H19V13ZM7 15H10V17H7V15ZM7 7H10V9H7V7Z"></path></svg><span>Servers</span></button></li><li data-sidebar="menu-item" class="group/menu-item relative group/collapsible" data-state="closed"><button data-sidebar="menu-button" data-size="default" data-active="false" class="peer/menu-button flex w-full items-center gap-2 overflow-hidden rounded-md p-2 text-left outline-hidden ring-sidebar-ring transition-[width,height,padding] focus-visible:ring-2 disabled:pointer-events-none disabled:opacity-50 group-has-data-[sidebar=menu-action]/menu-item:pr-8 aria-disabled:pointer-events-none aria-disabled:opacity-50 data-[active=true]:bg-sidebar-accent data-[active=true]:font-medium data-[active=true]:text-sidebar-accent-foreground data-[state=open]:hover:bg-sidebar-accent data-[state=open]:hover:text-sidebar-accent-foreground group-data-[collapsible=icon]:size-8! group-data-[collapsible=icon]:p-2! [&amp;&gt;span:last-child]:truncate [&amp;&gt;svg]:size-4 [&amp;&gt;svg]:shrink-0 h-8 text-sm bg-sidebar-accent text-primary hover:bg-sidebar-accent/90 hover:text-primary active:bg-sidebar-accent/90 active:text-primary min-w-8 duration-200 ease-linear" type="button" aria-controls="radix-«R1mjcqtb»" aria-expanded="false" data-state="closed"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" style="cursor:pointer" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M6.75 2.5C9.09721 2.5 11 4.40279 11 6.75V11H6.75C4.40279 11 2.5 9.09721 2.5 6.75C2.5 4.40279 4.40279 2.5 6.75 2.5ZM9 9V6.75C9 5.50736 7.99264 4.5 6.75 4.5C5.50736 4.5 4.5 5.50736 4.5 6.75C4.5 7.99264 5.50736 9 6.75 9H9ZM6.75 13H11V17.25C11 19.5972 9.09721 21.5 6.75 21.5C4.40279 21.5 2.5 19.5972 2.5 17.25C2.5 14.9028 4.40279 13 6.75 13ZM6.75 15C5.50736 15 4.5 16.0074 4.5 17.25C4.5 18.4926 5.50736 19.5 6.75 19.5C7.99264 19.5 9 18.4926 9 17.25V15H6.75ZM17.25 2.5C19.5972 2.5 21.5 4.40279 21.5 6.75C21.5 9.09721 19.5972 11 17.25 11H13V6.75C13 4.40279 14.9028 2.5 17.25 2.5ZM17.25 9C18.4926 9 19.5 7.99264 19.5 6.75C19.5 5.50736 18.4926 4.5 17.25 4.5C16.0074 4.5 15 5.50736 15 6.75V9H17.25ZM13 13H17.25C19.5972 13 21.5 14.9028 21.5 17.25C21.5 19.5972 19.5972 21.5 17.25 21.5C14.9028 21.5 13 19.5972 13 17.25V13ZM15 15V17.25C15 18.4926 16.0074 19.5 17.25 19.5C18.4926 19.5 19.5 18.4926 19.5 17.25C19.5 16.0074 18.4926 15 17.25 15H15Z"></path></svg><span>Clients</span></button></li><li data-sidebar="menu-item" class="group/menu-item relative group/collapsible" data-state="closed"><button data-sidebar="menu-button" data-size="default" data-active="false" class="peer/menu-button flex w-full items-center gap-2 overflow-hidden rounded-md p-2 text-left outline-hidden ring-sidebar-ring transition-[width,height,padding] focus-visible:ring-2 active:bg-sidebar-accent active:text-sidebar-accent-foreground disabled:pointer-events-none disabled:opacity-50 group-has-data-[sidebar=menu-action]/menu-item:pr-8 aria-disabled:pointer-events-none aria-disabled:opacity-50 data-[active=true]:bg-sidebar-accent data-[active=true]:font-medium data-[active=true]:text-sidebar-accent-foreground data-[state=open]:hover:bg-sidebar-accent data-[state=open]:hover:text-sidebar-accent-foreground group-data-[collapsible=icon]:size-8! group-data-[collapsible=icon]:p-2! [&amp;&gt;span:last-child]:truncate [&amp;&gt;svg]:size-4 [&amp;&gt;svg]:shrink-0 hover:bg-sidebar-accent hover:text-sidebar-accent-foreground h-8 text-sm" type="button" aria-controls="radix-«R26jcqtb»" aria-expanded="false" data-state="closed"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" style="cursor:pointer" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M3 4H21V6H3V4ZM3 11H15V13H3V11ZM3 18H21V20H3V18Z"></path></svg><span>Categories</span></button></li><li data-sidebar="menu-item" class="group/menu-item relative group/collapsible" data-state="closed"><button data-sidebar="menu-button" data-size="default" data-active="false" class="peer/menu-button flex w-full items-center gap-2 overflow-hidden rounded-md p-2 text-left outline-hidden ring-sidebar-ring transition-[width,height,padding] focus-visible:ring-2 active:bg-sidebar-accent active:text-sidebar-accent-foreground disabled:pointer-events-none disabled:opacity-50 group-has-data-[sidebar=menu-action]/menu-item:pr-8 aria-disabled:pointer-events-none aria-disabled:opacity-50 data-[active=true]:bg-sidebar-accent data-[active=true]:font-medium data-[active=true]:text-sidebar-accent-foreground data-[state=open]:hover:bg-sidebar-accent data-[state=open]:hover:text-sidebar-accent-foreground group-data-[collapsible=icon]:size-8! group-data-[collapsible=icon]:p-2! [&amp;&gt;span:last-child]:truncate [&amp;&gt;svg]:size-4 [&amp;&gt;svg]:shrink-0 hover:bg-sidebar-accent hover:text-sidebar-accent-foreground h-8 text-sm" type="button" aria-controls="radix-«R2mjcqtb»" aria-expanded="false" data-state="closed"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" style="cursor:pointer" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M7.78428 14L8.2047 10H4V8H8.41491L8.94043 3H10.9514L10.4259 8H14.4149L14.9404 3H16.9514L16.4259 8H20V10H16.2157L15.7953 14H20V16H15.5851L15.0596 21H13.0486L13.5741 16H9.58509L9.05957 21H7.04855L7.57407 16H4V14H7.78428ZM9.7953 14H13.7843L14.2047 10H10.2157L9.7953 14Z"></path></svg><span>Tags</span></button></li><li data-sidebar="menu-item" class="group/menu-item relative group/collapsible" data-state="closed"><button data-sidebar="menu-button" data-size="default" data-active="false" class="peer/menu-button flex w-full items-center gap-2 overflow-hidden rounded-md p-2 text-left outline-hidden ring-sidebar-ring transition-[width,height,padding] focus-visible:ring-2 active:bg-sidebar-accent active:text-sidebar-accent-foreground disabled:pointer-events-none disabled:opacity-50 group-has-data-[sidebar=menu-action]/menu-item:pr-8 aria-disabled:pointer-events-none aria-disabled:opacity-50 data-[active=true]:bg-sidebar-accent data-[active=true]:font-medium data-[active=true]:text-sidebar-accent-foreground data-[state=open]:hover:bg-sidebar-accent data-[state=open]:hover:text-sidebar-accent-foreground group-data-[collapsible=icon]:size-8! group-data-[collapsible=icon]:p-2! [&amp;&gt;span:last-child]:truncate [&amp;&gt;svg]:size-4 [&amp;&gt;svg]:shrink-0 hover:bg-sidebar-accent hover:text-sidebar-accent-foreground h-8 text-sm" type="button" aria-controls="radix-«R36jcqtb»" aria-expanded="false" data-state="closed"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" style="cursor:pointer" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M10.0014 14.6757C10.0011 14.6551 10.001 14.6345 10.001 14.6138C10.001 12.1055 12.0175 9.99564 14.7539 9.38092C14.3904 7.07873 11.9602 5.19995 8.90098 5.19995C5.58037 5.19995 3.00098 7.41344 3.00098 9.9793C3.00098 10.9487 3.36131 11.88 4.04082 12.6781C4.0728 12.7157 4.12443 12.7717 4.19342 12.8427C4.78537 13.4517 5.13709 14.2457 5.19546 15.0805C5.90857 14.6683 6.74285 14.5123 7.55832 14.6392C7.72416 14.665 7.85986 14.6847 7.96345 14.6982C8.27111 14.7383 8.58419 14.7586 8.90098 14.7586C9.27825 14.7586 9.64595 14.7301 10.0014 14.6757ZM10.4581 16.627C9.95467 16.7133 9.43399 16.7586 8.90098 16.7586C8.49441 16.7586 8.09502 16.7323 7.70499 16.6815C7.58312 16.6656 7.4317 16.6436 7.25073 16.6154C6.87693 16.5572 6.49436 16.6321 6.1713 16.8268L4.26653 17.9745C4.12052 18.0646 3.94891 18.1057 3.77733 18.0916C3.33814 18.0554 3.01178 17.6744 3.04837 17.2405L3.19859 15.4596C3.23664 15.0086 3.07664 14.5632 2.75931 14.2367C2.66182 14.1364 2.5814 14.0491 2.51802 13.9747C1.56406 12.8542 1.00098 11.4732 1.00098 9.9793C1.00098 6.23517 4.53793 3.19995 8.90098 3.19995C12.9601 3.19995 16.3041 5.82699 16.7504 9.20788C20.1225 9.36136 22.801 11.723 22.801 14.6138C22.801 15.8068 22.3448 16.9097 21.572 17.8044C21.5206 17.8639 21.4555 17.9336 21.3765 18.0137C21.1194 18.2744 20.9898 18.6301 21.0206 18.9903L21.1423 20.4125C21.172 20.759 20.9076 21.0632 20.5518 21.0921C20.4128 21.1034 20.2738 21.0706 20.1555 20.9986L18.6124 20.0821C18.3506 19.9266 18.0407 19.8668 17.7379 19.9133C17.5913 19.9358 17.4686 19.9533 17.3699 19.966C17.0539 20.0066 16.7303 20.0277 16.401 20.0277C13.7074 20.0277 11.4025 18.6201 10.4581 16.627ZM17.4346 17.9364C18.0019 17.8494 18.5793 17.911 19.1105 18.1111C19.2492 17.5503 19.5373 17.0304 19.9524 16.6094C20.0027 16.5585 20.0388 16.5198 20.0584 16.4971C20.5467 15.9318 20.801 15.2839 20.801 14.6138C20.801 12.8095 18.8983 11.2 16.401 11.2C13.9037 11.2 12.001 12.8095 12.001 14.6138C12.001 16.4181 13.9037 18.0277 16.401 18.0277C16.6424 18.0277 16.8809 18.0124 17.115 17.9823C17.1957 17.972 17.3029 17.9566 17.4346 17.9364Z"></path></svg><span>Feed</span></button></li></ul></div></div><div data-sidebar="group" class="relative flex w-full min-w-0 flex-col p-2 mt-auto"><div data-sidebar="group-content" class="w-full text-sm"><ul data-sidebar="menu" class="flex w-full min-w-0 flex-col gap-1"><li data-sidebar="menu-item" class="group/menu-item relative"><a target="_self" data-sidebar="menu-button" data-size="default" data-active="false" class="peer/menu-button flex w-full items-center gap-2 overflow-hidden rounded-md p-2 text-left outline-hidden ring-sidebar-ring transition-[width,height,padding] focus-visible:ring-2 active:bg-sidebar-accent active:text-sidebar-accent-foreground disabled:pointer-events-none disabled:opacity-50 group-has-data-[sidebar=menu-action]/menu-item:pr-8 aria-disabled:pointer-events-none aria-disabled:opacity-50 data-[active=true]:bg-sidebar-accent data-[active=true]:font-medium data-[active=true]:text-sidebar-accent-foreground data-[state=open]:hover:bg-sidebar-accent data-[state=open]:hover:text-sidebar-accent-foreground group-data-[collapsible=icon]:size-8! group-data-[collapsible=icon]:p-2! [&amp;&gt;span:last-child]:truncate [&amp;&gt;svg]:size-4 [&amp;&gt;svg]:shrink-0 hover:bg-sidebar-accent hover:text-sidebar-accent-foreground h-8 text-sm" href="/my-servers"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" style="cursor:pointer" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M12 1L21.5 6.5V17.5L12 23L2.5 17.5V6.5L12 1ZM12 3.311L4.5 7.65311V16.3469L12 20.689L19.5 16.3469V7.65311L12 3.311ZM12 16C9.79086 16 8 14.2091 8 12C8 9.79086 9.79086 8 12 8C14.2091 8 16 9.79086 16 12C16 14.2091 14.2091 16 12 16ZM12 14C13.1046 14 14 13.1046 14 12C14 10.8954 13.1046 10 12 10C10.8954 10 10 10.8954 10 12C10 13.1046 10.8954 14 12 14Z"></path></svg><span>Settings</span></a></li></ul></div></div></div><div data-sidebar="footer" class="flex flex-col gap-2"><div class="flex justify-center items-center h-full px-4 py-4"><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-hidden focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 bg-primary text-primary-foreground hover:bg-primary/90 h-10 px-4 py-2 w-full">Sign In</button></div><div class="w-full flex items-center justify-center mx-auto gap-x-4 px-0 py-4 border-t"><div class="cursor-pointer hover:text-primary"><a target="_blank" class="cursor-pointer" href="https://x.com/chatmcp"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" class="text-xl" style="cursor:pointer" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M17.6874 3.0625L12.6907 8.77425L8.37045 3.0625H2.11328L9.58961 12.8387L2.50378 20.9375H5.53795L11.0068 14.6886L15.7863 20.9375H21.8885L14.095 10.6342L20.7198 3.0625H17.6874ZM16.6232 19.1225L5.65436 4.78217H7.45745L18.3034 19.1225H16.6232Z"></path></svg></a></div><div class="cursor-pointer hover:text-primary"><a target="_blank" class="cursor-pointer" href="https://github.com/chatmcp/mcpso/issues"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" class="text-xl" style="cursor:pointer" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M12.001 2C6.47598 2 2.00098 6.475 2.00098 12C2.00098 16.425 4.86348 20.1625 8.83848 21.4875C9.33848 21.575 9.52598 21.275 9.52598 21.0125C9.52598 20.775 9.51348 19.9875 9.51348 19.15C7.00098 19.6125 6.35098 18.5375 6.15098 17.975C6.03848 17.6875 5.55098 16.8 5.12598 16.5625C4.77598 16.375 4.27598 15.9125 5.11348 15.9C5.90098 15.8875 6.46348 16.625 6.65098 16.925C7.55098 18.4375 8.98848 18.0125 9.56348 17.75C9.65098 17.1 9.91348 16.6625 10.201 16.4125C7.97598 16.1625 5.65098 15.3 5.65098 11.475C5.65098 10.3875 6.03848 9.4875 6.67598 8.7875C6.57598 8.5375 6.22598 7.5125 6.77598 6.1375C6.77598 6.1375 7.61348 5.875 9.52598 7.1625C10.326 6.9375 11.176 6.825 12.026 6.825C12.876 6.825 13.726 6.9375 14.526 7.1625C16.4385 5.8625 17.276 6.1375 17.276 6.1375C17.826 7.5125 17.476 8.5375 17.376 8.7875C18.0135 9.4875 18.401 10.375 18.401 11.475C18.401 15.3125 16.0635 16.1625 13.8385 16.4125C14.201 16.725 14.5135 17.325 14.5135 18.2625C14.5135 19.6 14.501 20.675 14.501 21.0125C14.501 21.275 14.6885 21.5875 15.1885 21.4875C19.259 20.1133 21.9999 16.2963 22.001 12C22.001 6.475 17.526 2 12.001 2Z"></path></svg></a></div><div class="cursor-pointer hover:text-primary"><a target="_blank" class="cursor-pointer" href="https://discord.gg/RsYPRrnyqg"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" class="text-xl" style="cursor:pointer" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M19.3034 5.33716C17.9344 4.71103 16.4805 4.2547 14.9629 4C14.7719 4.32899 14.5596 4.77471 14.411 5.12492C12.7969 4.89144 11.1944 4.89144 9.60255 5.12492C9.45397 4.77471 9.2311 4.32899 9.05068 4C7.52251 4.2547 6.06861 4.71103 4.70915 5.33716C1.96053 9.39111 1.21766 13.3495 1.5891 17.2549C3.41443 18.5815 5.17612 19.388 6.90701 19.9187C7.33151 19.3456 7.71356 18.73 8.04255 18.0827C7.41641 17.8492 6.82211 17.5627 6.24904 17.2231C6.39762 17.117 6.5462 17.0003 6.68416 16.8835C10.1438 18.4648 13.8911 18.4648 17.3082 16.8835C17.4568 17.0003 17.5948 17.117 17.7434 17.2231C17.1703 17.5627 16.576 17.8492 15.9499 18.0827C16.2789 18.73 16.6609 19.3456 17.0854 19.9187C18.8152 19.388 20.5875 18.5815 22.4033 17.2549C22.8596 12.7341 21.6806 8.80747 19.3034 5.33716ZM8.5201 14.8459C7.48007 14.8459 6.63107 13.9014 6.63107 12.7447C6.63107 11.5879 7.45884 10.6434 8.5201 10.6434C9.57071 10.6434 10.4303 11.5879 10.4091 12.7447C10.4091 13.9014 9.57071 14.8459 8.5201 14.8459ZM15.4936 14.8459C14.4535 14.8459 13.6034 13.9014 13.6034 12.7447C13.6034 11.5879 14.4323 10.6434 15.4936 10.6434C16.5442 10.6434 17.4038 11.5879 17.3825 12.7447C17.3825 13.9014 16.5548 14.8459 15.4936 14.8459Z"></path></svg></a></div><div class="cursor-pointer hover:text-primary"><a target="_self" class="cursor-pointer" href="mailto:support@mcp.so"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" class="text-xl" style="cursor:pointer" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M3 3H21C21.5523 3 22 3.44772 22 4V20C22 20.5523 21.5523 21 21 21H3C2.44772 21 2 20.5523 2 20V4C2 3.44772 2.44772 3 3 3ZM20 7.23792L12.0718 14.338L4 7.21594V19H20V7.23792ZM4.51146 5L12.0619 11.662L19.501 5H4.51146Z"></path></svg></a></div><div data-orientation="vertical" role="none" class="shrink-0 bg-border w-px h-4"></div><div class="flex items-center gap-x-2 px-2"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" class="cursor-pointer text-lg text-muted-foreground" width="1em" height="1em" xmlns="http://www.w3.org/2000/svg"><path d="M6 .278a.77.77 0 0 1 .08.858 7.2 7.2 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277q.792-.001 1.533-.16a.79.79 0 0 1 .81.316.73.73 0 0 1-.031.893A8.35 8.35 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.75.75 0 0 1 6 .278M4.858 1.311A7.27 7.27 0 0 0 1.025 7.71c0 4.02 3.279 7.276 7.319 7.276a7.32 7.32 0 0 0 5.205-2.162q-.506.063-1.029.063c-4.61 0-8.343-3.714-8.343-8.29 0-1.167.242-2.278.681-3.286"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.73 1.73 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.73 1.73 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.73 1.73 0 0 0 1.097-1.097zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.16 1.16 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.16 1.16 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732z"></path></svg></div></div></div></div></div></div><main class="relative flex w-full flex-1 flex-col bg-background md:peer-data-[variant=inset]:m-2 md:peer-data-[variant=inset]:peer-data-[state=collapsed]:ml-2 md:peer-data-[variant=inset]:ml-0 md:peer-data-[variant=inset]:rounded-xl md:peer-data-[variant=inset]:shadow-sm"><header class="flex py-2 shrink-0 items-center gap-2 transition-[width,height] ease-linear group-has-data-[collapsible=icon]/sidebar-wrapper:h-12"><div class="w-full flex items-center gap-4 px-4"><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-hidden focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 hover:bg-accent hover:text-accent-foreground h-7 w-7 -ml-1 cursor-pointer" data-sidebar="trigger"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-panel-left"><rect width="18" height="18" x="3" y="3" rx="2"></rect><path d="M9 3v18"></path></svg><span class="sr-only">Toggle Sidebar</span></button><div class="flex items-center gap-4"></div><div class="flex-1"></div><div class="flex gap-4"><a class="flex items-center gap-1 text-sm font-medium text-primary rounded-full px-4 hover:bg-primary/10" href="/submit"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" class="w-4 h-4 shrink-0" style="cursor:pointer" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M11 11V5H13V11H19V13H13V19H11V13H5V11H11Z"></path></svg>Submit</a><div><button type="button" role="combobox" aria-controls="radix-«R1alcqtb»" aria-expanded="false" aria-autocomplete="none" dir="ltr" data-state="closed" class="h-10 w-full justify-between rounded-md border border-input bg-background px-3 py-2 text-sm ring-offset-background placeholder:text-muted-foreground focus:outline-hidden focus:ring-ring disabled:cursor-not-allowed disabled:opacity-50 [&amp;&gt;span]:line-clamp-1 flex items-center gap-x-2 border-none text-muted-foreground outline-hidden hover:bg-transparent focus:ring-0 focus:ring-offset-0"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" class="text-xl" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path fill="none" d="M0 0h24v24H0z"></path><path d="M11.99 2C6.47 2 2 6.48 2 12s4.47 10 9.99 10C17.52 22 22 17.52 22 12S17.52 2 11.99 2zm6.93 6h-2.95a15.65 15.65 0 0 0-1.38-3.56A8.03 8.03 0 0 1 18.92 8zM12 4.04c.83 1.2 1.48 2.53 1.91 3.96h-3.82c.43-1.43 1.08-2.76 1.91-3.96zM4.26 14C4.1 13.36 4 12.69 4 12s.1-1.36.26-2h3.38c-.08.66-.14 1.32-.14 2 0 .68.06 1.34.14 2H4.26zm.82 2h2.95c.32 1.25.78 2.45 1.38 3.56A7.987 7.987 0 0 1 5.08 16zm2.95-8H5.08a7.987 7.987 0 0 1 4.33-3.56A15.65 15.65 0 0 0 8.03 8zM12 19.96c-.83-1.2-1.48-2.53-1.91-3.96h3.82c-.43 1.43-1.08 2.76-1.91 3.96zM14.34 14H9.66c-.09-.66-.16-1.32-.16-2 0-.68.07-1.35.16-2h4.68c.09.65.16 1.32.16 2 0 .68-.07 1.34-.16 2zm.25 5.56c.6-1.11 1.06-2.31 1.38-3.56h2.95a8.03 8.03 0 0 1-4.33 3.56zM16.36 14c.08-.66.14-1.32.14-2 0-.68-.06-1.34-.14-2h3.38c.16.64.26 1.31.26 2s-.1 1.36-.26 2h-3.38z"></path></svg><span class="hidden md:block">English</span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down h-4 w-4 opacity-50" aria-hidden="true"><path d="m6 9 6 6 6-6"></path></svg></button><select aria-hidden="true" tabindex="-1" style="position:absolute;border:0;width:1px;height:1px;padding:0;margin:-1px;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;word-wrap:normal"></select></div></div></div></header><div class="flex-1 px-4 md:px-6 pb-16 pt-4"><div class="mb-4 w-full max-w-2xl overflow-hidden"><h1 class="text-2xl font-bold mb-2">MCP Clients</h1><p class="text-sm text-muted-foreground">A list of MCP Clients.</p><nav class="flex items-center space-x-4 lg:space-x-4 overflow-x-auto py-4 mt-4 w-full max-w-full"><a class="text-xs md:text-sm font-medium px-1 py-1 pb-2 flex items-center gap-1 whitespace-nowrap flex-shrink-0 text-primary border-b-2 border-primary" href="/clients"><h2 class="block">All</h2></a><a class="text-xs md:text-sm font-medium px-1 py-1 pb-2 flex items-center gap-1 whitespace-nowrap flex-shrink-0 text-muted-foreground" href="/clients?tag=featured"><h2 class="block">Featured</h2></a><a class="text-xs md:text-sm font-medium px-1 py-1 pb-2 flex items-center gap-1 whitespace-nowrap flex-shrink-0 text-muted-foreground" href="/clients?tag=latest"><h2 class="block">Latest</h2></a></nav></div><div class="grid gap-4 grid-cols-1 sm:grid-cols-2 lg:grid-cols-3 xl:grid-cols-4"><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/php-mcp-client/SWIS"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">P</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">PHP MCP Client</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">Model Context Protocol client implementation for PHP</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/mistr-agent/itisaevalex"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">M</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">Mistr. Agent</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">A MCP client that enables Mistral AI models to autonomously execute complex tasks across web and local environments through standardized agentic capabilities.</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/install-mcp/Dhravya"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">I</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">Install MCP CLI</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">A simple CLI to install MCP servers into any client</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/fast-agent/evalstate"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">F</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">Fast Agent</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">Define, Prompt and Test MCP enabled Agents and Workflows</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/toolbase/Toolbase-AI"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">T</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">Toolbase</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">A desktop application that adds powerful tools to Claude and AI platforms</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/zola/ibelick"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">Z</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">Zola</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">The open-source interface for AI chat. Self-hostable, developer-first, and model-agnostic.</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/chat-mcp/AI-QL"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">M</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">MCP Chat Desktop App</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">A Desktop Chat App that leverages MCP(Model Context Protocol) to interface with other LLMs.</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/EasyMCP/mshojaei77"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">E</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">EasyMCP</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">A beginner-friendly client for the MCP (Model Context Protocol). Connect to SSE, NPX, and UV servers, and integrate with OpenAI for dynamic tool interactions. Perfect for exploring server connections and chat enhancements.</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/evo-ai/EvolutionAPI"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">E</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">Evo AI - AI Agents Platform</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">Evo AI is an open-source platform for creating and managing AI agents, enabling integration with different AI models and services.</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/mcp_chatbot/keli-wen"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">M</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">MCPChatbot Example</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">A chatbot implementation compatible with MCP (terminal / streamlit supported)</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/ClaudeR/IMNMV"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">C</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">ClaudeR</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">R MCP Integration for Claude</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/prodex-js/tarasyarema"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">P</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">prodex-js</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">The ultimate vide coding MCP!</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/goose/block"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">C</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">codename goose</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">an open source, extensible AI agent that goes beyond code suggestions - install, execute, edit, and test with any LLM</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/mcp-ai-agent/fkesheh"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">M</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">MCP AI Agent</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">A TypeScript library that enables AI agents to leverage MCP (Model Context Protocol) servers for enhanced capabilities. This library integrates with the AI SDK to provide a seamless way to connect to MCP servers and use their tools in AI-powered applications.</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/minions/HazyResearch"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">W</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">Where On-Device and Cloud LLMs Meet</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">Big &amp; Small LLMs working together</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/Osmosis-MCP-4B-demo/Gulp-AI"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">O</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">Osmosis-MCP-4B</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">An Open Source SLM Trained for MCP</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/groq-desktop-beta/groq"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">G</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">Groq Desktop</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">Local Groq Desktop chat app with MCP support</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/multimodal-mcp-client/Ejb503"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">S</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">Systemprompt Multimodal MCP Client</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">A Multi-modal MCP client for voice powered agentic workflows</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/nerve/evilsocket"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">N</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">Nerve</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">The Simple Agent Development Kit.</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/mcp-client-cli/williamvd4"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">M</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">MCP CLI client</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">A simple CLI to run LLM prompt and implement MCP client.</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/playwright-mcp"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">P</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">PLAYWRIGHT-MCP</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">This MCP Server will help you run browser automation and webscraping using Playwright</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/ollama-mcp-client/mihirrd"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">O</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">Ollama MCP (Model Context Protocol)</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">MCP client for local ollama models</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/DrssionPageMCP/wxhzhwxhzh"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">D</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">DrissionPage MCP Server -- 骚神出品</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">基于DrissionPage和FastMCP的浏览器自动化MCP服务器，提供丰富的浏览器操作API供AI调用</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/tinyagents/albertvillanova"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">T</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">tinyagents</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">Tiny Agents: LLM + MCP Tools</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/n8n-coolify-mcp-tools/wrediam"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">C</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">Coolify MCP Workflow</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">This workflow leverages the Community n8n MCP Client and my new Coolify MCP Server to interact with your Coolify infrastructure using MCP (Model Context Protocol).</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/proteus-ai/alishangtian"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">P</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">Proteus Workflow Engine</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">一个强大的、可扩展的多智能体工作流引擎，支持Multi-Agent系统、auto-workflow、MCP-SERVER接入等功能，支持多种工具和资源，提供智能代理和自动化服务执行。</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/mattermost-mcp-host/jagan-shanmugam"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">M</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">Mattermost MCP Host</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">A Mattermost integration that connects to Model Context Protocol (MCP) servers, leveraging a LangGraph-based Agent.</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/codemcp/ezyang"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">C</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">codemcp</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">Coding assistant MCP for Claude Desktop</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/Axiom/aasherkamal216"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">A</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">Axiom - A Docs Expert Agent</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">A documentation AI Agent built with LangGraph, MCP Docs, and Chainlit, designed to help users create different projects using natural language.</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/chat/kamath"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">A</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">Anirudh&#x27;s Chat</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">A simple NextJS MCP client with sensible keybindings</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/LLM-Amap-mcp/blacsheep"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">高</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">高德地址解析服务</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">大模型搭配高德mcp的地址服务</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/install-mcp/supermemoryai"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">I</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">Install MCP CLI</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">A simple CLI to install MCP servers into any client</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/airbnb_mcp/Sakil786"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">A</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">airbnb_mcp</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">airbnb_mcp</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/oneshot/Destiner"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">O</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">oneshot</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">Anthropic MCP client for macOS</p></div></a><a rel="noopener noreferrer nofollow" class="relative group flex items-start gap-4 rounded-xl border p-4 transition-colors border-gray-950/[.1] bg-gray-950/[.01] hover:bg-gray-950/[.05] dark:border-gray-50/[.1] dark:bg-gray-50/[.10] dark:hover:bg-gray-50/[.15]" href="/client/mcphost/mark3labs"><div class="flex-none"><span class="relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">M</span></span></div><div class="min-w-0 flex-1"><div class="flex items-center gap-2"><h3 class="text-base text-sm font-semibold truncate">MCPHost 🤖</h3><div class="flex-1"></div></div><p class="mt-1 text-xs text-muted-foreground line-clamp-2">A CLI host application that enables Large Language Models (LLMs) to interact with external tools through the Model Context Protocol (MCP).</p></div></a></div><nav role="navigation" aria-label="pagination" class="mx-auto flex w-full justify-center mt-8"><ul class="flex flex-row items-center gap-1"><li class="cursor-pointer"><a class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-hidden focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 hover:bg-accent hover:text-accent-foreground h-10 px-4 py-2 gap-1 pl-2.5 cursor-pointer" aria-label="Go to previous page"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-left h-4 w-4"><path d="m15 18-6-6 6-6"></path></svg><span>Previous</span></a></li><li class=""><a class="inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-hidden focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 hover:bg-accent hover:text-accent-foreground h-10 w-10 cursor-pointer">1</a></li><li class=""><a class="inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-hidden focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 hover:bg-accent hover:text-accent-foreground h-10 w-10 cursor-pointer">2</a></li><span aria-hidden="true" class="flex h-9 w-9 items-center justify-center"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-ellipsis h-4 w-4"><circle cx="12" cy="12" r="1"></circle><circle cx="19" cy="12" r="1"></circle><circle cx="5" cy="12" r="1"></circle></svg><span class="sr-only">More pages</span></span><li class=""><a aria-current="page" class="inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-hidden focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 border border-input bg-background hover:bg-accent hover:text-accent-foreground h-10 w-10 cursor-pointer">5</a></li><span aria-hidden="true" class="flex h-9 w-9 items-center justify-center"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-ellipsis h-4 w-4"><circle cx="12" cy="12" r="1"></circle><circle cx="19" cy="12" r="1"></circle><circle cx="5" cy="12" r="1"></circle></svg><span class="sr-only">More pages</span></span><li class=""><a class="inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-hidden focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 hover:bg-accent hover:text-accent-foreground h-10 w-10 cursor-pointer">7</a></li><li class=""><a class="inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-hidden focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 hover:bg-accent hover:text-accent-foreground h-10 w-10 cursor-pointer">8</a></li><li class="cursor-pointer"><a class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-hidden focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 hover:bg-accent hover:text-accent-foreground h-10 px-4 py-2 gap-1 pr-2.5" aria-label="Go to next page"><span>Next</span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-right h-4 w-4"><path d="m9 18 6-6-6-6"></path></svg></a></li></ul></nav><!--$--><!--/$--><!--$--><!--/$--></div><div class="bg-background px-6 py-4 flex flex-col justify-between gap-4 border-t text-center text-sm font-normal text-muted-foreground lg:flex-row lg:items-center lg:text-left"><div class="flex flex-row flex-wrap items-center justify-center gap-1">© 2025 MCP.so. All rights reserved.<p>Sponsored by <a href="https://skywork.ai/?utm_source=mcp.so&amp;utm_medium=referral&amp;utm_campaign=202508&amp;utm_id=000001&amp;utm_term=web_footer&amp;utm_content=v2" target="_blank" rel="noopener noreferrer nofollow" class="text-[#6E29F6] underline font-medium">Skywork</a> Super Agent.</p></div><ul class="flex justify-center gap-4 lg:justify-start"><li class="hover:text-primary"><a target="_self" rel="noopener noreferrer nofollow" href="/explore">Explore</a></li><li class="hover:text-primary"><a target="_self" rel="noopener noreferrer nofollow" href="/playground">Playground</a></li><li class="hover:text-primary"><a target="_self" rel="noopener noreferrer nofollow" href="/posts">Blog</a></li><li class="hover:text-primary"><a target="_self" rel="noopener noreferrer nofollow" href="/usercases">Cases</a></li><li class="hover:text-primary"><a target="_self" rel="noopener noreferrer nofollow" href="/dxt">DXT</a></li><li class="hover:text-primary"><a target="_self" rel="noopener noreferrer nofollow" href="/partners">Partners</a></li></ul><ul class="flex justify-center gap-4 lg:justify-start"><li class="hover:text-primary"><a href="/privacy-policy" rel="noopener noreferrer nofollow">Privacy</a></li><li class="hover:text-primary"><a href="/terms-of-service" rel="noopener noreferrer nofollow">Terms</a></li></ul></div></main></div><section aria-label="Notifications alt+T" tabindex="-1" aria-live="polite" aria-relevant="additions text" aria-atomic="false"></section><script defer="" data-domain="mcp.so" src="https://pla.trys.ai/js/script.js"></script><script src="/_next/static/chunks/webpack-1e56b3c3206210e2.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n5:I[90825,[],\"\"]\n6:I[94061,[],\"\"]\n8:I[32559,[],\"MetadataBoundary\"]\na:I[32559,[],\"OutletBoundary\"]\nd:I[12837,[],\"AsyncMetadataOutlet\"]\nf:I[32559,[],\"ViewportBoundary\"]\n11:I[64168,[],\"\"]\n12:\"$Sreact.suspense\"\n13:I[12837,[],\"AsyncMetadata\"]\n15:I[421,[\"7626\",\"static/chunks/f6ab4fea-4425dae3da7674a3.js\",\"8548\",\"static/chunks/b27dc69b-e5acb70cd9d66ddf.js\",\"7291\",\"static/chunks/f3ac7ef3-57e0d20b76b6c0bf.js\",\"5232\",\"static/chunks/5232-db5923cda78a4960.js\",\"6465\",\"static/chunks/6465-524ff7a6f6bb774c.js\",\"9388\",\"static/chunks/9388-d1a745fd53a10850.js\",\"1508\",\"static/chunks/1508-abbc58115cbb4fd0.js\",\"236\",\"static/chunks/236-f8f5e32bce4e096f.js\",\"7101\",\"static/chunks/7101-9ab34b82066b9cd8.js\",\"6125\",\"static/chunks/6125-6f7cbc86df4f2503.js\",\"2919\",\"static/chunks/2919-7652e8907f6a6ad4.js\",\"7640\",\"static/chunks/7640-74d36ce7728a4200.js\",\"4091\",\"static/chunks/4091-69e125b8fd863f9f.js\",\"976\",\"static/chunks/976-8676cce570a8fe93.js\",\"4942\",\"static/chunks/4942-51fd33fb9762f202.js\",\"4359\",\"static/chunks/app/%5Blocale%5D/(home)/layout-e297fbc0fe7ac09d.js\"],\"SidebarProvider\"]\n16:I[44942,[\"7626\",\"static/chunks/f6ab4fea-4425dae3da7674a3.js\",\"8548\",\"static/chunks/b27dc69b-e5acb70cd9d66ddf.js\",\"7291\",\"static/chunks/f3ac7ef3-57e0d20b76b6c0bf.js\",\"5232\",\"static/chunks/5232-db5923cda78a4960.js\",\"6465\",\"static/chunks/6465-524ff7a6f6bb774c.js\",\"9388\",\"static/chunks/9388-d1a745fd53a10850.js\",\"1508\",\"static/chunks/1508-abbc58115cbb4fd0.js\",\"236\",\"static/chunks/236-f8f5e32bce4e096f.js\",\"7101\",\"static/chunks/7101-9ab34b82066b9cd8.js\",\"6125\",\"static/chunks/6125-6f7cbc86df4f2503.js\",\"2919\",\"static/chunks/2919-7652e8907f6a6ad4.js\",\"7640\",\"static/chunks/7640-74d36ce7728a4200.js\",\"4091\",\"static/chunks/4091-69e125b8fd863f9f.js\",\"976\",\"static/chunks/976-8676cce570a8fe93.js\",\"4942\",\"static/chunks/4942-51fd33fb9762f202.js\",\"4359\",\"static/chunks/app/%5Blocale%5D/(home)/layout-e297fbc0fe7ac09d.js\"],\"default\"]\n17:I[421,[\"7626\",\"static/chunks/f6ab4fea-4425dae3da7674a3.js\",\"8548\",\"static/chunks/b27dc69b-e5acb70cd9d66ddf.js\",\"7291\",\"static/chunks/f3ac7ef3-57e0d20b76b6c0bf.js\",\"5232\",\"static/chunks/5232-db5923cda78a4960.js\",\"6465\",\"static/chunks/6465-524ff7a6f6bb774c.js\",\"9388\",\"static/chunks/9388-d1a745fd53a10850.js\",\"1508\",\"static/chunks/1508-abbc58115cbb4fd0.js\",\"236\",\"static/chunks/236-f8f5e32bce4e096f.js\",\"7101\",\"static/chunks/7101-9ab34b82066b9cd8.js\",\"6125\",\"static/chunks/6125-6f7cbc86df4f2503.js\",\"2919\",\"static/chunks/2919-7652e8907f6a6ad4.js\",\"7640\",\"static/chunks/7640-74d36ce7728a4200.js\",\"4091\",\"static/chunks/4091-69e125b8fd863f9f.js\",\"976\",\"static/chunks/976-8676cce570a8fe93.js\",\"4942\",\"static/chunks/4942-51fd33fb9762f202.js\",\"4359\",\"static/chunks/app/%5Blocale%5D/(home)/layout-e297fbc0fe7ac09d.js\"],\"SidebarInset\"]\n18:I[88430,[\"7626\",\"static/chunks/f6ab4fea-4425dae3da7674a3.js\",\"8548\",\"static/chunks/b27dc69b-e5acb70cd9d66ddf.js\",\"9175\",\"static/chunks/1866796a-670a712d9364e62c.js\",\"7291\",\"static/chunks/f3ac7ef3-57e0d20b76b6c0bf.js\",\"5232\",\"static/chunks/5232-db5923cda78a4960.js\",\"6465\",\"static/chunks/6465-524ff7a6f6bb774c.js\",\"9388\",\"static/chunks/9388-d1a745fd53a10850.js\",\"1508\",\"static/chunks/1508-abbc58115cbb4fd0.js\",\"236\",\"static/chunks/236-f8f5e32bce4e096f.js\",\"7101\",\"static/chunks/7101-9ab34b82066b9cd8.js\",\"6125\",\"static/chunks/6125-6f7cbc86df4f2503.js\",\"2919\",\"static/chunks/2919-7652e8907f6a6ad4.js\",\"6541\",\"static/chunks/6541-2f0250d6a0967fe4.js\",\"7640\",\"static/chunks/7640-74d36ce7728a4200.js\",\"4091\",\"static/chunks/4091-69e125b8fd863f9f.js\",\"976\",\"static/chunks/976-8676cce570a8fe93.js\",\"4942\",\"static/chunks/4942-51fd33fb9762f202.js\",\"3425\",\"static/chunks/app/%5Blocale%5D/(default)/layout-681f37f67bcbff3e.js\"],\"TopBanner\"]\n19:I[37484,[\"7626\",\"static/chunks/f6ab4fea-4425dae3da7674a3.js\",\"8548\",\"static/chunks/b27dc69b-e5acb70cd9d66ddf.js\",\"9175\",\"static/chunks/1866796a-670a712d9364e62c.js\",\"7291\",\"static/chunks/f3ac7ef3-57e0d20b76b6c0bf.js\",\"5232\",\"static/chunks/5232-db5923cda78a4960.js\",\"6465\",\"static/chunks/6465-524ff7a6f6bb774c.js\",\"9388\",\"static/chunks/9388-d1a745fd53a10850.js\",\"1508\",\"static/chunks/1508-abbc58115cbb4fd0.js\",\"236\",\""])</script><script>self.__next_f.push([1,"static/chunks/236-f8f5e32bce4e096f.js\",\"7101\",\"static/chunks/7101-9ab34b82066b9cd8.js\",\"6125\",\"static/chunks/6125-6f7cbc86df4f2503.js\",\"2919\",\"static/chunks/2919-7652e8907f6a6ad4.js\",\"6541\",\"static/chunks/6541-2f0250d6a0967fe4.js\",\"7640\",\"static/chunks/7640-74d36ce7728a4200.js\",\"4091\",\"static/chunks/4091-69e125b8fd863f9f.js\",\"976\",\"static/chunks/976-8676cce570a8fe93.js\",\"4942\",\"static/chunks/4942-51fd33fb9762f202.js\",\"3425\",\"static/chunks/app/%5Blocale%5D/(default)/layout-681f37f67bcbff3e.js\"],\"Header\"]\n20:I[16296,[\"7626\",\"static/chunks/f6ab4fea-4425dae3da7674a3.js\",\"5232\",\"static/chunks/5232-db5923cda78a4960.js\",\"6465\",\"static/chunks/6465-524ff7a6f6bb774c.js\",\"9388\",\"static/chunks/9388-d1a745fd53a10850.js\",\"9732\",\"static/chunks/9732-8158987224fdfa35.js\",\"1358\",\"static/chunks/1358-a345831bbdffdf07.js\",\"1013\",\"static/chunks/app/%5Blocale%5D/(default)/clients/page-5664c6f855076eab.js\"],\"default\"]\n21:I[31096,[\"7626\",\"static/chunks/f6ab4fea-4425dae3da7674a3.js\",\"8548\",\"static/chunks/b27dc69b-e5acb70cd9d66ddf.js\",\"9175\",\"static/chunks/1866796a-670a712d9364e62c.js\",\"7291\",\"static/chunks/f3ac7ef3-57e0d20b76b6c0bf.js\",\"5232\",\"static/chunks/5232-db5923cda78a4960.js\",\"6465\",\"static/chunks/6465-524ff7a6f6bb774c.js\",\"9388\",\"static/chunks/9388-d1a745fd53a10850.js\",\"1508\",\"static/chunks/1508-abbc58115cbb4fd0.js\",\"236\",\"static/chunks/236-f8f5e32bce4e096f.js\",\"7101\",\"static/chunks/7101-9ab34b82066b9cd8.js\",\"6125\",\"static/chunks/6125-6f7cbc86df4f2503.js\",\"2919\",\"static/chunks/2919-7652e8907f6a6ad4.js\",\"6541\",\"static/chunks/6541-2f0250d6a0967fe4.js\",\"7640\",\"static/chunks/7640-74d36ce7728a4200.js\",\"4091\",\"static/chunks/4091-69e125b8fd863f9f.js\",\"976\",\"static/chunks/976-8676cce570a8fe93.js\",\"4942\",\"static/chunks/4942-51fd33fb9762f202.js\",\"3425\",\"static/chunks/app/%5Blocale%5D/(default)/layout-681f37f67bcbff3e.js\"],\"default\"]\n22:I[76550,[\"8548\",\"static/chunks/b27dc69b-e5acb70cd9d66ddf.js\",\"3960\",\"static/chunks/4b7059ae-76f7529d720584f8.js\",\"5232\",\"static/chunks/5232-db5923cda78a4960.js\",\"6465\",\"static/chunks/6465-524ff7a6f6bb774c.js\",\"4902\",\"static/chunks/4902-f1919cf5204afe9f.js\",\"1508\",\"static/chunks/1508-abbc58115cbb4fd0.js\",\"236\",\"static/chunks/236-f8f5e32bce4e096f.js\",\"6125\",\"static/chunks/6125-6f7cbc86df4f2503.js\",\"2407\",\"static/chunks/2407-4b5edf4e84c5d717.js\",\"8450\",\"static/chunks/app/%5Blocale%5D/layout-2c849eca4082e774.js\"],\"NextAuthSessionProvider\"]\n23:I[44777,[\"8548\",\"static/chunks/b27dc69b-e5acb70cd9d66ddf.js\",\"3960\",\"static/chunks/4b7059ae-76f7529d720584f8.js\",\"5232\",\"static/chunks/5232-db5923cda78a4960.js\",\"6465\",\"static/chunks/6465-524ff7a6f6bb774c.js\",\"4902\",\"static/chunks/4902-f1919cf5204afe9f.js\",\"1508\",\"static/chunks/1508-abbc58115cbb4fd0.js\",\"236\",\"static/chunks/236-f8f5e32bce4e096f.js\",\"6125\",\"static/chunks/6125-6f7cbc86df4f2503.js\",\"2407\",\"static/chunks/2407-4b5edf4e84c5d717.js\",\"8450\",\"static/chunks/app/%5Blocale%5D/layout-2c849eca4082e774.js\"],\"AppContextProvider\"]\n24:I[84369,[\"8548\",\"static/chunks/b27dc69b-e5acb70cd9d66ddf.js\",\"3960\",\"static/chunks/4b7059ae-76f7529d720584f8.js\",\"5232\",\"static/chunks/5232-db5923cda78a4960.js\",\"6465\",\"static/chunks/6465-524ff7a6f6bb774c.js\",\"4902\",\"static/chunks/4902-f1919cf5204afe9f.js\",\"1508\",\"static/chunks/1508-abbc58115cbb4fd0.js\",\"236\",\"static/chunks/236-f8f5e32bce4e096f.js\",\"6125\",\"static/chunks/6125-6f7cbc86df4f2503.js\",\"2407\",\"static/chunks/2407-4b5edf4e84c5d717.js\",\"8450\",\"static/chunks/app/%5Blocale%5D/layout-2c849eca4082e774.js\"],\"ChatContextProvider\"]\n25:I[89735,[\"8548\",\"static/chunks/b27dc69b-e5acb70cd9d66ddf.js\",\"3960\",\"static/chunks/4b7059ae-76f7529d720584f8.js\",\"5232\",\"static/chunks/5232-db5923cda78a4960.js\",\"6465\",\"static/chunks/6465-524ff7a6f6bb774c.js\",\"4902\",\"static/chunks/4902-f1919cf5204afe9f.js\",\"1508\",\"static/chunks/1508-abbc58115cbb4fd0.js\",\"236\",\"static/chunks/236-f8f5e32bce4e096f.js\",\"6125\",\"static/chunks/6125-6f7cbc86df4f2503.js\",\"2407\",\"static/chunks/2407-4b5edf4e84c5d717.js\",\"8450\",\"static/chunks/app/%5Blocale%5D/layout-2c849eca4082e774.js\"],\"ThemeProvider\"]\n:HL[\"/_next/static/css/66449ed0e802001c.css\",\"style\"]\n0:{\"P\":null,\"b\":\"VHjKmOxKOsloonDc-p1aa\",\"p\":"])</script><script>self.__next_f.push([1,"\"\",\"c\":[\"\",\"en\",\"clients?page=5\"],\"i\":false,\"f\":[[[\"\",{\"children\":[[\"locale\",\"en\",\"d\"],{\"children\":[\"(default)\",{\"children\":[\"clients\",{\"children\":[\"__PAGE__?{\\\"page\\\":\\\"5\\\"}\",{}]}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/66449ed0e802001c.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],\"$L2\"]}],{\"children\":[[\"locale\",\"en\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,\"$L3\"]}],{\"children\":[\"(default)\",[\"$\",\"$1\",\"c\",{\"children\":[null,\"$L4\"]}],{\"children\":[\"clients\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L7\",[\"$\",\"$L8\",null,{\"children\":\"$L9\"}],null,[\"$\",\"$La\",null,{\"children\":[\"$Lb\",\"$Lc\",[\"$\",\"$Ld\",null,{\"promise\":\"$@e\"}]]}]]}],{},null,false]},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"IKvj3-jrcZylUu1dRdFBM\",{\"children\":[[\"$\",\"$Lf\",null,{\"children\":\"$L10\"}],null]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$11\",\"$undefined\"],\"s\":false,\"S\":false}\n9:[\"$\",\"$12\",null,{\"fallback\":null,\"children\":[\"$\",\"$L13\",null,{\"promise\":\"$@14\"}]}]\n2:[\"$\",\"html\",null,{\"lang\":\"$undefined\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"/favicon.ico\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://fonts.googleapis.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://fonts.gstatic.com\",\"crossOrigin\":\"anonymous\"}],[\"$\",\"link\",null,{\"rel\":\"alternate\",\"hrefLang\":\"en\",\"href\":\"https://mcp.so\"}],[\"$\",\"link\",null,{\"rel\":\"alternate\",\"hrefLang\":\"zh\",\"href\":\"https://mcp.so/zh/\"}],[\"$\",\"link\",null,{\"rel\":\"alternate\",\"hrefLang\":\"ja\",\"href\":\"https://mcp.so/ja/\"}],[\"$\",\"link\",null,{\"rel\":\"alternate\",\"hrefLang\":\"x-default\",\"href\":\"https://mcp.so\"}],[\"$\",\"meta\",null,{\"name\":\"google-adsense-account\",\"content\":\"ca-pub-2123767634383915\"}]]}],[\"$\",\"body\",null,{\"className\":\"min-h-screen bg-background antialiased overflow-x-hidden\",\"children\":[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]]}]\nc:null\n4:[\"$\",\"$L15\",null,{\"children\":[[\"$\",\"$L16\",null,{\"variant\":\"floating\",\"sidebar\":{\"brand\":{\"title\":\"MCP.so\",\"logo\":{\"src\":\"/logo.png\",\"alt\":\"MCP.so\"},\"url\":\"/\"},\"nav\":{\"items\":[{\"title\":\"Home\",\"url\":\"/\",\"icon\":\"RiHomeLine\"},{\"title\":\"Servers\",\"url\":\"/servers?tag=featured\",\"icon\":\"RiServerLine\"},{\"title\":\"Clients\",\"url\":\"/clients?tag=featured\",\"icon\":\"RiAppsLine\"},{\"title\":\"Categories\",\"url\":\"/categories\",\"ico"])</script><script>self.__next_f.push([1,"n\":\"RiMenu2Fill\"},{\"title\":\"Tags\",\"url\":\"/tags\",\"icon\":\"RiHashtag\"},{\"title\":\"Feed\",\"url\":\"/feed\",\"icon\":\"RiWechatLine\"}]},\"bottomNav\":{\"items\":[{\"title\":\"Settings\",\"url\":\"/my-servers\",\"icon\":\"RiSettingsLine\",\"target\":\"_self\"}]},\"social\":{\"items\":[{\"title\":\"X\",\"icon\":\"RiTwitterXFill\",\"url\":\"https://x.com/chatmcp\",\"target\":\"_blank\"},{\"title\":\"Github\",\"icon\":\"RiGithubFill\",\"url\":\"https://github.com/chatmcp/mcpso/issues\",\"target\":\"_blank\"},{\"title\":\"Discord\",\"icon\":\"RiDiscordFill\",\"url\":\"https://discord.gg/RsYPRrnyqg\",\"target\":\"_blank\"},{\"title\":\"Email\",\"icon\":\"RiMailLine\",\"url\":\"mailto:support@mcp.so\",\"target\":\"_self\"}]},\"collapsible\":\"offcanvas\",\"variant\":\"floating\"}}],[\"$\",\"$L17\",null,{\"children\":[[\"$\",\"$L18\",null,{\"message\":\"Skywork: All-in-one Super Agent.\",\"highlight\":\"Skywork\",\"link\":{\"text\":\"Learn more\",\"href\":\"https://skywork.ai?utm_source=mcp.so\u0026utm_medium=referral\u0026utm_campaign=202508\u0026utm_id=000001\u0026utm_term=web_banner\u0026utm_content=v2\",\"target\":\"_blank\"}}],[\"$\",\"$L19\",null,{\"nav\":{\"items\":[]}}],[\"$\",\"div\",null,{\"className\":\"flex-1 px-4 md:px-6 pb-16 pt-4\",\"children\":[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"div\",null,{\"className\":\"bg-background px-6 py-4 flex flex-col justify-between gap-4 border-t text-center text-sm font-normal text-muted-foreground lg:flex-row lg:items-center lg:text-left\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-row flex-wrap items-center justify-center gap-1\",\"children\":[\"© 2025 MCP.so. All rights reserved.\",[\"$\",\"p\",null,{\"children\":[\"Sponsored by \",[\"$\",\"a\",null,{\"href\":\"https://skywork.ai/?utm_source=mcp.so\u0026utm_medium=referral\u0026utm_campaign=202508\u0026utm_id=000001\u0026utm_term=web_footer\u0026utm_content=v2\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer nofollow\",\"className\":\"text-[#6E29F6] underline font-medium\",\"children\":\"Skywork\"}],\" Super Agent.\"]}]]}],[\"$\",\"ul\",null,{\"className\":\"flex justify-center gap-4 lg:justify-start\",\"children\":[[\"$\",\"li\",\"0\",{\"className\":\"hover:text-primary\",\"children\":\"$L1a\"}],[\"$\",\"li\",\"1\",{\"className\":\"hover:text-primary\",\"children\":\"$L1b\"}],[\"$\",\"li\",\"2\",{\"className\":\"hover:text-primary\",\"children\":\"$L1c\"}],[\"$\",\"li\",\"3\",{\"className\":\"hover:text-primary\",\"children\":\"$L1d\"}],[\"$\",\"li\",\"4\",{\"className\":\"hover:text-primary\",\"children\":\"$L1e\"}],[\"$\",\"li\",\"5\",{\"className\":\"hover:text-primary\",\"children\":\"$L1f\"}]]}],[\"$\",\"ul\",null,{\"className\":\"flex justify-center gap-4 lg:justify-start\",\"children\":[[\"$\",\"li\",\"0\",{\"className\":\"hover:text-primary\",\"children\":[\"$\",\"a\",null,{\"href\":\"/privacy-policy\",\"target\":\"$undefined\",\"rel\":\"noopener noreferrer nofollow\",\"children\":\"Privacy\"}]}],[\"$\",\"li\",\"1\",{\"className\":\"hover:text-primary\",\"children\":[\"$\",\"a\",null,{\"href\":\"/terms-of-service\",\"target\":\"$undefined\",\"rel\":\"noopener noreferrer nofollow\",\"children\":\"Terms\"}]}]]}]]}]]}]]}]\n1a:[\"$\",\"$L20\",null,{\"ref\":\"$undefined\",\"href\":\"/explore\",\"locale\":\"$undefined\",\"localeCookie\":{\"name\":\"NEXT_LOCALE\",\"sameSite\":\"lax\"},\"target\":\"_self\",\"rel\":\"noopener noreferrer nofollow\",\"children\":\"Explore\"}]\n1b:[\"$\",\"$L20\",null,{\"ref\":\"$undefined\",\"href\":\"/playground\",\"locale\":\"$undefined\",\"localeCookie\":\"$1a:props:localeCookie\",\"target\":\"_self\",\"rel\":\"noopener noreferrer nofollow\",\"children\":\"Playground\"}]\n1c:[\"$\",\"$L20\",null,{\"ref\":\"$undefined\",\"href\":\"/posts\",\"locale\":\"$undefined\",\"localeCookie\":\"$1a:props:localeCookie\",\"target\":\"_self\",\"rel\":\"noopener noreferrer nofollow\",\"children\":\"Blog\"}]\n1d:[\"$\",\"$L20\",null,{\"ref\":\"$undefined\",\"href\":\"/usercases\",\"locale\":\"$undefined\",\"localeCookie\":\"$1a:props:localeCookie\",\"target\":\"_self\",\"rel\":\"noopener noreferrer nofollow\",\"children\":\"Cases\"}]\n1e:[\"$\",\"$L20\",null,{\"ref\":\"$undefined\",\"href\":\"/dxt\",\"locale\":\"$undefined\",\"localeCookie\":\"$1a:props:localeCookie\",\"target\":\"_self\",\"rel\":\"noopener noreferrer nofollow\",\"children\":\"DXT\"}]\n1f:[\"$\",\"$L20\",null,{\"ref\":\"$undefined\",\"href\":\"/partners\",\"locale\""])</script><script>self.__next_f.push([1,":\"$undefined\",\"localeCookie\":\"$1a:props:localeCookie\",\"target\":\"_self\",\"rel\":\"noopener noreferrer nofollow\",\"children\":\"Partners\"}]\n3:[\"$\",\"$L21\",null,{\"formats\":\"$undefined\",\"locale\":\"en\",\"messages\":{\"metadata\":{\"title\":\"MCP Servers\",\"description\":\"The largest collection of MCP Servers, including Awesome MCP Servers and Claude MCP integration. Search and discover MCP servers to enhance your AI capabilities.\",\"keywords\":\"MCP Servers, Awesome MCP Servers, Claude MCP, Model Context Protocol\"},\"user\":{\"sign_in\":\"Sign In\",\"sign_out\":\"Sign Out\",\"credits\":\"Credits\",\"api_keys\":\"API Keys\",\"my_orders\":\"My Orders\",\"user_center\":\"User Center\",\"admin_system\":\"Admin System\",\"my_servers\":\"My Servers\",\"my_clients\":\"My Clients\"},\"sign_modal\":{\"sign_in_title\":\"Sign In\",\"sign_in_description\":\"Sign in to your account\",\"sign_up_title\":\"Sign Up\",\"sign_up_description\":\"Create an account\",\"email_title\":\"Email\",\"email_placeholder\":\"Input your email here\",\"password_title\":\"Password\",\"password_placeholder\":\"Input your password here\",\"forgot_password\":\"Forgot password?\",\"or\":\"Or\",\"continue\":\"Continue\",\"no_account\":\"Don't have an account?\",\"email_sign_in\":\"Sign in with Email\",\"google_sign_in\":\"Sign in with Google\",\"github_sign_in\":\"Sign in with GitHub\",\"close_title\":\"Close\",\"cancel_title\":\"Cancel\"},\"my_orders\":{\"title\":\"My Orders\",\"description\":\"orders paid with ShipAny.\",\"no_orders\":\"No orders found\",\"tip\":\"\",\"activate_order\":\"Activate Order\",\"actived\":\"Activated\",\"join_discord\":\"Join Discord\",\"read_docs\":\"Read Docs\",\"table\":{\"order_no\":\"Order No\",\"email\":\"Email\",\"product_name\":\"Product Name\",\"amount\":\"Amount\",\"paid_at\":\"Paid At\",\"github_username\":\"GitHub Username\",\"status\":\"Status\"}},\"my_credits\":{\"title\":\"My Credits\",\"left_tip\":\"left credits: {left_credits}\",\"no_credits\":\"No credits records\",\"recharge\":\"Recharge\",\"table\":{\"trans_no\":\"Trans No\",\"trans_type\":\"Trans Type\",\"credits\":\"Credits\",\"updated_at\":\"Updated At\",\"status\":\"Status\"}},\"api_keys\":{\"title\":\"API Keys\",\"tip\":\"Please keep your apikey safe to avoid leaks\",\"no_api_keys\":\"No API Keys\",\"create_api_key\":\"Create API Key\",\"table\":{\"name\":\"Name\",\"key\":\"Key\",\"created_at\":\"Created At\"},\"form\":{\"name\":\"Name\",\"name_placeholder\":\"API Key Name\",\"submit\":\"Submit\"}},\"blog\":{\"title\":\"Blog\",\"description\":\"News, resources, and updates about MCP Servers\",\"read_more_text\":\"Read More\"},\"my_invites\":{\"title\":\"My Invites\",\"description\":\"View your invite records\",\"no_invites\":\"No invite records found\",\"my_invite_link\":\"My Invite Link\",\"edit_invite_link\":\"Edit Invite Link\",\"copy_invite_link\":\"Copy Invite Link\",\"invite_code\":\"Invite Code\",\"invite_tip\":\"Invite 1 friend to buy ShipAny, reward $50.\",\"invite_balance\":\"Invite Reward Balance\",\"total_invite_count\":\"Total Invite Count\",\"total_paid_count\":\"Total Paid Count\",\"total_award_amount\":\"Total Award Amount\",\"update_invite_code\":\"Set Invite Code\",\"update_invite_code_tip\":\"Input your custom invite code\",\"update_invite_button\":\"Save\",\"no_orders\":\"You can't invite others before you bought ShipAny\",\"no_affiliates\":\"You're not allowed to invite others, please contact us to apply for permission.\",\"table\":{\"invite_time\":\"Invite Time\",\"invite_user\":\"Invite User\",\"status\":\"Status\",\"reward_percent\":\"Reward Percent\",\"reward_amount\":\"Reward Amount\",\"pending\":\"Pending\",\"completed\":\"Completed\"}},\"my_servers\":{\"title\":\"My Servers\",\"submit_server\":\"Submit Server\",\"edit_server\":\"Edit Server\",\"table\":{\"avatar\":\"Avatar\",\"name\":\"Name\",\"title\":\"Title\",\"description\":\"Description\",\"status\":\"Status\",\"created_at\":\"Created At\",\"empty_message\":\"No servers\",\"edit\":\"Edit\",\"view\":\"View\",\"url\":\"Github URL\",\"avatar_url\":\"Avatar URL\",\"author_name\":\"Author Name\",\"server_config\":\"Server Config\",\"overview\":\"Overview\",\"content\":\"Content\",\"tags\":\"Tags\",\"category\":\"Category\",\"type\":\"Type\"}},\"mcp\":{\"index_label\":\"Indexed\",\"search_placeholder\":\"Search with keywords\",\"featured_servers\":\"Featured MCP Servers\",\"official_servers\":\"Official MCP Servers\",\"featured_clients\":\"Featured MCP Clients\",\"official_clients\":\"Official MCP Clients\",\"latest_servers\":\"Latest MCP Servers\",\"latest_clients\":\"Latest MCP Client"])</script><script>self.__next_f.push([1,"s\",\"hosted_servers\":\"Hosted MCP Servers\",\"innovation_projects\":\"Innovation Projects\",\"view_all\":\"View All\",\"tabs\":{\"all\":\"All\",\"hosted\":\"Hosted\",\"official\":\"Official\",\"clients\":\"Clients\",\"servers\":\"Servers\",\"categories\":\"Categories\",\"tags\":\"Tags\",\"featured\":\"Featured\",\"latest\":\"Latest\",\"today\":\"Today\",\"random\":\"Random\",\"sponsors\":\"Sponsors\",\"innovation\":\"Innovations\"}},\"usercases\":{\"title\":\"MCP Server Use Cases\",\"description\":\"How to use MCP Servers with creative ways.\"},\"feed\":{\"title\":\"Feed\",\"description\":\"Recent MCP Servers and Clients submitted by users\",\"sub_title\":\"Recent Submissions\",\"submit_my_server\":\"Submit My Server\",\"submitted_at\":\"Submit\",\"created_by\":\"Created By\",\"latest\":\"Latest\",\"comments\":\"Comments\",\"all\":\"All\",\"clients\":\"Clients\",\"servers\":\"Servers\"},\"explore\":{\"title\":\"Explore MCP Servers and Clients\",\"highlight_text\":\"MCP Servers\",\"description\":\"What are you looking for?\",\"trending\":\"Trending\",\"categories\":\"Categories\",\"mcp_servers_title\":\"MCP Servers\",\"mcp_servers_description\":\"Find the best MCP Servers for your needs.\",\"mcp_clients_title\":\"MCP Clients\",\"mcp_clients_description\":\"Find the best MCP Clients for your needs.\",\"nav\":{\"all\":\"All\",\"hosting_servers\":\"Hosting Servers\",\"official_servers\":\"Official Servers\",\"featured_servers\":\"Featured Servers\",\"latest_servers\":\"Latest Servers\",\"featured_clients\":\"Featured Clients\",\"official_clients\":\"Official Clients\",\"latest_clients\":\"Latest Clients\",\"all_servers\":\"All Servers\",\"all_clients\":\"All Clients\",\"innovation_projects\":\"Innovation Projects\"}},\"categories\":{\"title\":\"Categories\",\"description\":\"A list of categories for MCP Servers and Clients.\",\"crumb\":{\"home\":\"Home\",\"categories\":\"Categories\"},\"tabs\":{\"all\":\"All\",\"servers\":\"Servers\",\"clients\":\"Clients\"},\"results\":\"{count} results found\"},\"submit\":{\"title\":\"Submit MCP Server or Client\",\"description\":\"hunt awesome MCP servers or clients, share with the community.\",\"form\":{\"name\":\"Name\",\"name_placeholder\":\"Input the name of the MCP server or client\",\"type\":\"Type\",\"type_placeholder\":\"Select the type of MCP server or client\",\"url\":\"URL\",\"url_placeholder\":\"Input the URL of the MCP server or client\",\"server_config\":\"Server Config\",\"submitting\":\"Submitting...\",\"submit\":\"Submit\",\"hosting_on_cloud\":\"Host your server on cloud 👉\",\"is_innovation\":\"Innovation\",\"innovation_tip\":\"World First MCP Innovation Challenge👉\",\"is_dxt\":\"Is DXT\",\"is_dxt_tip\":\"Desktop Extensions: One-click local MCP server installation in desktop apps\"}},\"project\":{\"created_at\":\"Created At\",\"created_by\":\"Created By\",\"recommend_servers\":\"Recommend Servers\",\"recommend_clients\":\"Recommend Clients\",\"vist_server\":\"Visit Server\",\"vist_client\":\"Visit Client\",\"connect_server\":\"Connect Server\",\"connected\":\"Server Connected\",\"connect_with_sse_url\":\"Connect Server with SSE URL\",\"available_clients\":\"Available Clients\",\"sse_tip\":\"SSE Connect is unstable for connection, please use Stdio or Streamable HTTP to connect server.\",\"try_in_playground\":\"Try in Playground\",\"server_config\":\"Server Config\",\"list_tools\":\"List Tools\",\"loading\":\"Loading...\",\"crumb\":{\"home\":\"Home\",\"server\":\"Servers\",\"client\":\"Clients\"},\"tab\":{\"overview\":\"Overview\",\"content\":\"Content\",\"config\":\"Config\",\"tools\":\"Tools\",\"comments\":\"Comments\",\"chat\":\"Chat\",\"not_configured\":\"Not configured\",\"run\":\"Run\",\"loading\":\"Loading...\",\"connecting\":\"Connecting...\",\"connect\":\"Connect\",\"reconnect\":\"Reconnect\",\"running\":\"Running...\"},\"search_results\":\"Search Results\",\"no_results\":\"No results found\"},\"playground\":{\"title\":\"MCP Server Playground\",\"description\":\"Calling MCP Server Tools online\",\"edit\":\"Edit\",\"reconnect\":\"Reconnect\",\"select_server\":\"Select MCP Server\",\"tools\":\"Tools\",\"call_tool\":\"Call Tool\",\"result\":\"Result\",\"server_not_connected\":\"Server not connected\",\"loading\":\"Loading...\"},\"ranking\":{\"title\":\"Call Ranking\",\"description\":\"Top MCP Servers and Clients ranked by calls\",\"sub_title\":\"Ranking\",\"clients\":\"Clients\",\"servers\":\"Servers\",\"today\":\"Today\",\"this_week\":\"This Week\",\"this_month\":\"This Month\",\"this_year\":\"This Year\",\"calls\":\"Calls\",\"loading\":\"Loading...\"},\"project_info\":{\"title\":\"Project In"])</script><script>self.__next_f.push([1,"fo\",\"featured\":\"Featured\",\"hosted\":\"Hosted\",\"official\":\"Official\",\"remote\":\"Remote\",\"created_at\":\"Created At\",\"updated_at\":\"Updated At\",\"author_name\":\"Author Name\",\"star\":\"Star\",\"language\":\"Language\",\"license\":\"License\",\"category\":\"Category\",\"tags\":\"Tags\",\"homepage\":\"Homepage\",\"source_code\":\"Source Code\",\"update\":\"Update\",\"claim\":\"Claim\",\"report\":\"Report\"},\"nav\":{\"home\":\"Home\",\"explore\":\"Explore\",\"servers\":\"Servers\",\"clients\":\"Clients\",\"feed\":\"Feed\",\"submit\":\"Submit\",\"settings\":\"Settings\",\"chat\":\"Chat\",\"usercases\":\"Cases\",\"playground\":\"Playground\",\"categories\":\"Categories\",\"tags\":\"Tags\",\"blog\":\"Blog\",\"dxt\":\"DXT\",\"partners\":\"Partners\"},\"tags\":{\"title\":\"Tags\",\"description\":\"A list of tags for MCP Servers and Clients.\",\"tabs\":{\"all\":\"All\"},\"results\":\"{count} results found\"},\"clients\":{\"title\":\"MCP Clients\",\"description\":\"A list of MCP Clients.\",\"nav\":{\"all\":\"All\",\"featured\":\"Featured\",\"latest\":\"Latest\"}},\"servers\":{\"title\":\"MCP Servers\",\"description\":\"A list of MCP Servers.\",\"nav\":{\"all\":\"All\",\"featured\":\"Featured\",\"latest\":\"Latest\"}},\"community\":{\"title\":\"Community\",\"description\":\"Meet MCP Lovers here\",\"tabs\":{\"latest\":\"Latest\",\"hot\":\"Hot\"},\"new_topic\":\"New Topic\"},\"dxt\":{\"title\":\"DXT\",\"description\":\"Desktop Extensions: One-click local MCP server installation in desktop apps\",\"tabs\":{\"all\":\"All\"}},\"partners\":{\"title\":\"Partners\",\"description\":\"Trusted Partnerships\"}},\"now\":\"$undefined\",\"timeZone\":\"UTC\",\"children\":[\"$\",\"$L22\",null,{\"children\":[\"$\",\"$L23\",null,{\"children\":[\"$\",\"$L24\",null,{\"children\":[\"$\",\"$L25\",null,{\"attribute\":\"class\",\"disableTransitionOnChange\":true,\"children\":[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]}]}]}]\n10:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\nb:null\n14:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"MCP Clients\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Find the best MCP Clients for your needs.\"}],[\"$\",\"meta\",\"2\",{\"name\":\"keywords\",\"content\":\"MCP Servers, Awesome MCP Servers, Claude MCP, Model Context Protocol\"}],[\"$\",\"link\",\"3\",{\"rel\":\"canonical\",\"href\":\"https://mcp.so/clients\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:title\",\"content\":\"MCP Servers\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:description\",\"content\":\"The largest collection of MCP Servers, including Awesome MCP Servers and Claude MCP integration. Search and discover MCP servers to enhance your AI capabilities.\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:url\",\"content\":\"https://mcp.so\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:site_name\",\"content\":\"MCP.so\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:locale\",\"content\":\"en\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:image\",\"content\":\"https://mcp.so/logo.png\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:image:width\",\"content\":\"1200\"}],[\"$\",\"meta\",\"11\",{\"property\":\"og:image:height\",\"content\":\"630\"}],[\"$\",\"meta\",\"12\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"14\",{\"name\":\"twitter:site\",\"content\":\"MCP.so\"}],[\"$\",\"meta\",\"15\",{\"name\":\"twitter:title\",\"content\":\"MCP Servers\"}],[\"$\",\"meta\",\"16\",{\"name\":\"twitter:description\",\"content\":\"The largest collection of MCP Servers, including Awesome MCP Servers and Claude MCP integration. Search and discover MCP servers to enhance your AI capabilities.\"}],[\"$\",\"meta\",\"17\",{\"name\":\"twitter:image\",\"content\":\"https://mcp.so/logo.png\"}]],\"error\":null,\"digest\":\"$undefined\"}\ne:{\"metadata\":\"$14:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n26:I[66774,[\"7626\",\"static/chunks/f6ab4fea-4425dae3da7674a3.js\",\"5232\",\"static/chunks/5232-db5923cda78a4960.js\",\"6465\",\"static/chunks/6465-524ff7a6f6bb774c.js\",\"9388\",\"static/chunks/9388-d1a745fd53a10850.js\",\"9732\",\"static/chunks/9732-8158987224fdfa35.js\",\"1358\",\"static/chunks/1358-a345831bbdffdf07.js\",\"1013\",\"static/chunks/app/%5Blocale%5D/(default)/clients/page-5664c6f8550"])</script><script>self.__next_f.push([1,"76eab.js\"],\"PageTitleTabs\"]\n27:I[85939,[\"7626\",\"static/chunks/f6ab4fea-4425dae3da7674a3.js\",\"5232\",\"static/chunks/5232-db5923cda78a4960.js\",\"6465\",\"static/chunks/6465-524ff7a6f6bb774c.js\",\"9388\",\"static/chunks/9388-d1a745fd53a10850.js\",\"9732\",\"static/chunks/9732-8158987224fdfa35.js\",\"1358\",\"static/chunks/1358-a345831bbdffdf07.js\",\"1013\",\"static/chunks/app/%5Blocale%5D/(default)/clients/page-5664c6f855076eab.js\"],\"default\"]\n67:I[41664,[\"7626\",\"static/chunks/f6ab4fea-4425dae3da7674a3.js\",\"5232\",\"static/chunks/5232-db5923cda78a4960.js\",\"6465\",\"static/chunks/6465-524ff7a6f6bb774c.js\",\"9388\",\"static/chunks/9388-d1a745fd53a10850.js\",\"9732\",\"static/chunks/9732-8158987224fdfa35.js\",\"1358\",\"static/chunks/1358-a345831bbdffdf07.js\",\"1013\",\"static/chunks/app/%5Blocale%5D/(default)/clients/page-5664c6f855076eab.js\"],\"Pagination\"]\n28:T13f1,# Model Context Protocol client implementation for PHP\n\n[![PHP from Packagist](https://img.shields.io/packagist/php-v/swisnl/mcp-client.svg)](https://packagist.org/packages/swisnl/mcp-client)\n[![Latest Version on Packagist](https://img.shields.io/packagist/v/swisnl/mcp-client.svg)](https://packagist.org/packages/swisnl/mcp-client)\n[![Software License](https://img.shields.io/packagist/l/swisnl/mcp-client.svg)](LICENSE.md)\n[![Buy us a tree](https://img.shields.io/badge/Treeware-%F0%9F%8C%B3-lightgreen.svg)](https://plant.treeware.earth/swisnl/mcp-client)\n[![Build Status](https://img.shields.io/github/actions/workflow/status/swisnl/mcp-client/run-tests.yml?label=tests\u0026branch=master)](https://github.com/swisnl/mcp-client/actions/workflows/run-tests.yml)\n[![Made by SWIS](https://img.shields.io/badge/%F0%9F%9A%80-made%20by%20SWIS-%230737A9.svg)](https://www.swis.nl)\n\nA PHP client library for interacting with Model Context Protocol (MCP) servers.\n\n## Installation\n\nYou can install the package via composer:\n\n```bash\ncomposer require swisnl/mcp-client\n```\n\n## Requirements\n\n- PHP 8.2 or higher\n- [ReactPHP](https://reactphp.org/) packages\n\n## Features\n\n- Multiple transport mechanisms:\n  - SSE (Server-Sent Events)\n  - Stdio (Standard input/output)\n  - Process (External process communication)\n  - StreamableHttp (HTTP with session management)\n- Promise-based API with ReactPHP\n- PSR-3 Logger interface support\n- Most of MCP protocol support (2025-03-26)\n- Tool annotation support\n\n## Basic Usage\n\n### SSE Transport\n\n```php\nuse Swis\\McpClient\\Client;\n\n// Create client with SSE transporter\n$endpoint = 'https://your-mcp-server.com/sse';\n$client = Client::withSse($endpoint);\n\n// Connect to the server\n$client-\u003econnect(function($initResponse) {\n    echo \"Connected to server: \" . json_encode($initResponse['serverInfo']) . \"\\n\";\n});\n\n// List available tools\n$tools = $client-\u003elistTools();\nforeach ($tools-\u003egetTools() as $tool) {\n    echo \"- {$tool-\u003egetName()}: {$tool-\u003egetDescription()}\\n\";\n    \n    // Access tool annotations if available\n    if ($annotations = $tool-\u003egetAnnotations()) {\n        echo \"  * Read-only: \" . ($annotations-\u003egetReadOnlyHint() ? 'Yes' : 'No') . \"\\n\";\n        echo \"  * Title: \" . ($annotations-\u003egetTitle() ?? 'N/A') . \"\\n\";\n    }\n}\n\n// Call a tool\n$result = $client-\u003ecallTool('echo', ['message' =\u003e 'Hello World!']);\necho $result-\u003egetResult() . \"\\n\";\n```\n\n### Process Transport\n\n```php\nuse Swis\\McpClient\\Client;\n\n// Create client with a process transporter\n[$client, $process] = Client::withProcess('/path/to/mcp-server/binary');\n\n// Connect to the server\n$client-\u003econnect();\n\n// Use the client...\n\n// Disconnect when done\n$client-\u003edisconnect();\n```\n\n### StreamableHttp Transport\n\n```php\nuse Swis\\McpClient\\Client;\n\n// Create client with StreamableHttp transporter\n$endpoint = 'https://your-mcp-server.com/';\n$client = Client::withStreamableHttp($endpoint);\n\n// Connect to the server\n$client-\u003econnect();\n\n// The transporter will automatically manage session IDs from the Mcp-Session-Id header\n\n// Use the client...\n\n// Disconnect when done\n$client-\u003edisconnect();\n```\n\n## Use in combination with Agents SDK\n\nFirst, install Agents SDK\n\n```bash\ncomposer require swisnl/agents-sdk\n```\n\n```php\nuse Swis\\Agents\\Agent;\nuse Swis\\Age"])</script><script>self.__next_f.push([1,"nts\\Mcp\\McpConnection;\nuse Swis\\McpClient\\Client;\nuse Swis\\Agents\\Orchestrator;\n\n$agent = new Agent(\n    name: 'Calculator Agent',\n    description: 'This Agent can perform arithmetic operations.',\n    mcpConnections: [\n        new MathMcpConnection(),\n    ]\n);\n\n$orchestrator = new Orchestrator($agent);\necho $orchestrator\n        -\u003ewithUserInstruction('What\\'s 5 + 5?')\n        -\u003erun($agent)\n\nclass MathMcpConnection extends McpConnection\n{\n    public function __construct()\n    {\n        [$client, $process] = Client::withProcess(\n            command: 'node ' . realpath(__DIR__ . '/node_modules/math-mcp/build/index.js'),\n        );\n\n        parent::__construct(\n            client: $client,\n            name: 'Math MCP',\n        );\n    }\n}\n```\n\n## Advanced Usage\n\n### Custom Transporter\n\nYou can implement your own transporter by implementing the `TransporterInterface`:\n\n```php\nuse Swis\\McpClient\\TransporterInterface;\nuse Swis\\McpClient\\EventDispatcher;\n\nclass CustomTransporter implements TransporterInterface\n{\n    // Implement required methods\n}\n\n// Create a client with your custom transporter\n$transporter = new CustomTransporter();\n$eventDispatcher = new EventDispatcher();\n$client = new Client($transporter, $eventDispatcher);\n```\n\n### Async Operations\n\nThe client supports async operations using ReactPHP promises:\n\n```php\n$client-\u003esendRequest(new ListToolsRequest())-\u003ethen(...);\n```\n\n## License\n\nThis package is open-sourced software licensed under the [MIT license](https://opensource.org/licenses/MIT).\n\nThis package is [Treeware](https://treeware.earth). If you use it in production, then we ask that you [**buy the world a tree**](https://plant.treeware.earth/swisnl/mcp-client) to thank us for our work. By contributing to the Treeware forest you’ll be creating employment for local families and restoring wildlife habitats.\n29:T1be4,# Mistr. Agent  \n\nAn autonomous MCP client for Mistral AI models with Model Context Protocol (MCP) integration, enabling AI-driven task execution across local and web environments.\n\n![Main Interface](assets/images/main-interface.png)\n\n## Overview\n\nThe Agentic MCP Client provides a modern interface that connects Mistral language models with real-world capabilities through the Model Context Protocol (MCP). It enables the LLM to autonomously perform tasks by orchestrating multiple tools across different environments - from searching the web to manipulating local files and executing system commands.\n\nUnlike basic chat interfaces, this agent can maintain context across multi-step operations, self-correct when encountering errors, and provide security mechanisms for controlling tool access - creating truly agentic AI experiences with practical real-world applications.\n\n## Screenshots\n\n\u003ctable\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e\u003cimg src=\"assets/images/tool-call-example.png\" alt=\"Tool Call Example\" width=\"400\"/\u003e\u003c/td\u003e\n    \u003ctd\u003e\u003cimg src=\"assets/images/dark-mode.png\" alt=\"Dark Mode\" width=\"400\"/\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003eTool Call Integration\u003c/td\u003e\n    \u003ctd\u003eDark Mode Interface\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\n\u003ctable\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e\u003cimg src=\"assets/images/saved-conversations.png\" alt=\"Saved Conversations Example\" width=\"200\"/\u003e\u003c/td\u003e\n    \u003ctd\u003e\u003cimg src=\"assets/images/tool-list.png\" alt=\"Tool List\" width=\"200\"/\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003eChat History Interface\u003c/td\u003e\n    \u003ctd\u003eTool List Dropdown\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\n## Core Capabilities\n\n🧠 **Autonomous Task Execution**\n- Multi-turn, multi-tool task completion with context maintenance\n- Self-correction of tool usage when parameters are incorrect\n- Sophisticated error handling with detailed feedback\n- Tool state management across conversation turns\n\n🛠️ **Tool Integration Framework**\n- Dynamic loading of MCP servers and their capabilities\n- Automatic tool discovery and capability negotiation\n- Intelligent tool routing across multiple servers\n- Enhanced tool descriptions with parameter validation\n\n🔒 **Security \u0026 Control**\n- Human-in-the-loop approval for sensitive operations\n- Rate limiting for tool access\n- Detection of potentially dangerous operations\n- Detailed audit logging for all too"])</script><script>self.__next_f.push([1,"l usage\n\n💻 **System Integration**\n- Terminal command execution and process management\n- Local filesystem operations (read, write, list, search, edit)\n- File content analysis and manipulation\n- Cross-platform compatibility\n\n🌐 **External Services**\n- Web search via Perplexity AI integration\n- Expandable to other API services through MCP\n- Weather information retrieval\n- Capability to add custom service integrations\n\n## System Requirements\n\n- **Internet Connection**: Required for API access to Mistral models and Perplexity search\n- **Node.js**: v16.x or higher\n- **RAM**: 4GB minimum (8GB recommended)\n- **Storage**: 500MB for installation\n\nThe application can run locally with smaller models like Mistral 8B if you have access to them.\n\n## Supported Models\n\nAll Mistral chat models are supported. The application has been tested and verified with:\n- Mistral 8B\n- Mistral Large\n\nYou can configure your preferred model in the settings.\n\n## Architecture Overview\n\nThe Agentic MCP Client implements a modular architecture:\n\n- **MCP Adapter**: Core orchestration layer connecting Mistral models to MCP servers\n- **Server Manager**: Handles connections to multiple tool servers with capability negotiation\n- **Tool Manager**: Registers and validates tools with enhanced schema information\n- **Security Manager**: Controls tool access with approval workflows and validation\n- **Modern UI**: React/Next.js interface with conversation management\n\n## Installation\n\n### From GitHub\n\n1. **Clone the repository**\n   ```bash\n   git clone https://github.com/itisaevalex/mistr-agent.git\n   cd mistr-agent\n   ```\n\n2. **Install dependencies**\n   ```bash\n   npm install\n   ```\n\n3. **Configure your API keys**\n   ```bash\n   # Create a .env.local file\n   cp .env.example .env.local\n   # Edit the file with your API keys\n   ```\n\n4. **Start the development server**\n   ```bash\n   npm run dev\n   ```\n\n5. **Access the application**\n   Open http://localhost:3000 in your browser\n\n\n## MCP Server Configuration\n\nThe agent connects to Model Context Protocol (MCP) servers to extend its capabilities. These servers are configured in the `mcp-config.json` file:\n\n### Pre-Configured MCP Servers\n\n#### 1. Perplexity Search Server\n\nProvides web search capabilities using Perplexity AI's API:\n```json\n\"perplexity-direct\": {\n  \"type\": \"stdio\",\n  \"name\": \"PerplexityDirect\",\n  \"command\": \"uvx\",\n  \"args\": [\"perplexity-mcp\"],\n  \"env\": {\n    \"PERPLEXITY_API_KEY\": \"your_perplexity_api_key_here\",\n    \"PERPLEXITY_MODEL\": \"sonar\"\n  },\n  \"cwd\": \"/path/to/perplexity-mcp\",\n  \"description\": \"Web search using Perplexity AI\"\n}\n```\n\n#### 2. Desktop Commander Server\n\nProvides local system access with terminal and filesystem operations:\n```json\n\"desktop-commander\": {\n  \"type\": \"stdio\",\n  \"name\": \"DesktopCommander\",\n  \"command\": \"npx\",\n  \"args\": [\n    \"-y\",\n    \"@wonderwhy-er/desktop-commander\"\n  ],\n  \"description\": \"Provides terminal, filesystem, and editing tools\"\n}\n```\n\n### Adding Custom MCP Servers\n\nYou can extend the agent's capabilities by adding your own MCP-compatible servers:\n\n1. Install or set up the server process\n2. Add a new entry to the servers object in mcp-config.json\n3. Restart the development server\n\n## Example Use Cases\n\nThe Agentic MCP Client excels at complex tasks spanning multiple tools:\n\n- **Research Assistant**: Search the web for information, compile findings into local files, and generate summaries\n- **Code Helper**: Search through local repositories, analyze code, execute tests, and explain results\n- **System Administrator**: Monitor processes, execute commands, and manage files with natural language\n- **Content Creator**: Research topics online and use the findings to create structured documents\n\n## Advanced Configuration\n\n### Tool Approval Workflow\n\nThe agent provides a UI toggle for \"Auto Approve\" which can be configured to:\n\n- **Manual Mode**: Present tool calls to the user for approval before execution\n- **Auto Mode**: Execute tool calls automatically without user confirmation\n\nThis allows granular control over which operations require human oversight.\n\n### Adding New Capab"])</script><script>self.__next_f.push([1,"ilities\n\nTo extend the agent's capabilities:\n\n1. Install additional MCP servers that provide the desired tools\n2. Update your mcp-config.json to include the new servers\n3. For custom tools, implement a new MCP server following the protocol specification\n\nThe project has been tested with the following MCP servers: \n- https://github.com/jsonallen/perplexity-mcp\n- https://github.com/wonderwhy-er/DesktopCommanderMCP\n\n## Contributing\n\nContributions are welcome! Key areas for improvement include:\n\n- Additional MCP server integrations\n- Enhanced security policies\n- Improved tool orchestration\n- UI enhancements for better visualization of tool operations\n- Comprehensive testing across different environments\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.2a:T58c,## what is Mistr. Agent?  \nMistr. Agent is an autonomous MCP client designed for Mistral AI models, enabling them to execute complex tasks across web and local environments through standardized agentic capabilities.  \n\n## how to use Mistr. Agent?  \nTo use Mistr. Agent, clone the repository from GitHub, install the dependencies, configure your API keys, and start the development server to access the application in your browser.  \n\n## key features of Mistr. Agent?  \n- Autonomous task execution with context maintenance  \n- Multi-tool integration for complex operations  \n- Security mechanisms for controlling tool access  \n- Support for local filesystem operations and web searches  \n\n## use cases of Mistr. Agent?  \n1. Research Assistant: Compile findings from web searches into local files.  \n2. Code Helper: Analyze code and execute tests.  \n3. System Administrator: Manage files and processes using natural language.  \n4. Content Creator: Research topics and create structured documents.  \n\n## FAQ from Mistr. Agent?  \n- What is the Model Context Protocol (MCP)?  \n\u003e MCP is a protocol that allows Mistral models to interact with various tools and services for enhanced task execution.  \n\n- Is Mistr. Agent free to use?  \n\u003e Yes! Mistr. Agent is open-source and free to use.  \n\n- What are the system requirements?  \n\u003e You need Node.js v16.x or higher, at least 4GB of RAM, and an internet connection for API access.2b:T493,## what is Install MCP CLI? \nInstall MCP CLI is a command-line interface tool designed to simplify the installation and management of MCP servers for various clients.\n\n## how to use Install MCP CLI? \nTo use Install MCP CLI, run the command `npx install-mcp i '\u003ccommand\u003e' --client \u003cclient\u003e` where `\u003cclient\u003e` can be one of the specified options like `claude`, `cline`, etc. It also supports SSE URLs.\n\n## key features of Install MCP CLI? \n- Simplifies the installation of MCP servers.\n- Supports multiple client options.\n- Easy command-line usage with npx.\n\n## use cases of Install MCP CLI? \n1. Quickly installing MCP servers for development environments.\n2. Managing multiple MCP server installations efficiently.\n3. Integrating MCP server setup into automated scripts.\n\n## FAQ from Install MCP CLI? \n- What clients are supported by Install MCP CLI?\n\u003e Supported clients include `claude`, `cline`, `roo-cline`, `windsurf`, `witsy`, and `enconvo`.\n\n- Is there a license for Install MCP CLI?\n\u003e Yes, it is licensed under the MIT license.\n\n- How do I install Install MCP CLI?\n\u003e You can install it using the command `npx install-mcp` followed by your desired command and client.2c:T3e9a,\u003cp align=\"center\"\u003e\n\u003ca href=\"https://pypi.org/project/fast-agent-mcp/\"\u003e\u003cimg src=\"https://img.shields.io/pypi/v/fast-agent-mcp?color=%2334D058\u0026label=pypi\" /\u003e\u003c/a\u003e\n\u003ca href=\"#\"\u003e\u003cimg src=\"https://github.com/evalstate/fast-agent/actions/workflows/main-checks.yml/badge.svg\" /\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/evalstate/fast-agent/issues\"\u003e\u003cimg src=\"https://img.shields.io/github/issues-raw/evalstate/fast-agent\" /\u003e\u003c/a\u003e\n\u003ca href=\"https://discord.gg/xg5cJ7ndN6\"\u003e\u003cimg src=\"https://img.shields.io/discord/1358470293990936787\" alt=\"discord\" /\u003e\u003c/a\u003e\n\u003cimg alt=\"Pepy Total Downloads\" src=\"https://img.shields.io/pepy/dt/fast-agent-mcp?label=pypi%20%7C%20downloads\"/\u003e\n\u003ca href=\"https://github.com"])</script><script>self.__next_f.push([1,"/evalstate/fast-agent-mcp/blob/main/LICENSE\"\u003e\u003cimg src=\"https://img.shields.io/pypi/l/fast-agent-mcp\" /\u003e\u003c/a\u003e\n\u003c/p\u003e\n\n## Overview\n\n\u003e [!TIP]\n\u003e Documentation site is in production here : https://fast-agent.ai. Feel free to feed back what's helpful and what's not. There is also an LLMs.txt [here](https://fast-agent.ai/llms.txt)\n\n**`fast-agent`** enables you to create and interact with sophisticated Agents and Workflows in minutes. It is the first framework with complete, end-to-end tested MCP Feature support including Sampling. Both Anthropic (Haiku, Sonnet, Opus) and OpenAI models (gpt-4o/gpt-4.1 family, o1/o3 family) are supported.\n\nThe simple declarative syntax lets you concentrate on composing your Prompts and MCP Servers to [build effective agents](https://www.anthropic.com/research/building-effective-agents).\n\n`fast-agent` is multi-modal, supporting Images and PDFs for both Anthropic and OpenAI endpoints via Prompts, Resources and MCP Tool Call results. The inclusion of passthrough and playback LLMs enable rapid development and test of Python glue-code for your applications.\n\n\u003e [!IMPORTANT]\n\u003e\n\u003e `fast-agent` The fast-agent documentation repo is here: https://github.com/evalstate/fast-agent-docs. Please feel free to submit PRs for documentation, experience reports or other content you think others may find helpful. All help and feedback warmly received.\n\n### Agent Application Development\n\nPrompts and configurations that define your Agent Applications are stored in simple files, with minimal boilerplate, enabling simple management and version control.\n\nChat with individual Agents and Components before, during and after workflow execution to tune and diagnose your application. Agents can request human input to get additional context for task completion.\n\nSimple model selection makes testing Model \u003c-\u003e MCP Server interaction painless. You can read more about the motivation behind this project [here](https://llmindset.co.uk/resources/fast-agent/)\n\n![2025-03-23-fast-agent](https://github.com/user-attachments/assets/8f6dbb69-43e3-4633-8e12-5572e9614728)\n\n## Get started:\n\nStart by installing the [uv package manager](https://docs.astral.sh/uv/) for Python. Then:\n\n```bash\nuv pip install fast-agent-mcp          # install fast-agent!\n\nuv run fast-agent setup                # create an example agent and config files\nuv run agent.py                        # run your first agent\nuv run agent.py --model=o3-mini.low    # specify a model\nuv run fast-agent quickstart workflow  # create \"building effective agents\" examples\n```\n\nOther quickstart examples include a Researcher Agent (with Evaluator-Optimizer workflow) and Data Analysis Agent (similar to the ChatGPT experience), demonstrating MCP Roots support.\n\n\u003e [!TIP]\n\u003e Windows Users - there are a couple of configuration changes needed for the Filesystem and Docker MCP Servers - necessary changes are detailed within the configuration files.\n\n### Basic Agents\n\nDefining an agent is as simple as:\n\n```python\n@fast.agent(\n  instruction=\"Given an object, respond only with an estimate of its size.\"\n)\n```\n\nWe can then send messages to the Agent:\n\n```python\nasync with fast.run() as agent:\n  moon_size = await agent(\"the moon\")\n  print(moon_size)\n```\n\nOr start an interactive chat with the Agent:\n\n```python\nasync with fast.run() as agent:\n  await agent.interactive()\n```\n\nHere is the complete `sizer.py` Agent application, with boilerplate code:\n\n```python\nimport asyncio\nfrom mcp_agent.core.fastagent import FastAgent\n\n# Create the application\nfast = FastAgent(\"Agent Example\")\n\n@fast.agent(\n  instruction=\"Given an object, respond only with an estimate of its size.\"\n)\nasync def main():\n  async with fast.run() as agent:\n    await agent.interactive()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\nThe Agent can then be run with `uv run sizer.py`.\n\nSpecify a model with the `--model` switch - for example `uv run sizer.py --model sonnet`.\n\n### Combining Agents and using MCP Servers\n\n_To generate examples use `fast-agent quickstart workflow`. This example can be run with `uv run workflow/chaining.py`. fast-"])</script><script>self.__next_f.push([1,"agent looks for configuration files in the current directory before checking parent directories recursively._\n\nAgents can be chained to build a workflow, using MCP Servers defined in the `fastagent.config.yaml` file:\n\n```python\n@fast.agent(\n    \"url_fetcher\",\n    \"Given a URL, provide a complete and comprehensive summary\",\n    servers=[\"fetch\"], # Name of an MCP Server defined in fastagent.config.yaml\n)\n@fast.agent(\n    \"social_media\",\n    \"\"\"\n    Write a 280 character social media post for any given text.\n    Respond only with the post, never use hashtags.\n    \"\"\",\n)\n@fast.chain(\n    name=\"post_writer\",\n    sequence=[\"url_fetcher\", \"social_media\"],\n)\nasync def main():\n    async with fast.run() as agent:\n        # using chain workflow\n        await agent.post_writer(\"http://llmindset.co.uk\")\n```\n\nAll Agents and Workflows respond to `.send(\"message\")` or `.prompt()` to begin a chat session.\n\nSaved as `social.py` we can now run this workflow from the command line with:\n\n```bash\nuv run workflow/chaining.py --agent post_writer --message \"\u003curl\u003e\"\n```\n\nAdd the `--quiet` switch to disable progress and message display and return only the final response - useful for simple automations.\n\n## Workflows\n\n### Chain\n\nThe `chain` workflow offers a more declarative approach to calling Agents in sequence:\n\n```python\n\n@fast.chain(\n  \"post_writer\",\n   sequence=[\"url_fetcher\",\"social_media\"]\n)\n\n# we can them prompt it directly:\nasync with fast.run() as agent:\n  await agent.post_writer()\n\n```\n\nThis starts an interactive session, which produces a short social media post for a given URL. If a _chain_ is prompted it returns to a chat with last Agent in the chain. You can switch the agent to prompt by typing `@agent-name`.\n\nChains can be incorporated in other workflows, or contain other workflow elements (including other Chains). You can set an `instruction` to precisely describe it's capabilities to other workflow steps if needed.\n\n### Human Input\n\nAgents can request Human Input to assist with a task or get additional context:\n\n```python\n@fast.agent(\n    instruction=\"An AI agent that assists with basic tasks. Request Human Input when needed.\",\n    human_input=True,\n)\n\nawait agent(\"print the next number in the sequence\")\n```\n\nIn the example `human_input.py`, the Agent will prompt the User for additional information to complete the task.\n\n### Parallel\n\nThe Parallel Workflow sends the same message to multiple Agents simultaneously (`fan-out`), then uses the `fan-in` Agent to process the combined content.\n\n```python\n@fast.agent(\"translate_fr\", \"Translate the text to French\")\n@fast.agent(\"translate_de\", \"Translate the text to German\")\n@fast.agent(\"translate_es\", \"Translate the text to Spanish\")\n\n@fast.parallel(\n  name=\"translate\",\n  fan_out=[\"translate_fr\",\"translate_de\",\"translate_es\"]\n)\n\n@fast.chain(\n  \"post_writer\",\n   sequence=[\"url_fetcher\",\"social_media\",\"translate\"]\n)\n```\n\nIf you don't specify a `fan-in` agent, the `parallel` returns the combined Agent results verbatim.\n\n`parallel` is also useful to ensemble ideas from different LLMs.\n\nWhen using `parallel` in other workflows, specify an `instruction` to describe its operation.\n\n### Evaluator-Optimizer\n\nEvaluator-Optimizers combine 2 agents: one to generate content (the `generator`), and the other to judge that content and provide actionable feedback (the `evaluator`). Messages are sent to the generator first, then the pair run in a loop until either the evaluator is satisfied with the quality, or the maximum number of refinements is reached. The final result from the Generator is returned.\n\nIf the Generator has `use_history` off, the previous iteration is returned when asking for improvements - otherwise conversational context is used.\n\n```python\n@fast.evaluator_optimizer(\n  name=\"researcher\",\n  generator=\"web_searcher\",\n  evaluator=\"quality_assurance\",\n  min_rating=\"EXCELLENT\",\n  max_refinements=3\n)\n\nasync with fast.run() as agent:\n  await agent.researcher.send(\"produce a report on how to make the perfect espresso\")\n```\n\nWhen used in a workflow, it returns the last `generator` message as the "])</script><script>self.__next_f.push([1,"result.\n\nSee the `evaluator.py` workflow example, or `fast-agent quickstart researcher` for a more complete example.\n\n### Router\n\nRouters use an LLM to assess a message, and route it to the most appropriate Agent. The routing prompt is automatically generated based on the Agent instructions and available Servers.\n\n```python\n@fast.router(\n  name=\"route\",\n  agents=[\"agent1\",\"agent2\",\"agent3\"]\n)\n```\n\nLook at the `router.py` workflow for an example.\n\n### Orchestrator\n\nGiven a complex task, the Orchestrator uses an LLM to generate a plan to divide the task amongst the available Agents. The planning and aggregation prompts are generated by the Orchestrator, which benefits from using more capable models. Plans can either be built once at the beginning (`plantype=\"full\"`) or iteratively (`plantype=\"iterative\"`).\n\n```python\n@fast.orchestrator(\n  name=\"orchestrate\",\n  agents=[\"task1\",\"task2\",\"task3\"]\n)\n```\n\nSee the `orchestrator.py` or `agent_build.py` workflow example.\n\n## Agent Features\n\n### Calling Agents\n\nAll definitions allow omitting the name and instructions arguments for brevity:\n\n```python\n@fast.agent(\"You are a helpful agent\")          # Create an agent with a default name.\n@fast.agent(\"greeter\",\"Respond cheerfully!\")    # Create an agent with the name \"greeter\"\n\nmoon_size = await agent(\"the moon\")             # Call the default (first defined agent) with a message\n\nresult = await agent.greeter(\"Good morning!\")   # Send a message to an agent by name using dot notation\nresult = await agent.greeter.send(\"Hello!\")     # You can call 'send' explicitly\n\nawait agent.greeter()                           # If no message is specified, a chat session will open\nawait agent.greeter.prompt()                    # that can be made more explicit\nawait agent.greeter.prompt(default_prompt=\"OK\") # and supports setting a default prompt\n\nagent[\"greeter\"].send(\"Good Evening!\")          # Dictionary access is supported if preferred\n```\n\n### Defining Agents\n\n#### Basic Agent\n\n```python\n@fast.agent(\n  name=\"agent\",                          # name of the agent\n  instruction=\"You are a helpful Agent\", # base instruction for the agent\n  servers=[\"filesystem\"],                # list of MCP Servers for the agent\n  model=\"o3-mini.high\",                  # specify a model for the agent\n  use_history=True,                      # agent maintains chat history\n  request_params=RequestParams(temperature= 0.7), # additional parameters for the LLM (or RequestParams())\n  human_input=True,                      # agent can request human input\n)\n```\n\n#### Chain\n\n```python\n@fast.chain(\n  name=\"chain\",                          # name of the chain\n  sequence=[\"agent1\", \"agent2\", ...],    # list of agents in execution order\n  instruction=\"instruction\",             # instruction to describe the chain for other workflows\n  cumulative=False,                      # whether to accumulate messages through the chain\n  continue_with_final=True,              # open chat with agent at end of chain after prompting\n)\n```\n\n#### Parallel\n\n```python\n@fast.parallel(\n  name=\"parallel\",                       # name of the parallel workflow\n  fan_out=[\"agent1\", \"agent2\"],          # list of agents to run in parallel\n  fan_in=\"aggregator\",                   # name of agent that combines results (optional)\n  instruction=\"instruction\",             # instruction to describe the parallel for other workflows\n  include_request=True,                  # include original request in fan-in message\n)\n```\n\n#### Evaluator-Optimizer\n\n```python\n@fast.evaluator_optimizer(\n  name=\"researcher\",                     # name of the workflow\n  generator=\"web_searcher\",              # name of the content generator agent\n  evaluator=\"quality_assurance\",         # name of the evaluator agent\n  min_rating=\"GOOD\",                     # minimum acceptable quality (EXCELLENT, GOOD, FAIR, POOR)\n  max_refinements=3,                     # maximum number of refinement iterations\n)\n```\n\n#### Router\n\n```python\n@fast.router(\n  name=\"route\",                          # name of the router\n  agents=[\"agent1\", \"agent2\", \"agent3\""])</script><script>self.__next_f.push([1,"], # list of agent names router can delegate to\n  model=\"o3-mini.high\",                  # specify routing model\n  use_history=False,                     # router maintains conversation history\n  human_input=False,                     # whether router can request human input\n)\n```\n\n#### Orchestrator\n\n```python\n@fast.orchestrator(\n  name=\"orchestrator\",                   # name of the orchestrator\n  instruction=\"instruction\",             # base instruction for the orchestrator\n  agents=[\"agent1\", \"agent2\"],           # list of agent names this orchestrator can use\n  model=\"o3-mini.high\",                  # specify orchestrator planning model\n  use_history=False,                     # orchestrator doesn't maintain chat history (no effect).\n  human_input=False,                     # whether orchestrator can request human input\n  plan_type=\"full\",                      # planning approach: \"full\" or \"iterative\"\n  max_iterations=5,                      # maximum number of full plan attempts, or iterations\n)\n```\n\n### Multimodal Support\n\nAdd Resources to prompts using either the inbuilt `prompt-server` or MCP Types directly. Convenience class are made available to do so simply, for example:\n\n```python\n  summary: str =  await agent.with_resource(\n      \"Summarise this PDF please\",\n      \"mcp_server\",\n      \"resource://fast-agent/sample.pdf\",\n  )\n```\n\n#### MCP Tool Result Conversion\n\nLLM APIs have restrictions on the content types that can be returned as Tool Calls/Function results via their Chat Completions API's:\n\n- OpenAI supports Text\n- Anthropic supports Text and Image\n\nFor MCP Tool Results, `ImageResources` and `EmbeddedResources` are converted to User Messages and added to the conversation.\n\n### Prompts\n\nMCP Prompts are supported with `apply_prompt(name,arguments)`, which always returns an Assistant Message. If the last message from the MCP Server is a 'User' message, it is sent to the LLM for processing. Prompts applied to the Agent's Context are retained - meaning that with `use_history=False`, Agents can act as finely tuned responders.\n\nPrompts can also be applied interactively through the interactive interface by using the `/prompt` command.\n\n### Sampling\n\nSampling LLMs are configured per Client/Server pair. Specify the model name in fastagent.config.yaml as follows:\n\n```yaml\nmcp:\n  servers:\n    sampling_resource:\n      command: \"uv\"\n      args: [\"run\", \"sampling_resource_server.py\"]\n      sampling:\n        model: \"haiku\"\n```\n\n### Secrets File\n\n\u003e [!TIP]\n\u003e fast-agent will look recursively for a fastagent.secrets.yaml file, so you only need to manage this at the root folder of your agent definitions.\n\n### Interactive Shell\n\n![fast-agent](https://github.com/user-attachments/assets/3e692103-bf97-489a-b519-2d0fee036369)\n\n## Project Notes\n\n`fast-agent` builds on the [`mcp-agent`](https://github.com/lastmile-ai/mcp-agent) project by Sarmad Qadri.\n\n### Contributing\n\nContributions and PRs are welcome - feel free to raise issues to discuss. Full guidelines for contributing and roadmap coming very soon. Get in touch!2d:T509,## What is Fast Agent? \nFast Agent is a framework that allows users to define, prompt, and test multi-component (MCP) enabled agents and workflows quickly and efficiently.\n\n## How to use Fast Agent? \nTo use Fast Agent, install the `uv` package manager for Python, then run commands to set up and execute agents. For example, you can create an agent with `fast-agent setup` and run it using `uv run agent.py`.\n\n## Key features of Fast Agent? \n- Simple declarative syntax for defining agents and workflows.\n- Ability to chain agents and use multiple models for different tasks.\n- Support for human input to enhance task completion.\n- Built-in workflows for common tasks like evaluation and optimization.\n\n## Use cases of Fast Agent? \n1. Developing complex agent applications for data analysis.\n2. Creating workflows that combine multiple agents for enhanced functionality.\n3. Testing and evaluating different models for specific tasks.\n\n## FAQ from Fast Agent? \n- Can I use Fast Agent for any type of agent?  \n\u003e Yes! Fast Agent "])</script><script>self.__next_f.push([1,"is designed to support a wide range of agent applications and workflows.\n\n- Is Fast Agent free to use?  \n\u003e Yes! Fast Agent is open-source and free to use.\n\n- How do I get support for Fast Agent?  \n\u003e You can report issues or ask questions on the GitHub repository.2e:Tcac,\u003e ⚠️ We’re currently in the process of updating the open-source Toolbase repository to reflect our latest changes after testing with alpha testers.\n\n# Toolbase\n\n\u003cimg width=\"1392\" alt=\"app\" src=\"https://github.com/user-attachments/assets/d60edc11-cabb-49c8-ba52-1a1418a9ebea\" /\u003e\n\nToolbase is a desktop application that makes it easy to add tools and plugins to Claude and AI Platforms. It provides a simple interface for discovering, installing, and managing tools without requiring technical expertise.\n\nLearn more at [https://gettoolbase.ai](https://gettoolbase.ai)\n\nPowered by the [Model Context Protocol (MCP)](https://modelcontextprotocol.io)\n\n\u003e ⚠️ **Developer Preview**: This project is currently in active development. APIs and features may change without notice.\n\n## Features\n\n- 🔍 Visual tool browser\n- 🚀 One-click tool installation\n- 🔧 Simple configuration management\n- 🔌 Seamless Claude Desktop integration\n\n## Getting Started\n\n### Prerequisites\n- [Claude Desktop](https://claude.ai/download) installed\n\n### Download and Run Toolbase\n\u003e Toolbase currently only supports macOS. Support for Windows and Linux in active development.\n\n1. Download the latest version of Toolbase from [https://gettoolbase.ai](https://gettoolbase.ai)\n2. Open the application and start using tools and plugins with Claude Desktop\n\n## Project Structure\n\n```\ntoolbase/\n├── packages/\n   ├── app/      # Desktop application for tool management\n   └── runner/   # Secure local runner for MCP servers\n\n```\n\nThis monorepo contains two main packages:\n\n- `packages/app` - Desktop application for tool management and configuration, built with Electron\n- `packages/runner` - Local runner for executing MCP servers and tools, built with Deno\n\nSee individual package READMEs for detailed documentation:\n\n- [Desktop App Documentation](packages/app/README.md)\n- [Runner Documentation](packages/runner/README.md)\n\n## Development Setup\n\nAlthough setup as a monorepo, each package are managed independently.\n\nClone the repository:\n```bash\ngit clone https://github.com/Toolbase-AI/toolbase.git\ncd toolbase\n```\n\n### Desktop App (`packages/app`)\n\nThe desktop application is built with Electron and provides the main user interface.\n\n- [Desktop App Documentation](packages/app/README.md)\n\n#### Prequisites\n- Node 22\n- npm\n\n```bash\ncd packages/app\nnpm install\n\n# Run the Electron development application\nnpm start\n```\n\n### Runner (`packages/runner`)\n\nThe runner is a Deno CLI and library that executes MCP servers locally to provide tools and plugins to Claude Desktop and other AI platforms. This will be built into a binary to run on the user's local devices. \n\n- [Runner Documentation](packages/runner/README.md)\n\n#### Prequisites\n- Deno 2\n\n```bash\ncd packages/runner\n\n# Run locally\ndeno task dev [OPTIONS]\n\n# Build binary\ndeno task build\n```\n\n## License\n\nThis project is licensed under the Apache 2.0 License - see the [LICENSE](LICENSE) file for details.\n\n## Support\n\n- 📖 [Website](https://gettoolbase.ai)\n- 📧 [Email Support](mailto:dev@gettoolbase.ai)\n\n## Acknowledgments\n\n- Built for enhancing [Claude Desktop](https://claude.ai)\n- Uses the [Model Context Protocol](https://modelcontextprotocol.io)\n- Inspired by the MCP community2f:T4b6,## What is Toolbase? \nToolbase is a desktop application designed to enhance Claude and AI platforms by providing a user-friendly interface for adding and managing tools and plugins.\n\n## How to use Toolbase? \nTo use Toolbase, download the application from [gettoolbase.ai](https://gettoolbase.ai), install it on macOS, and start integrating tools with Claude Desktop.\n\n## Key features of Toolbase? \n- 🔍 Visual tool browser for easy discovery\n- 🚀 One-click installation of tools\n- 🔧 Simple configuration management\n- 🔌 Seamless integration with Claude Desktop\n\n##"])</script><script>self.__next_f.push([1," Use cases of Toolbase? \n1. Enhancing AI capabilities by adding new tools\n2. Simplifying tool management for developers\n3. Facilitating the integration of plugins into AI platforms\n\n## FAQ from Toolbase? \n- Is Toolbase available for Windows and Linux?  \n\u003e Currently, Toolbase only supports macOS, but support for Windows and Linux is in active development.\n\n- How do I install Toolbase?  \n\u003e Download the latest version from [gettoolbase.ai](https://gettoolbase.ai) and follow the installation instructions.\n\n- What is the Model Context Protocol (MCP)?  \n\u003e MCP is a protocol that Toolbase uses to manage and execute tools and plugins.30:T90d,# Zola\n\n[zola.chat](https://zola.chat)\n\n**Zola** is the open-source interface for AI chat.\n\n[![Chat with this repo](https://zola.chat/button/github.svg)](https://zola.chat/?agent=github/ibelick/zola)\n\n![zola screenshot](./public/cover_zola.webp)\n\n## Features\n\n- Multi-model support: OpenAI, Mistral, Claude, Gemini, **Ollama (local models)**\n- File uploads with context-aware answers\n- Clean, responsive UI with light/dark themes\n- Built with Tailwind, shadcn/ui, and prompt-kit\n- Fully open-source and self-hostable\n- Customizable: user system prompt, multiple layout options\n- **Local AI with Ollama**: Run models locally with automatic model detection\n\n## Agent Features (WIP)\n\n- `@agent` mentions\n- Early tool and MCP integration for agent workflows\n- Foundation for more powerful, customizable agents (more coming soon)\n\n## Quick Start\n\n### Option 1: With OpenAI (Cloud)\n\n```bash\ngit clone https://github.com/ibelick/zola.git\ncd zola\nnpm install\necho \"OPENAI_API_KEY=your-key\" \u003e .env.local\nnpm run dev\n```\n\n### Option 2: With Ollama (Local)\n\n```bash\n# Install and start Ollama\ncurl -fsSL https://ollama.ai/install.sh | sh\nollama pull llama3.2  # or any model you prefer\n\n# Clone and run Zola\ngit clone https://github.com/ibelick/zola.git\ncd zola\nnpm install\nnpm run dev\n```\n\nZola will automatically detect your local Ollama models!\n\n### Option 3: Docker with Ollama\n\n```bash\ngit clone https://github.com/ibelick/zola.git\ncd zola\ndocker-compose -f docker-compose.ollama.yml up\n```\n\n[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/ibelick/zola)\n\nTo unlock features like auth, file uploads, and agents, see [INSTALL.md](./INSTALL.md).\n\n## Built with\n\n- [prompt-kit](https://prompt-kit.com/) — AI components\n- [shadcn/ui](https://ui.shadcn.com) — core components\n- [motion-primitives](https://motion-primitives.com) — animated components\n- [vercel ai sdk](https://vercel.com/blog/introducing-the-vercel-ai-sdk) — model integration, AI features\n- [supabase](https://supabase.com) — auth and storage\n\n## Sponsors\n\n\u003ca href=\"https://vercel.com/oss\"\u003e\n  \u003cimg alt=\"Vercel OSS Program\" src=\"https://vercel.com/oss/program-badge.svg\" /\u003e\n\u003c/a\u003e\n\n## License\n\nApache License 2.0\n\n## Notes\n\nThis is a beta release. The codebase is evolving and may change.31:T482,## What is Zola? \nZola is an open-source interface for AI chat that is self-hostable and designed for developers, supporting multiple AI models.\n\n## How to use Zola? \nTo use Zola, you can clone the repository from GitHub, install the necessary dependencies, and set up your environment with either OpenAI's cloud models or local models using Ollama.\n\n## Key features of Zola? \n- Multi-model support including OpenAI, Mistral, Claude, Gemini, and local models with Ollama.\n- Context-aware answers with file uploads.\n- Responsive UI with light/dark themes.\n- Fully customizable user prompts and layout options.\n- Local AI model support with automatic detection.\n\n## Use cases of Zola? \n1. Building custom AI chat applications.\n2. Integrating various AI models for diverse functionalities.\n3. Creating responsive chat interfaces for websites.\n\n## FAQ from Zola? \n- Is Zola free to use?  \n\u003e Yes! Zola is fully open-source and free to use.\n\n- Can I run Zola locally?  \n\u003e Yes! Zola can be self-hosted and run locally using Ollama.\n\n- What programming languages is Zola built with?  \n\u003e Zola is built with TypeScript and utilizes various "])</script><script>self.__next_f.push([1,"modern web technologies.32:T2028,# MCP Chat Desktop App\n## A Cross-Platform Interface for LLMs\n\nThis desktop application utilizes the MCP (Model Context Protocol) to seamlessly connect and interact with various Large Language Models (LLMs). Built on Electron, the app ensures full cross-platform compatibility, enabling smooth operation across different operating systems.\n\nThe primary objective of this project is to deliver a clean, minimalistic codebase that simplifies understanding the core principles of MCP. Additionally, it provides a quick and efficient way to test multiple servers and LLMs, making it an ideal tool for developers and researchers alike.\n\n## News\n\nThis project originated as a modified version of Chat-UI, initially adopting a minimalist code approach to implement core MCP functionality for educational purposes. \n\nThrough iterative updates to MCP, I received community feedback advocating for a completely new architecture - one that eliminates third-party CDN dependencies and establishes clearer modular structure to better support derivative development and debugging workflows. \n\nThis led to the creation of [Tool-Unified UI](https://github.com/AI-QL/tuui),  a restructured desktop application optimized for AI-powered development. Building upon the original foundation, TUUI serves as a practical AI-assisted development paradigm, if you're interested, you can also leverage AI to develop new features for TUUI. The platform employs a strict linting and formatting system to ensure AI-generated code adheres to coding standards.. \n\n## Features\n\n- Cross-Platform Compatibility: Supports Linux, macOS, and Windows.\n\n- Flexible Apache-2.0 License: Allows easy modification and building of your own desktop applications.\n\n- Dynamic LLM Configuration: Compatible with all OpenAI SDK-supported LLMs, enabling quick testing of multiple backends through manual or preset configurations.\n\n- Multi-Client Management: Configure and manage multiple clients to connect to multiple servers using MCP config.\n\n- UI Adaptability: The UI can be directly extracted for web use, ensuring consistent ecosystem and interaction logic across web and desktop versions.\n\n\n## Architecture\n\nAdopted a straightforward architecture consistent with the MCP documentation to facilitate a clear understanding of MCP principles by:\n\n[DeepWiki](https://deepwiki.com/AI-QL/chat-mcp)\n\n## How to use\n\nAfter cloning or downloading this repository:\n\n1. Please modify the `config.json` file located in [src/main](src/main).  \n   Ensure that the `command` and `path` specified in the `args` are valid.\n\n2. Please ensure that [Node.js](https://nodejs.org/) is installed on your system.  \n   You can verify this by running `node -v` and `npm -v` in your terminal to check their respective versions.\n\n3. `npm install`\n\n4. `npm start`\n\n## Configuration\n\nCreate a `.json` file and paste the following content into it. This file can then be provided as the interface configuration for the Chat UI.\n\n- `gtp-api.json`\n\n    ```json\n    {\n        \"chatbotStore\": {\n            \"apiKey\": \"\",\n            \"url\": \"https://api.aiql.com\",\n            \"path\": \"/v1/chat/completions\",\n            \"model\": \"gpt-4o-mini\",\n            \"max_tokens_value\": \"\",\n            \"mcp\": true\n        },\n        \"defaultChoiceStore\": {\n            \"model\": [\n                \"gpt-4o-mini\",\n                \"gpt-4o\",\n                \"gpt-4\",\n                \"gpt-4-turbo\"\n            ]\n        }\n    }\n    ```\n\nYou can replace the 'url' if you have direct access to the OpenAI API.\n\nAlternatively, you can also use another API endpoint that supports function calls: \n\n- `qwen-api.json`\n\n    ```json\n    {\n        \"chatbotStore\": {\n            \"apiKey\": \"\",\n            \"url\": \"https://dashscope.aliyuncs.com/compatible-mode\",\n            \"path\": \"/v1/chat/completions\",\n            \"model\": \"qwen-turbo\",\n            \"max_tokens_value\": \"\",\n            \"mcp\": true\n        },\n        \"defaultChoiceStore\": {\n            \"model\": [\n                \"qwen-turbo\",\n                \"qwen-plus\",\n                \"qwen-max\"\n            ]\n   "])</script><script>self.__next_f.push([1,"     }\n    }\n    ```\n\n- `deepinfra.json`\n\n    ```json\n    {\n        \"chatbotStore\": {\n            \"apiKey\": \"\",\n            \"url\": \"https://api.deepinfra.com\",\n            \"path\": \"/v1/openai/chat/completions\",\n            \"model\": \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n            \"max_tokens_value\": \"32000\",\n            \"mcp\": true\n        },\n        \"defaultChoiceStore\": {\n            \"model\": [\n                \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n                \"meta-llama/Meta-Llama-3.1-405B-Instruct\",\n                \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n            ]\n        }\n    }\n    ```\n\n## Build Application\n\nYou can build your own desktop application by:\n\n```bash\nnpm run build-app\n```\n\nThis CLI helps you build and package your application for your current OS, with artifacts stored in the /artifacts directory.\n\nFor Debian/Ubuntu users experiencing RPM build issues, try one of the following solutions: \n\n- Edit `package.json` to skip the RPM build step. Or \n\n- Install `rpm` using `sudo apt-get install rpm` (You may need to run `sudo apt update` to ensure your package list is up-to-date)\n\n\n# Troubleshooting\n\n## Error: spawn npx ENOENT - [ISSUE 40](https://github.com/modelcontextprotocol/servers/issues/40)\n\nModify the `config.json` in [src/main](src/main)\n\nOn windows, npx may not work, please refer my workaround: [ISSUE 101](https://github.com/modelcontextprotocol/typescript-sdk/issues/101)\n\n- Or you can use `node` in config.json: \n    ```json\n    {\n        \"mcpServers\": {\n            \"filesystem\": {\n            \"command\": \"node\",\n            \"args\": [\n                \"node_modules/@modelcontextprotocol/server-filesystem/dist/index.js\",\n                \"D:/Github/mcp-test\"\n            ]\n            }\n        }\n    }\n    ```\n\nPlease ensure that the provided path is valid, especially if you are using a relative path. It is highly recommended to provide an absolute path for better clarity and accuracy.\n\nBy default, I will install `server-everything`, `server-filesystem`, and `server-puppeteer` for test purposes. However, you can install additional server libraries or use `npx` to utilize other server libraries as needed.\n\n## Installation timeout\n\nGenerally, after executing `npm install` for the entire project, the total size of files in the `node_modules` directory typically exceeds 500MB. \n\nIf the installation process stalls at less than 300MB and the progress bar remains static, it is likely due to a timeout during the installation of the latter part, specifically Electron.\n\nThis issue often arises because the download speed from Electron's default server is excessively slow or even inaccessible in certain regions. To resolve this, you can modify the environment or global variable `ELECTRON_MIRROR` to switch to an Electron mirror site that is accessible from your location.\n\n## Electron builder timeout\n\nWhen using electron-builder to package files, it automatically downloads several large release packages from GitHub. If the network connection is unstable, this process may be interrupted or timeout.\n\nOn Windows, you may need to clear the cache located under the `electron` and `electron-builder` directories within `C:\\Users\\YOURUSERNAME\\AppData\\Local` before attempting to retry.\n\nDue to potential terminal permission issues, it is recommended to use the default shell terminal instead of VSCode's built-in terminal.\n\n## Demo\n\n### Multimodal Support\n![](https://gcore.jsdelivr.net/gh/AI-QL/.github/public/chat-mcp/demo-multimodal.png)\n\n### Reasoning and Latex Support\n![](https://gcore.jsdelivr.net/gh/AI-QL/.github/public/chat-mcp/demo-latex.png)\n\n### MCP Tools Visualization\n![](https://gcore.jsdelivr.net/gh/AI-QL/.github/public/chat-mcp/demo-tools.png)\n\n### MCP Toolcall Process Overview\n![](https://gcore.jsdelivr.net/gh/AI-QL/.github/public/chat-mcp/demo-toolcall.png)\n\n### MCP Prompts Template\n![](https://gcore.jsdelivr.net/gh/AI-QL/.github/public/chat-mcp/demo-prompts.png)\n\n### Dynamic LLM Config\n![](https://gcore.jsdelivr.net/gh/AI-QL/.github/public/chat-mcp/demo-llms.png)\n\n### DevTool Troubleshooting\n![](http"])</script><script>self.__next_f.push([1,"s://gcore.jsdelivr.net/gh/AI-QL/.github/public/chat-mcp/demo-devtool.png)33:T52c,## What is MCP Chat Desktop App? \nMCP Chat Desktop App is a cross-platform desktop application that utilizes the Model Context Protocol (MCP) to interface with various Large Language Models (LLMs). It is built on Electron, ensuring compatibility across different operating systems.\n\n## How to use MCP Chat Desktop App? \nTo use the app, clone or download the repository, modify the `config.json` file, ensure Node.js is installed, run `npm install`, and then start the application with `npm start`.\n\n## Key features of MCP Chat Desktop App? \n- Cross-platform compatibility (Linux, macOS, Windows)\n- Flexible Apache-2.0 license for easy modification\n- Dynamic LLM configuration for testing multiple backends\n- Multi-client management for connecting to various servers\n- UI adaptability for web and desktop versions\n\n## Use cases of MCP Chat Desktop App? \n1. Testing and interacting with different LLMs.\n2. Developing and modifying desktop applications using MCP.\n3. Managing multiple LLM clients for research and development.\n\n## FAQ from MCP Chat Desktop App? \n- Is MCP Chat Desktop App free to use?  \n\u003e Yes! The app is open-source and free to use.\n\n- What platforms does it support?  \n\u003e It supports Linux, macOS, and Windows.\n\n- How can I modify the application?  \n\u003e You can modify the codebase under the Apache-2.0 license.34:T12e9,# EasyMCP\n\nEasyMCP is a flexible and beginner-friendly client for the Model Context Protocol (MCP). It allows you to connect to different types of MCP servers—SSE, NPX, and UV—so you can interact with various tools (e.g., file operations) and integrate with the OpenAI API for an enhanced chat experience.\n\n## Features\n\n- **Multiple Server Support**  \n  - **SSE Servers:** Connect via Server-Sent Events using a server URL.\n  - **NPX Servers:** Launch servers using NPX commands (compatible with Windows and non-Windows systems).\n  - **UV Servers:** Run servers configured with UV commands.\n- **Dynamic Tool Integration:**  \n  The client automatically retrieves available tools from the connected server and uses them to process user queries.\n- **Interactive Chat Loop:**  \n  Type queries and let the client process responses using OpenAI and the available MCP tools.\n- **Configuration Management:**  \n  Easily add new server configurations with `add_server.py`, which updates the appropriate configuration file (e.g., `sse_servers.json`, `npx_servers.json`, or `uv_servers.json`).\n\n## Prerequisites\n\n- **Python 3.10+** (for compatibility with `asyncio` and modern async features)\n- A valid OpenAI API key (set in your `.env` file)\n- Other API keys as needed for additional integrations\n\n## Installation\n\n1. **Clone the repository:**\n\n   ```bash\n   git clone https://github.com/yourusername/EasyMCP.git\n   cd EasyMCP\n   ```\n\n2. **Create a virtual environment and activate it:**\n\n   On Windows:\n   ```bash\n   python -m venv .venv\n   .venv\\Scripts\\activate\n   ```\n\n   On macOS/Linux:\n   ```bash\n   python3 -m venv .venv\n   source .venv/bin/activate\n   ```\n\n3. **Install the required packages:**\n\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n4. **Set up your environment variables:**  \n   Rename the provided `.env.sample` to `.env` (or create your own `.env`) and fill in the necessary API keys and configurations.\n\n## Server Configuration Files\n\nEasyMCP uses several JSON configuration files to manage servers:\n\n- **sse_servers.json:** Contains configurations for SSE-based servers.  \n  Example:\n  ```json\n  {\n    \"mcpServers\": {\n      \"@modelcontextprotocol/time\": {\n        \"url\": \"https://router.mcp.so/sse/pnabizm8lkazpr\"\n      }\n    }\n  }\n  ```\n\n- **npx_servers.json:** Contains configurations for NPX servers.  \n  Example:\n  ```json\n  {\n    \"mcpServers\": {\n      \"filesystem\": {\n        \"command\": \"npx\",\n        \"args\": [\n          \"-y\",\n          \"@modelcontextprotocol/server-filesystem\",\n          \"C:\\\\Users\\\\lotus\\\\Documents\\\\llm_books_papers\",\n          \"C:\\\\Users\\\\lotus\\\\Documents\\\\llm_books_papers\"\n        ]\n      }\n    }\n  }\n  ```\n\n- **uv_servers.json:"])</script><script>self.__next_f.push([1,"** Contains configurations for UV servers.  \n  Example:\n  ```json\n  {\n    \"mcpServers\": {\n      \"sqlite\": {\n        \"command\": \"uv\",\n        \"args\": [\n          \"--directory\",\n          \"parent_of_servers_repo/servers/src/sqlite\",\n          \"run\",\n          \"mcp-server-sqlite\",\n          \"--db-path\",\n          \"~/test.db\"\n        ]\n      }\n    }\n  }\n  ```\n\n## Usage\n\n1. **Run the MCP Client:**\n\n   ```bash\n   python main.py\n   ```\n\n2. **Select a Server:**  \n   The client will load available servers from `sse_servers.json`, `npx_servers.json`, and `uv_servers.json`. When prompted, enter the corresponding number to select a server.\n\n3. **Interact With the Client:**  \n   Once connected, type your queries. For example:\n   - To read a PDF file: `read Build a Large Language Model.pdf`\n   - To use a file tool: `use read_file tool and read 2308.11432v5.pdf`\n\n   The client will guide you through constructing proper file paths based on the allowed directories provided by the MCP server.\n\n## Adding a New Server\n\nTo add a new MCP server configuration:\n\n1. **Prepare your JSON configuration** for the new server.\n2. **Run `add_server.py`:**\n\n   ```bash\n   python add_server.py\n   ```\n\n   This script will:\n   - Detect the server type (NPX, UV, or default to SSE).\n   - Append the new configuration to the appropriate file (e.g., `npx_servers.json`, `uv_servers.json`, or `sse_servers.json`).\n\n## Example Files\n\n- **main.py:**  \n  The main entry point of the EasyMCP client. It handles server connections, the chat loop, and processing queries with OpenAI integration.\n\n- **add_server.py:**  \n  A script to add new MCP server configurations to the JSON files.\n\n- **.env:**  \n  Contains environment variables such as API keys and model configurations.\n\n- **requirements.txt:**  \n  Lists the project dependencies.  \n\n## Contributing\n\nContributions are welcome! Please fork the repository and submit pull requests with detailed descriptions of your changes.\n\n## License\n\nThis project is open source and available under the [MIT License](LICENSE).\n\n## Contact\n\nFor questions or feature requests, feel free to open an issue in the GitHub repository.\n\nHappy coding!35:T480,## what is EasyMCP? \nEasyMCP is a beginner-friendly client for the Model Context Protocol (MCP) that allows users to connect to various server types (SSE, NPX, UV) and integrate with OpenAI for enhanced chat interactions.\n\n## how to use EasyMCP? \nTo use EasyMCP, clone the repository, set up a virtual environment, install the required packages, and run the client. You can then select a server and interact with it by typing queries.\n\n## key features of EasyMCP? \n- Multiple server support (SSE, NPX, UV)\n- Dynamic tool integration for processing user queries\n- Interactive chat loop with OpenAI API\n- Easy server configuration management\n\n## use cases of EasyMCP? \n1. Connecting to different MCP servers for various tasks.\n2. Enhancing chat experiences with OpenAI integration.\n3. Managing server configurations easily through JSON files.\n\n## FAQ from EasyMCP? \n- What programming language is EasyMCP written in?  \n\u003e EasyMCP is written in Python.\n\n- Do I need an OpenAI API key to use EasyMCP?  \n\u003e Yes, a valid OpenAI API key is required for integration.\n\n- Is EasyMCP open source?  \n\u003e Yes, EasyMCP is open source and available under the MIT License.36:T295a,\u003ch1 align=\"center\"\u003eEvo AI - AI Agents Platform\u003c/h1\u003e\n\n\u003cdiv align=\"center\"\u003e\n\n[![Whatsapp Group](https://img.shields.io/badge/Group-WhatsApp-%2322BC18)](https://evolution-api.com/whatsapp)\n[![Discord Community](https://img.shields.io/badge/Discord-Community-blue)](https://evolution-api.com/discord)\n[![Postman Collection](https://img.shields.io/badge/Postman-Collection-orange)](https://evolution-api.com/postman)\n[![Documentation](https://img.shields.io/badge/Documentation-Official-green)](https://doc.evolution-api.com)\n[![License](https://img.shields.io/badge/license-Apache--2.0-blue)](./LICENSE)\n[![Support](https://img.shields.io/badge/Donation-picpay-green)](https://app.picpay.com/user/davidsongomes1998)\n[![Sponsors](https://img.shields.io/badge/Github-sponsor"])</script><script>self.__next_f.push([1,"-orange)](https://github.com/sponsors/EvolutionAPI)\n\n\u003c/div\u003e\n\n## Evo AI - AI Agents Platform\n\nEvo AI is an open-source platform for creating and managing AI agents, enabling integration with different AI models and services.\n\n## 🚀 Overview\n\nThe Evo AI platform allows:\n\n- Creation and management of AI agents\n- Integration with different language models\n- Client management and MCP server configuration\n- Custom tools management\n- **[Google Agent Development Kit (ADK)](https://google.github.io/adk-docs/)**: Base framework for agent development\n- **[CrewAI Support](https://github.com/crewAI/crewAI)**: Alternative framework for agent development (in development)\n- JWT authentication with email verification\n- **[Agent 2 Agent (A2A) Protocol Support](https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/)**: Interoperability between AI agents\n- **[Workflow Agent with LangGraph](https://www.langchain.com/langgraph)**: Building complex agent workflows\n- **Secure API Key Management**: Encrypted storage of API keys\n- **Agent Organization**: Folder structure for organizing agents by categories\n\n## 🤖 Agent Types\n\nEvo AI supports different types of agents that can be flexibly combined:\n\n### 1. LLM Agent (Language Model)\n\nAgent based on language models like GPT-4, Claude, etc. Can be configured with tools, MCP servers, and sub-agents.\n\n### 2. A2A Agent (Agent-to-Agent)\n\nAgent that implements Google's A2A protocol for agent interoperability.\n\n### 3. Sequential Agent\n\nExecutes a sequence of sub-agents in a specific order.\n\n### 4. Parallel Agent\n\nExecutes multiple sub-agents simultaneously.\n\n### 5. Loop Agent\n\nExecutes sub-agents in a loop with a defined maximum number of iterations.\n\n### 6. Workflow Agent\n\nExecutes sub-agents in a custom workflow defined by a graph structure using LangGraph.\n\n### 7. Task Agent\n\nExecutes a specific task using a target agent with structured task instructions.\n\n## 🛠️ Technologies\n\n### Backend\n- **FastAPI**: Web framework for building the API\n- **SQLAlchemy**: ORM for database interaction\n- **PostgreSQL**: Main database\n- **Alembic**: Migration system\n- **Pydantic**: Data validation and serialization\n- **Uvicorn**: ASGI server\n- **Redis**: Cache and session management\n- **JWT**: Secure token authentication\n- **SendGrid/SMTP**: Email service for notifications (configurable)\n- **Jinja2**: Template engine for email rendering\n- **Bcrypt**: Password hashing and security\n- **LangGraph**: Framework for building stateful, multi-agent workflows\n\n### Frontend\n- **Next.js 15**: React framework with App Router\n- **React 18**: User interface library\n- **TypeScript**: Type-safe JavaScript\n- **Tailwind CSS**: Utility-first CSS framework\n- **shadcn/ui**: Modern component library\n- **React Hook Form**: Form management\n- **Zod**: Schema validation\n- **ReactFlow**: Node-based visual workflows\n- **React Query**: Server state management\n\n## 📊 Langfuse Integration (Tracing \u0026 Observability)\n\nEvo AI platform natively supports integration with [Langfuse](https://langfuse.com/) for detailed tracing of agent executions, prompts, model responses, and tool calls, using the OpenTelemetry (OTel) standard.\n\n### How to configure\n\n1. **Set environment variables in your `.env`:**\n\n   ```env\n   LANGFUSE_PUBLIC_KEY=\"pk-lf-...\"\n   LANGFUSE_SECRET_KEY=\"sk-lf-...\"\n   OTEL_EXPORTER_OTLP_ENDPOINT=\"https://cloud.langfuse.com/api/public/otel\"\n   ```\n\n2. **View in the Langfuse dashboard**\n   - Access your Langfuse dashboard to see real-time traces.\n\n## 🤖 Agent 2 Agent (A2A) Protocol Support\n\nEvo AI implements the Google's Agent 2 Agent (A2A) protocol, enabling seamless communication and interoperability between AI agents.\n\nFor more information about the A2A protocol, visit [Google's A2A Protocol Documentation](https://google.github.io/A2A/).\n\n## 📋 Prerequisites\n\n### Backend\n- **Python**: 3.10 or higher\n- **PostgreSQL**: 13.0 or higher\n- **Redis**: 6.0 or higher\n- **Git**: For version control\n- **Make**: For running Makefile commands\n\n### Frontend\n- **Node.js**: 18.0 or higher\n- **pnpm**: Package manager (recomme"])</script><script>self.__next_f.push([1,"nded) or npm/yarn\n\n## 🔧 Installation\n\n### 1. Clone the Repository\n\n```bash\ngit clone https://github.com/EvolutionAPI/evo-ai.git\ncd evo-ai\n```\n\n### 2. Backend Setup\n\n#### Virtual Environment and Dependencies\n\n```bash\n# Create and activate virtual environment\nmake venv\nsource venv/bin/activate  # Linux/Mac\n# or on Windows: venv\\Scripts\\activate\n\n# Install development dependencies\nmake install-dev\n```\n\n#### Environment Configuration\n\n```bash\n# Copy and configure backend environment\ncp .env.example .env\n# Edit the .env file with your database, Redis, and other settings\n```\n\n#### Database Setup\n\n```bash\n# Initialize database and apply migrations\nmake alembic-upgrade\n\n# Seed initial data (admin user, sample clients, etc.)\nmake seed-all\n```\n\n### 3. Frontend Setup\n\n#### Install Dependencies\n\n```bash\n# Navigate to frontend directory\ncd frontend\n\n# Install dependencies using pnpm (recommended)\npnpm install\n\n# Or using npm\n# npm install\n\n# Or using yarn\n# yarn install\n```\n\n#### Frontend Environment Configuration\n\n```bash\n# Copy and configure frontend environment\ncp .env.example .env\n# Edit .env with your API URL (default: http://localhost:8000)\n```\n\nThe frontend `.env` should contain:\n\n```env\nNEXT_PUBLIC_API_URL=http://localhost:8000\n```\n\n## 🚀 Running the Application\n\n### Development Mode\n\n#### Start Backend (Terminal 1)\n```bash\n# From project root\nmake run\n# Backend will be available at http://localhost:8000\n```\n\n#### Start Frontend (Terminal 2)\n```bash\n# From frontend directory\ncd frontend\npnpm dev\n\n# Or using npm/yarn\n# npm run dev\n# yarn dev\n\n# Frontend will be available at http://localhost:3000\n```\n\n### Production Mode\n\n#### Backend\n```bash\nmake run-prod    # Production with multiple workers\n```\n\n#### Frontend\n```bash\ncd frontend\npnpm build \u0026\u0026 pnpm start\n\n# Or using npm/yarn\n# npm run build \u0026\u0026 npm start\n# yarn build \u0026\u0026 yarn start\n```\n\n## 🐳 Docker Installation\n\n### Full Stack with Docker Compose\n\n```bash\n# Build and start all services (backend + database + redis)\nmake docker-build\nmake docker-up\n\n# Initialize database with seed data\nmake docker-seed\n```\n\n### Frontend with Docker\n\n```bash\n# From frontend directory\ncd frontend\n\n# Build frontend image\ndocker build -t evo-ai-frontend .\n\n# Run frontend container\ndocker run -p 3000:3000 -e NEXT_PUBLIC_API_URL=http://localhost:8000 evo-ai-frontend\n```\n\nOr using the provided docker-compose:\n\n```bash\n# From frontend directory\ncd frontend\ndocker-compose up -d\n```\n\n## 🎯 Getting Started\n\nAfter installation, follow these steps:\n\n1. **Access the Frontend**: Open `http://localhost:3000`\n2. **Create Admin Account**: Use the seeded admin credentials or register a new account\n3. **Configure MCP Server**: Set up your first MCP server connection\n4. **Create Client**: Add a client to organize your agents\n5. **Build Your First Agent**: Create and configure your AI agent\n6. **Test Agent**: Use the chat interface to interact with your agent\n\n### Default Admin Credentials\n\nAfter running the seeders, you can login with:\n- **Email**: Check the seeder output for the generated admin email\n- **Password**: Check the seeder output for the generated password\n\n## 🖥️ API Documentation\n\nThe interactive API documentation is available at:\n\n- Swagger UI: `http://localhost:8000/docs`\n- ReDoc: `http://localhost:8000/redoc`\n\n## 👨‍💻 Development Commands\n\n### Backend Commands\n```bash\n# Database migrations\nmake alembic-upgrade            # Update database to latest version\nmake alembic-revision message=\"description\"  # Create new migration\n\n# Seeders\nmake seed-all                   # Run all seeders\n\n# Code verification\nmake lint                       # Verify code with flake8\nmake format                     # Format code with black\n```\n\n### Frontend Commands\n```bash\n# From frontend directory\ncd frontend\n\n# Development\npnpm dev                        # Start development server\npnpm build                      # Build for production\npnpm start                      # Start production server\npnpm lint                       # Run ESLint\n```\n\n## 🚀 Configuration\n\n### Backend Configuration (.env file)\n"])</script><script>self.__next_f.push([1,"\nKey settings include:\n\n```bash\n# Database settings\nPOSTGRES_CONNECTION_STRING=\"postgresql://postgres:root@localhost:5432/evo_ai\"\n\n# Redis settings\nREDIS_HOST=\"localhost\"\nREDIS_PORT=6379\n\n# AI Engine configuration\nAI_ENGINE=\"adk\"  # Options: \"adk\" (Google Agent Development Kit) or \"crewai\" (CrewAI framework)\n\n# JWT settings\nJWT_SECRET_KEY=\"your-jwt-secret-key\"\n\n# Email provider configuration\nEMAIL_PROVIDER=\"sendgrid\"  # Options: \"sendgrid\" or \"smtp\"\n\n# Encryption for API keys\nENCRYPTION_KEY=\"your-encryption-key\"\n```\n\n### Frontend Configuration (.env file)\n\n```bash\n# API Configuration\nNEXT_PUBLIC_API_URL=\"http://localhost:8000\"  # Backend API URL\n```\n\n\u003e **Note**: While Google ADK is fully supported, the CrewAI engine option is still under active development. For production environments, it's recommended to use the default \"adk\" engine.\n\n## 🔐 Authentication\n\nThe API uses JWT (JSON Web Token) authentication with:\n\n- User registration and email verification\n- Login to obtain JWT tokens\n- Password recovery flow\n- Account lockout after multiple failed login attempts\n\n## 🚀 Star Us on GitHub\n\nIf you find EvoAI useful, please consider giving us a star! Your support helps us grow our community and continue improving the product.\n\n[![Star History Chart](https://api.star-history.com/svg?repos=EvolutionAPI/evo-ai\u0026type=Date)](https://www.star-history.com/#EvolutionAPI/evo-ai\u0026Date)\n\n## 🤝 Contributing\n\nWe welcome contributions from the community! Please read our [Contributing Guidelines](CONTRIBUTING.md) for more details.\n\n## 📄 License\n\nThis project is licensed under the [Apache License 2.0](./LICENSE).37:T493,## What is Evo AI? \nEvo AI is an open-source platform designed for creating and managing AI agents, facilitating integration with various AI models and services.\n\n## How to use Evo AI? \nTo use Evo AI, clone the repository from GitHub, set up the backend and frontend environments, and run the application to start creating and managing AI agents.\n\n## Key features of Evo AI? \n- Creation and management of diverse AI agents\n- Integration with multiple language models\n- Support for Agent 2 Agent (A2A) protocol for interoperability\n- Custom tools management and secure API key management\n- Detailed tracing and observability with Langfuse\n\n## Use cases of Evo AI? \n1. Developing AI agents for customer support\n2. Creating complex workflows using multiple agents\n3. Integrating various AI models for enhanced functionality\n\n## FAQ from Evo AI? \n- Is Evo AI free to use?  \n\u003e Yes! Evo AI is open-source and free to use.\n\n- What programming languages are supported?  \n\u003e Evo AI primarily uses Python for the backend and TypeScript for the frontend.\n\n- How can I contribute to Evo AI?  \n\u003e Contributions are welcome! Please refer to the contributing guidelines in the repository.38:T1f5c,# MCPChatbot Example\n\n![MCP Chatbot](assets/mcp_chatbot_logo.png)\n\nThis project demonstrates how to integrate the Model Context Protocol (MCP) with customized LLM (e.g. Qwen), creating a powerful chatbot that can interact with various tools through MCP servers. The implementation showcases the flexibility of MCP by enabling LLMs to use external tools seamlessly.\n\n\u003e [!TIP]\n\u003e For Chinese version, please refer to [README_ZH.md](README_ZH.md).\n\n## Overview\n\n**Chatbot Streamlit Example**\n\n\u003cimg src=\"assets/mcp_chatbot_streamlit_demo_low.gif\" width=\"800\"\u003e\n\n**Workflow Tracer Example**\n\n\u003cimg src=\"assets/single_prompt_demo.png\" width=\"800\"\u003e\n\n- 🚩 Update (2025-04-11):\n  - Added chatbot streamlit example.\n- 🚩 Update (2025-04-10): \n  - More complex LLM response parsing, supporting multiple MCP tool calls and multiple chat iterations.\n  - Added single prompt examples with both regular and streaming modes.\n  - Added interactive terminal chatbot examples.\n\nThis project includes:\n\n- Simple/Complex CLI chatbot interface\n- Integration with some builtin MCP Server like (Markdown processing tools)\n- Support for customized LLM (e.g. Qwen) and Ollama\n- Example scripts for single prompt processing in both regular and streaming modes\n- Interactive terminal chatbot with regular and "])</script><script>self.__next_f.push([1,"streaming response modes\n\n## Requirements\n\n- Python 3.10+\n- Dependencies (automatically installed via requirements):\n  - python-dotenv\n  - mcp[cli]\n  - openai\n  - colorama\n\n## Installation\n\n1. **Clone the repository:**\n\n   ```bash\n   git clone git@github.com:keli-wen/mcp_chatbot.git\n   cd mcp_chatbot\n   ```\n\n2. **Set up a virtual environment (recommended):**\n\n   ```bash\n   cd folder\n   \n   # Install uv if you don't have it already\n   pip install uv\n\n   # Create a virtual environment and install dependencies\n   uv venv .venv --python=3.10\n\n   # Activate the virtual environment\n   # For macOS/Linux\n   source .venv/bin/activate\n   # For Windows\n   .venv\\Scripts\\activate\n\n   # Deactivate the virtual environment\n   deactivate\n   ```\n\n3. **Install dependencies:**\n\n   ```bash\n   pip install -r requirements.txt\n   # or use uv for faster installation\n   uv pip install -r requirements.txt\n   ```\n\n4. **Configure your environment:**\n   - Copy the `.env.example` file to `.env`:\n\n     ```bash\n     cp .env.example .env\n     ```\n\n   - Edit the `.env` file to add your Qwen API key (just for demo, you can use any LLM API key, remember to set the base_url and api_key in the .env file) and set the paths:\n\n     ```\n     LLM_MODEL_NAME=your_llm_model_name_here\n     LLM_BASE_URL=your_llm_base_url_here\n     LLM_API_KEY=your_llm_api_key_here\n     OLLAMA_MODEL_NAME=your_ollama_model_name_here\n     OLLAMA_BASE_URL=your_ollama_base_url_here\n     MARKDOWN_FOLDER_PATH=/path/to/your/markdown/folder\n     RESULT_FOLDER_PATH=/path/to/your/result/folder\n     ```\n\n## Important Configuration Notes ⚠️\n\nBefore running the application, you need to modify the following:\n\n1. **MCP Server Configuration**:\n   Edit `mcp_servers/servers_config.json` to match your local setup:\n\n   ```json\n   {\n       \"mcpServers\": {\n           \"markdown_processor\": {\n               \"command\": \"/path/to/your/uv\",\n               \"args\": [\n                   \"--directory\",\n                   \"/path/to/your/project/mcp_servers\",\n                   \"run\",\n                   \"markdown_processor.py\"\n               ]\n           }\n       }\n   }\n   ```\n\n   Replace `/path/to/your/uv` with the actual path to your uv executable. **You can use `which uv` to get the path**.\n   Replace `/path/to/your/project/mcp_servers` with the absolute path to the mcp_servers directory in your project. (For **Windows** users, you can take a look at the example in the [Troubleshooting](#troubleshooting) section)\n\n2. **Environment Variables**:\n   Make sure to set proper paths in your `.env` file:\n\n   ```\n   MARKDOWN_FOLDER_PATH=\"/path/to/your/markdown/folder\"\n   RESULT_FOLDER_PATH=\"/path/to/your/result/folder\"\n   ```\n\n   The application will validate these paths and throw an error if they contain placeholder values.\n\nYou can run the following command to check your configuration:\n\n```bash\nbash scripts/check.sh\n```\n\n## Usage\n\n### Unit Test\n\nYou can run the following command to run the unit test:\n\n```bash\nbash scripts/unittest.sh\n```\n\n### Examples\n\n#### Single Prompt Examples\n\nThe project includes two single prompt examples:\n\n1. **Regular Mode**: Process a single prompt and display the complete response\n   ```bash\n   python example/single_prompt/single_prompt.py\n   ```\n\n2. **Streaming Mode**: Process a single prompt with real-time streaming output\n   ```bash\n   python example/single_prompt/single_prompt_stream.py\n   ```\n\nBoth examples accept an optional `--llm` parameter to specify which LLM provider to use:\n```bash\npython example/single_prompt/single_prompt.py --llm=ollama\n```\n\n\u003e [!NOTE]\n\u003e For more details, see the [Single Prompt Example README](example/single_prompt/README.md).\n\n#### Terminal Chatbot Examples\n\nThe project includes two interactive terminal chatbot examples:\n\n1. **Regular Mode**: Interactive terminal chat with complete responses\n   ```bash\n   python example/chatbot_terminal/chatbot_terminal.py\n   ```\n\n2. **Streaming Mode**: Interactive terminal chat with streaming responses\n   ```bash\n   python example/chatbot_terminal/chatbot_terminal_stream.py\n   ```\n\nBoth examples accept an optional `--llm` par"])</script><script>self.__next_f.push([1,"ameter to specify which LLM provider to use:\n```bash\npython example/chatbot_terminal/chatbot_terminal.py --llm=ollama\n```\n\n\u003e [!NOTE]\n\u003e For more details, see the [Terminal Chatbot Example README](example/chatbot_terminal/README.md).\n\n#### Streamlit Web Chatbot Example\n\nThe project includes an interactive web-based chatbot example using Streamlit:\n\n```bash\nstreamlit run example/chatbot_streamlit/app.py\n```\n\nThis example features:\n- Interactive chat interface.\n- Real-time streaming responses.\n- Detailed MCP tool workflow visualization.\n- Configurable LLM settings (OpenAI/Ollama) and MCP tool display via the sidebar.\n\n![MCP Chatbot Streamlit Demo](assets/chatbot_streamlit_demo_light.png)\n\n\u003e [!NOTE]\n\u003e For more details, see the [Streamlit Chatbot Example README](example/chatbot_streamlit/README.md).\n\n\u003c/details\u003e\n\n## Project Structure\n\n- `mcp_chatbot/`: Core library code\n  - `chat/`: Chat session management\n  - `config/`: Configuration handling\n  - `llm/`: LLM client implementation\n  - `mcp/`: MCP client and tool integration\n  - `utils/`: Utility functions (e.g. `WorkflowTrace` and `StreamPrinter`)\n- `mcp_servers/`: Custom MCP servers implementation\n  - `markdown_processor.py`: Server for processing Markdown files\n  - `servers_config.json`: Configuration for MCP servers\n- `data-example/`: Example Markdown files for testing\n- `example/`: Example scripts for different use cases\n  - `single_prompt/`: Single prompt processing examples (regular and streaming)\n  - `chatbot_terminal/`: Interactive terminal chatbot examples (regular and streaming)\n  - `chatbot_streamlit/`: Interactive web chatbot example using Streamlit\n\n## Extending the Project\n\nYou can extend this project by:\n\n1. Adding new MCP servers in the `mcp_servers/` directory\n2. Updating the `servers_config.json` to include your new servers\n3. Implementing new functionalities in the existing servers\n4. Creating new examples based on the provided templates\n\n## Troubleshooting\n\nFor Windows users, you can take the following `servers_config.json` as an example:\n\n```json\n{\n    \"mcpServers\": {\n        \"markdown_processor\": {\n            \"command\": \"C:\\\\Users\\\\13430\\\\.local\\\\bin\\\\uv.exe\",\n            \"args\": [\n                \"--directory\",\n                \"C:\\\\Users\\\\13430\\\\mcp_chatbot\\\\mcp_servers\",\n                \"run\", \n                \"markdown_processor.py\"\n            ]\n        }\n    }\n}\n```\n\n- **Path Issues**: Ensure all paths in the configuration files are absolute paths appropriate for your system\n- **MCP Server Errors**: Make sure the tools are properly installed and configured\n- **API Key Errors**: Verify your API key is correctly set in the `.env` file39:T530,## what is MCPChatbot? \nMCPChatbot is a simple chatbot implementation that integrates the Model Context Protocol (MCP) with customized language models (LLMs) like Qwen, allowing for interaction with various tools through MCP servers.\n\n## how to use MCPChatbot? \nTo use MCPChatbot, clone the repository, set up a virtual environment, install dependencies, configure your environment variables, and run the chatbot using the command `python main.py`.\n\n## key features of MCPChatbot? \n- Simple CLI chatbot interface\n- Integration with Markdown processing tools via MCP\n- Support for customized LLMs\n- Example implementation for processing and summarizing Markdown files\n\n## use cases of MCPChatbot? \n1. Engaging in interactive conversations with an AI chatbot.\n2. Processing and summarizing Markdown content.\n3. Extending functionalities by adding new MCP servers.\n\n## FAQ from MCPChatbot? \n- Can MCPChatbot work with any LLM?\n\u003e Yes! MCPChatbot can be configured to work with any LLM API by setting the appropriate environment variables.\n\n- Is there a graphical interface for MCPChatbot?\n\u003e No, MCPChatbot currently operates through a command-line interface.\n\n- How can I extend the functionalities of MCPChatbot?\n\u003e You can extend the project by adding new MCP servers in the `mcp_servers/` directory and updating the configuration.3a:T2bed,# ClaudeR\n\n\n- ClaudeR is an R package that creates a direct connection between RStudio and Claude AI,"])</script><script>self.__next_f.push([1," allowing for interactive coding sessions where Claude can execute code in your active RStudio session and see the results in real-time.\n\n- It can explore the data autonomously, or be a collaborator. The choice is yours.\n\n- This will work with Cursor, or any service that allows for MCP servers that has the ability to run R. For Cusor, install the R add-on (R Extension for Visual Studio Code\n) and update the MCP file accordingly. You can find this in Cursor \u003e Settings \u003e Cursor Settings \u003e MCP\n\n# Features\n\nClaude has the following MCP tools:\n- execute_r – Execute R code and return the output.\n- execute_r_with_plot – Execute R code that generates a plot.\n- get_active_document – Get the content of the active document in RStudio.\n- get_r_info – Get information about the R environment.\n- modify_code_section – Modify a specific section of code in the active document.\n\nFrom these, you are able to do the following:\n\n- Direct Code Execution: Claude can write and execute R code in your active RStudio session (including installing packages)\n- Feedback/Assistance: Receive explanations of what your current R script does, and/or ask for edits at specific lines.\n- Visualization Creation: Claude can generate, see, and refine plots and visualizations \n- Data Analysis: Claude can analyze your datasets and iteratively provide insights\n- Code Logging: All code executed by Claude can be saved to log files for future reference\n- Console Printing: Option to print Claude's code to the console before execution\n- Environment Integration: Claude can access variables and functions in your R environment\n\n* Note: Claude is able to create Quarto Presentations. I recommend opening an active qmd file and asking for specific updates there. It is not perfect but I am actively working on improving this feature.\n\n# How It Works\n\n- ClaudeR leverages the Model Context Protocol (MCP) to create a bidirectional connection between Claude AI and your RStudio environment. \n\n- MCP is an open protocol developed by Anthropic that allows Claude to safely interact with local tools and data sources.\n\nIn this case:\n\n1. The Python MCP server acts as a bridge between Claude and RStudio\n2. When Claude wants to execute R code, it sends the request to the MCP server\n3. The MCP server forwards this to the R addin running in RStudio\n4. The code executes in your R session and results are sent back to Claude\n\n- This architecture ensures Claude can only perform approved operations through well-defined interfaces while maintaining complete control over your R environment.\n\nCheck out the youtube video below for a quick example of what to expect when you use it\n\n[![ClaudeR Demo Video](https://img.youtube.com/vi/KSKcuxRSZDY/0.jpg)](https://youtu.be/KSKcuxRSZDY)\n\n\n# Security Restrictions\n\nFor security reasons, ClaudeR implements strict restrictions on code execution:\n\n- **System commands**: All `system()` and `system2()` calls are blocked, `shell()`, and other methods of executing system commands.\n\n- **File deletion**: Operations that could delete files (like `unlink()`, `file.remove()`, or system commands containing `rm`) are prohibited.\n\n- **Error messages**: When Claude attempts to run restricted code, the operation will be blocked and a specific error message will be returned explaining why.\n\n## Why These Restrictions Matter\n\nThese security measures exist to protect your system from unintended consequences when using an AI assistant:\n\n1. **Data Protection**: While Claude is designed to be helpful, allowing unrestricted system access could potentially lead to accidental deletion or modification of important files.\n\n2. **Controlled Environment**: By limiting operations to data analysis, visualization, and non-destructive R functions, we ensure Claude remains a safe tool for collaboration.\n\n3. **Principle of Least Privilege**: Following security best practices, Claude is given only the permissions necessary to assist with data analysis tasks, not full system access.\n\n4. **Predictable Behavior**: These restrictions create clear boundaries around what actions can be performed aut"])</script><script>self.__next_f.push([1,"omatically versus what requires manual user intervention.\n\nThese restrictions only apply to code executed through the Claude integration. Normal R code you, the human, run directly is not affected by these limitations. If you need to perform restricted operations, you can do so directly in the R console. These restrictions are in place to protect you from any unexpected behavior. Claude is generally safe, but it's always better to be safe than sorry.\n\n\n# Installation\nPrerequisites:\n\nFor Claude Desktop App use:\n\n1) R 4.0+ and RStudio\n2) Python 3.8+ For the MCP server component\n3) Claude Desktop App: The desktop version of Claude AI\n\nFor Cursor:\n\n1) R Extension for Visual Studio Code\n2) Python 3.8+ For the MCP server component\n\n# Step 1: Install R Package Dependencies\n\nRun these inside your RStudio environment. \n```bash\ninstall.packages(c(\n  \"R6\",\n  \"httpuv\",\n  \"jsonlite\", \n  \"miniUI\",\n  \"shiny\", \n  \"base64enc\",\n  \"rstudioapi\",\n  \"devtools\"\n))\n```\n# Step 2: Install Python Dependencies\n\nRun this in terminal or command prompt\n```bash\npip install mcp httpx\n```\n# Step 3: Install ClaudeR from GitHub\n\n\nRun this in your RStudio environment\n```bash\ndevtools::install_github(\"IMNMV/ClaudeR\")\n```\n\n# Step 4: Configure Claude Desktop\nLocate or create the Claude Desktop configuration json file using terminal or command prompts:\n```bash\nMac: cd /Users/YOUR-NAME/Library/Application\\ Support/Claude/\nMac: open .\nWindows: cd %APPDATA%\\Claude\nWindows: explorer .\n```\nIf you can't open it, right click \u003e Open with \u003e pick the editor of your choice (text edit, notepad, vscode, xcode, etc.)\n\n\nOr, via the desktop app, open the Claude desktop App \u003e Click Claude in the top left \u003e Settings \u003e Developer \u003e Edit Config\n\nAdd the following to the claude_desktop_config.json file, or in the mcp.json file in the Cursor settings:\n\n```bash\n{\n  \"mcpServers\": {\n    \"r-studio\": {\n      \"command\": \"python\",  # Or full path to your Python executable\n      \"args\": [\"PATH_TO_REPOSITORY/ClaudeR/scripts/persistent_r_mcp.py\"],\n      \"env\": {\n        \"PYTHONPATH\": \"PATH_TO_PYTHON_SITE_PACKAGES\",  # Optional if using system Python\n        \"PYTHONUNBUFFERED\": \"1\"\n      }\n    }\n  }\n}\n```\nReplace\n- PATH_TO_REPOSITORY with the path to where the package is installed (use find.package(\"ClaudeR\") in R to locate it)\n- PATH_TO_PYTHON_SITE_PACKAGES with the path to your Python site-packages directory\n\n# Finding Your Python Site-Packages Path\n\nTo find your Python site-packages path:\n\n1. Open a terminal or command prompt\n2. Run this command:\n   ```bash\n   python -c \"import site; print(site.getsitepackages()[0])\"\n   ```\n3. Copy the output path and use it in your configuration\n\nIf you're using a virtual environment or conda, make sure to run this command in the correct environment where you installed the dependencies.\n\n# Usage\nStarting the Connection\n\n1) Load the ClaudeR package in your RStudio environment and start the addin:\n\n```bash\nlibrary(ClaudeR)\nclaudeAddin()\n```\n\n2) In the addin interface:\n![ClaudeR Addin Interface](assets/ui_interface.png)\n\n- Click \"Start Server\" to launch the connection\n- Configure logging settings if desired\n- Keep the addin window active while using Claude (you can switch to other views like Files, Plots, Viewers, etc. - just don't hit the stop sign/stop button)\n\n3) Open Claude Desktop and ask it to execute R code in your session\n\n# Logging Options\nThe ClaudeR addin provides several logging options:\n\n- Print Code to Console: Shows Claude's code in your R console before execution\n- Log Code to File: Saves all code executed by Claude to a log file\n- Custom Log Path: Specify where log files should be saved\n\nEach R session gets its own log file with timestamps for all code executed. That means all code made from chats will be saved in a single log file until the R session is restarted.\n\n# Example Interactions\nOnce connected, you can ask Claude things like:\n\n- \"I have a dataset loaded in my env named data, please perform exploratory data analysis on it and run relevant statistical analyses\"\n- \"Load the mtcars dataset and create a scatterplot of mpg vs. hp with a tr"])</script><script>self.__next_f.push([1,"end line\"\n- \"Fit a linear model to predict mpg based on weight and horsepower\"\n- \"Generate a correlation matrix for the iris dataset and visualize it\"\n- \"Create a function to calculate moving averages of a time series\"\n\nAll in all, if you (a human) can do it with R, Claude can do it with R. Go nuts with it. \n\n# Important Notes\n\n- Session Persistence: All variables, data, and functions created by Claude remain in your R session even after you close the connection\n- Code Visibility: By default, code executed by Claude is printed to your console for transparency\n- Port Configuration: The default port is 8787, but you can change it if needed\n- Log Files: Each R session gets its own log file when logging is enabled\n- Claude can install packages if you ask it to. Be careful with this - good prompting is very important. By default it tends to try other methods if it fails, but telling it what it should or shouldn't do as part of the initial prompt is good practice.\n\n# Troubleshooting\n\nFor Connection Issues:\n\n- Make sure Claude Desktop is properly configured\n- Check that the Python path is correct in your config file\n- Verify that you've started the server in the addin interface\n- Try restarting RStudio if the port is already in use\n- Most server issues can be solved by restarting the R session. Make sure to save your work before you do. \n\nFor Python Dependency Issues:\n\n- Ensure you've installed the required Python packages: mcp and httpx\n- Check that your Python environment is accessible\n\nClaude Can't See Results:\n\n- Make sure the addin is running (the window must stay open)\n- Check that the server status shows \"Running\"\n- Verify there are no error messages in the R console\n\nWarnings:\n\n- You may get a warning after installing dev tools, this will not mess with functionality. Bugs still exist, but I will work on fixing them as they arise.\n\n\n- If you stop the server then re-start it in the same R session, you may see the following:\n\n\n\"Listening on http://127.0.0.1:3071\n\ncreateTcpServer: address already in use\n\nError starting HTTP server: Failed to create server\"\n\nThis is a UI bug. The server is still active, and you can have Claude run code like normal. However, if you run into issues with Claude not being able to connect then the server you will need to restart RStudio.\nIf this issue causes Claude to not access the R environment please save your work and click the 'Force Kill Server response' in the viewer pane. This will run the kill command on the backend: \n\n```bash\nkill -9 [PID] \n```\n\nThis happens because the MCP server is made within the active R Studio session and thus that port is binded to it. So, by forcing this termination it will also terminate RStudio. \n\n\n# Limitations\n\n- The addin window must remain open for the connection to work\n- Each R session can only connect to one Claude session at a time\n\n# License\nMIT\n\n# Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.3b:T4ec,## What is ClaudeR? \nClaudeR is an R package that integrates Claude AI with RStudio, enabling interactive coding sessions where Claude can execute R code and provide real-time feedback.\n\n## How to use ClaudeR? \nTo use ClaudeR, install the package in RStudio, start the addin, and connect it to the Claude Desktop App. You can then ask Claude to execute R code and analyze data directly in your R environment.\n\n## Key features of ClaudeR? \n- Direct execution of R code and generation of plots.\n- Feedback and assistance on R scripts.\n- Data analysis and visualization capabilities.\n- Logging of executed code for future reference.\n- Integration with the R environment for seamless collaboration.\n\n## Use cases of ClaudeR? \n1. Performing exploratory data analysis on datasets.\n2. Generating visualizations and statistical analyses.\n3. Collaborating with Claude for coding assistance in R.\n\n## FAQ from ClaudeR? \n- Can ClaudeR execute any R code?  \n\u003e ClaudeR can execute most R code, but it has restrictions on system commands for security reasons.\n\n- Is ClaudeR free to use?  \n\u003e Yes! ClaudeR is open-source and free to use under the MIT l"])</script><script>self.__next_f.push([1,"icense.\n\n- What are the prerequisites for using ClaudeR?  \n\u003e You need R 4.0+, RStudio, and the Claude Desktop App installed.3c:T812,# prodex-js\n\nSimple JS library that let you vibe code to the next level!\n\n\nhttps://github.com/user-attachments/assets/5fc85e16-0a18-4b97-aa6c-e18e6767a407\n\n\n## Features\n\n- Component-level prompt;\n- Page level prompt;\n- Basic vision integration (MCP client can \"ask question\" to what you see in the screen);\n- (Not implemented) Screen capture integration.\n\n## Usage\n\n### Code setup\n\nTo use, add the following to the head of your HTML file:\n\n```html\n\u003cscript name=\"prodex\" src=\"http://cdn.jsdelivr.net/gh/tarasyarema/prodex-js@v0.1.4/core.min.js?k=test\"\u003e\u003c/script\u003e\n```\n\nif you set the `k` the magic components will be loaded, if you do not set it the magic components will not load (e.g. for production builds).\n\nYou can set the `@master` to always get the latest version, or a specific version. But it might be \"more\" unstable.\n\n### MCP setup\n\nAdd the foloowing to your MCP setup\n\n```json\n{\n    \"mcpServers\": {\n        \"prodex\": {\n            \"url\": \"https://prodex-api.onrender.com/mcp/sse?api_key=sk_test\"\n        }\n    }\n}\n```\n\nyou can use `sk_test` as the `api_key` for testing purposes.\n\nCurrently I'm hosting the backend myself, but in the future I might open source / distribute a binary so that you can\nrun the MCP locally, as it might be part of a bigger project.\n\nIf you are really interested in the backend or can not use the an external service, please let me know via LinkedIn\nand I may give you access / binaries.\n\n#### Claude code\n\nEnsure you have `0.2.54` or above\n\n```\nclaude mcp add prodex --transport sse https://prodex-api.onrender.com/mcp/sse?api_key=sk_test\n```\n\n### Disclaimers\n\n1. I tested with Cursor, but probably any other editor that has an MCP connection with their agentic code;\n2. Currently tested in development in two React based projects (with vite), not sure if it will work with other frameworks.\n\n## Development\n\nCheck the [`core.js`](core.js) file for the source code.\n\n## Contributing\n\nPlease open a PR, or just DM me in LinkedIn (Taras Yarema) if you have questions.\n\n## License\n\nMIT, see [LICENSE](LICENSE) for more information.3d:T4d6,## what is prodex-js? \nprodex-js is a simple JavaScript library designed to enhance coding experiences by integrating various prompts and basic vision capabilities.\n\n## how to use prodex-js? \nTo use prodex-js, include the script in the head of your HTML file and set up the MCP configuration as specified in the documentation.\n\n## key features of prodex-js? \n- Component-level prompt integration\n- Page-level prompt functionality\n- Basic vision integration allowing the MCP client to interact with on-screen content\n- Future plans for screen capture integration\n\n## use cases of prodex-js? \n1. Enhancing user interaction in web applications with dynamic prompts.\n2. Integrating visual recognition features in coding environments.\n3. Facilitating real-time coding assistance through MCP connections.\n\n## FAQ from prodex-js? \n- What is MCP?\n\u003e MCP stands for Multi-Component Prompt, which allows for enhanced interaction in coding environments.\n\n- Is prodex-js compatible with all JavaScript frameworks?\n\u003e Currently, it has been tested with React-based projects, but compatibility with other frameworks is not guaranteed.\n\n- How can I contribute to prodex-js?\n\u003e You can contribute by opening a pull request or contacting the author directly.3e:T7fa,\u003cdiv align=\"center\"\u003e\n\n# codename goose\n\n_a local, extensible, open source AI agent that automates engineering tasks_\n\n\u003cp align=\"center\"\u003e\n  \u003ca href=\"https://opensource.org/licenses/Apache-2.0\"\u003e\n    \u003cimg src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\"\u003e\n  \u003c/a\u003e\n  \u003ca href=\"https://discord.gg/7GaTvbDwga\"\u003e\n    \u003cimg src=\"https://img.shields.io/discord/1287729918100246654?logo=discord\u0026logoColor=white\u0026label=Join+Us\u0026color=blueviolet\" alt=\"Discord\"\u003e\n  \u003c/a\u003e\n  \u003ca href=\"https://github.com/block/goose/actions/workflows/ci.yml\"\u003e\n     \u003cimg src=\"https://img.shields.io/github/actions/workflow/status/block/goose/ci.yml?branch=main\" alt=\"CI\"\u003e\n"])</script><script>self.__next_f.push([1,"  \u003c/a\u003e\n\u003c/p\u003e\n\u003c/div\u003e\n\ngoose is your on-machine AI agent, capable of automating complex development tasks from start to finish. More than just code suggestions, goose can build entire projects from scratch, write and execute code, debug failures, orchestrate workflows, and interact with external APIs - _autonomously_.\n\nWhether you're prototyping an idea, refining existing code, or managing intricate engineering pipelines, goose adapts to your workflow and executes tasks with precision.\n\nDesigned for maximum flexibility, goose works with any LLM, seamlessly integrates with MCP servers, and is available as both a desktop app as well as CLI - making it the ultimate AI assistant for developers who want to move faster and focus on innovation. \n\n\n# Quick Links\n- [Quickstart](https://block.github.io/goose/docs/quickstart)\n- [Installation](https://block.github.io/goose/docs/getting-started/installation)\n- [Tutorials](https://block.github.io/goose/docs/category/tutorials)\n- [Documentation](https://block.github.io/goose/docs/category/getting-started)\n\n\n# Goose Around with Us\n- [Discord](https://discord.gg/block-opensource)\n- [YouTube](https://www.youtube.com/@blockopensource)\n- [LinkedIn](https://www.linkedin.com/company/block-opensource)\n- [Twitter/X](https://x.com/blockopensource)\n- [Bluesky](https://bsky.app/profile/opensource.block.xyz)\n- [Nostr](https://njump.me/opensource@block.xyz)3f:T4fd,## what is Codename Goose? \nCodename Goose is an open-source, extensible AI agent designed to automate engineering tasks, going beyond simple code suggestions to install, execute, edit, and test with any LLM.\n\n## how to use Codename Goose? \nTo use Codename Goose, follow the installation guide provided in the documentation, and start automating your development tasks by integrating it with your existing workflows.\n\n## key features of Codename Goose? \n- Automates complex development tasks from start to finish.\n- Capable of building entire projects, writing and executing code, and debugging failures autonomously.\n- Integrates seamlessly with any LLM and MCP-enabled APIs.\n\n## use cases of Codename Goose? \n1. Prototyping new software ideas quickly.\n2. Refining and debugging existing codebases.\n3. Managing intricate engineering pipelines efficiently.\n\n## FAQ from Codename Goose? \n- Can Codename Goose work with any programming language?\n\u003e Yes! Codename Goose is designed to work with any language supported by the LLM you choose.\n\n- Is Codename Goose free to use?\n\u003e Yes! Codename Goose is open-source and free for everyone to use.\n\n- How does Codename Goose ensure code quality?\n\u003e Codename Goose incorporates testing and debugging features to maintain high code quality.40:T641a,# MCP AI Agent\n\nA TypeScript library that enables AI agents to leverage MCP (Model Context Protocol) servers for enhanced capabilities. This library integrates with the AI SDK to provide a seamless way to connect to MCP servers and use their tools in AI-powered applications.\n\n## Features\n\n- Connect to multiple MCP servers using different transport methods (STDIO, SSE)\n- Automatically discover and use tools from MCP servers\n- Integrate with AI SDK for text generation with tool usage\n- Filter and combine MCP tools with custom tools\n- Preconfigured servers for easy initialization\n- Auto-configuration support for simplified setup\n- Agents can call other agents for specific tasks\n- Agent composition - create specialized agents and combine them\n- Named agents with descriptions for improved identification\n- Auto-initialization of agents (no need to call initialize explicitly)\n- Custom tool definitions directly within agent configuration\n- Default model support to simplify multi-agent systems\n- System prompt integration for specialized behaviors\n- Verbose mode for debugging\n\n## Roadmap\n\n- [x] Basic agent with MCP tools integration\n- [x] Auto handled MCP servers\n- [x] Multi-agent workflows\n- [x] Function based tool handlers\n- [ ] Automatic Swagger/OpenAPI to tools conversion (stateless servers simple integration)\n- [ ] API Server implementation (call your agent on a server)\n- [ ] Observabil"])</script><script>self.__next_f.push([1,"ty system\n\n## Installation\n\n```bash\nnpm install mcp-ai-agent\n```\n\nFor a comprehensive example implementation, check out the [mcp-ai-agent-example](https://github.com/fkesheh/mcp-ai-agent-example) repository.\n\n## Minimal Example\n\nHere's the most basic way to use AI Agent with preconfigured servers:\n\n```typescript\nimport { AIAgent, Servers } from \"mcp-ai-agent\";\nimport { openai } from \"@ai-sdk/openai\";\n\n// Use a preconfigured server\nconst agent = new AIAgent({\n  name: \"Sequential Thinking Agent\",\n  description: \"This agent can be used to solve complex tasks\",\n  model: openai(\"gpt-4o-mini\"),\n  toolsConfigs: [Servers.sequentialThinking],\n});\n\n// Use the agent\nconst response = await agent.generateResponse({\n  prompt: \"What is 25 * 25?\",\n});\nconsole.log(response.text);\n```\n\n### Multi-Agent Workflows (Agent Composition)\n\nYou can create specialized agents and compose them into a master agent that can delegate tasks:\n\n```typescript\nimport { AIAgent, Servers } from \"mcp-ai-agent\";\nimport { openai } from \"@ai-sdk/openai\";\n\n// Create specialized agents for different tasks\nconst sequentialThinkingAgent = new AIAgent({\n  name: \"Sequential Thinker\",\n  description:\n    \"Use this agent to think sequentially and resolve complex problems\",\n  model: openai(\"gpt-4o-mini\"),\n  toolsConfigs: [Servers.sequentialThinking],\n});\n\nconst braveSearchAgent = new AIAgent({\n  name: \"Brave Search\",\n  description: \"Use this agent to search the web for the latest information\",\n  model: openai(\"gpt-4o-mini\"),\n  toolsConfigs: [Servers.braveSearch],\n});\n\nconst memoryAgent = new AIAgent({\n  name: \"Memory Agent\",\n  description: \"Use this agent to store and retrieve memories\",\n  model: openai(\"gpt-4o-mini\"),\n  toolsConfigs: [\n    {\n      mcpServers: {\n        memory: {\n          command: \"npx\",\n          args: [\"-y\", \"@modelcontextprotocol/server-memory\"],\n        },\n      },\n    },\n  ],\n});\n\n// Create a master agent that can use all specialized agents\nconst masterAgent = new AIAgent({\n  name: \"Master Agent\",\n  description: \"An agent that can manage and delegate to specialized agents\",\n  model: openai(\"gpt-4o\"),\n  toolsConfigs: [\n    {\n      type: \"agent\",\n      agent: sequentialThinkingAgent,\n    },\n    {\n      type: \"agent\",\n      agent: memoryAgent,\n    },\n    {\n      type: \"agent\",\n      agent: braveSearchAgent,\n    },\n  ],\n});\n\n// Use the master agent\nconst response = await masterAgent.generateResponse({\n  prompt: \"What is the latest Bitcoin price? Store the answer in memory.\",\n});\n\nconsole.log(response.text);\n\n// You can ask the memory agent about information stored by the master agent\nconst memoryResponse = await masterAgent.generateResponse({\n  prompt: \"What information have we stored about Bitcoin price?\",\n});\n\nconsole.log(memoryResponse.text);\n```\n\n## Crew AI style workflow\n\nThe MCP AI Agent can be used to create a crew of specialized agents that work together to accomplish complex tasks, similar to the Crew AI pattern. Here's an example of setting up a project management workflow with multiple specialized agents:\n\n```typescript\nimport { AIAgent, CrewStyleHelpers, Servers } from \"mcp-ai-agent\";\nimport { openai } from \"@ai-sdk/openai\";\nimport { z } from \"zod\";\nimport * as dotenv from \"dotenv\";\n\n// Load environment variables\ndotenv.config();\n\n// Project details\nconst projectDetails = {\n  project: \"Website\",\n  industry: \"Technology\",\n  team_members: [\n    \"John Doe (Project Manager)\",\n    \"Jane Doe (Software Engineer)\",\n    \"Bob Smith (Designer)\",\n    \"Alice Johnson (QA Engineer)\",\n    \"Tom Brown (QA Engineer)\",\n  ],\n  project_requirements: [\n    \"Responsive design for desktop and mobile\",\n    \"Modern user interface\",\n    \"Intuitive navigation system\",\n    \"About Us page\",\n    \"Services page\",\n    \"Contact page with form\",\n    \"Blog section\",\n    \"SEO optimization\",\n    \"Social media integration\",\n    \"Testimonials section\",\n  ],\n};\n\n// Define tasks\nconst tasks = {\n  task_breakdown: (details) =\u003e ({\n    description: `Break down the ${details.project} project requirements into individual tasks.`,\n    expected_output: `A comprehensive list of tasks with descr"])</script><script>self.__next_f.push([1,"iptions, timelines, and dependencies.`,\n  }),\n  time_estimation: (details) =\u003e ({\n    description: `Estimate time and resources for each task in the ${details.project} project.`,\n    expected_output: `A detailed estimation report for each task.`,\n  }),\n  resource_allocation: (details) =\u003e ({\n    description: `Allocate tasks to team members based on skills and availability.`,\n    expected_output: `A resource allocation chart with assignments and timeline.`,\n  }),\n};\n\n// Create agent\nconst agent = new AIAgent({\n  name: \"Sequential Thinking Agent\",\n  description: \"A sequential thinking agent\",\n  toolsConfigs: [Servers.sequentialThinking],\n});\n\n// Define crew members\nconst crew = {\n  planner: {\n    name: \"Project Planner\",\n    goal: \"Break down the project into actionable tasks\",\n    backstory: \"Experienced project manager with attention to detail\",\n    agent: agent,\n    model: openai(\"gpt-4o-mini\"),\n  },\n  estimator: {\n    name: \"Estimation Analyst\",\n    goal: \"Provide accurate time and resource estimations\",\n    backstory: \"Expert in project estimation with data-driven approach\",\n    agent: agent,\n    model: openai(\"gpt-4o-mini\"),\n  },\n  allocator: {\n    name: \"Resource Allocator\",\n    goal: \"Optimize task allocation among team members\",\n    backstory: \"Specialist in team dynamics and resource management\",\n    agent: agent,\n    model: openai(\"gpt-4o-mini\"),\n  },\n};\n\n// Define schema\nconst planSchema = z.object({\n  rationale: z.string(),\n  tasks: z.array(\n    z.object({\n      task_name: z.string(),\n      estimated_time_hours: z.number(),\n      required_resources: z.array(z.string()),\n      assigned_to: z.string(),\n      start_date: z.string(),\n      end_date: z.string(),\n    })\n  ),\n  milestones: z.array(\n    z.object({\n      milestone_name: z.string(),\n      tasks: z.array(z.string()),\n      deadline: z.string(),\n    })\n  ),\n  workload_distribution: z.record(z.number()),\n});\n\nasync function runWorkflow() {\n  // Execute planning task\n  const projectPlan = await CrewStyleHelpers.executeTask({\n    agent: crew.planner,\n    task: tasks.task_breakdown(projectDetails),\n  });\n  console.log(\"Project Plan:\", projectPlan.text);\n\n  // Execute estimation task\n  const timeEstimation = await CrewStyleHelpers.executeTask({\n    agent: crew.estimator,\n    task: tasks.time_estimation(projectDetails),\n    previousTasks: { projectPlan: projectPlan.text },\n  });\n  console.log(\"Time Estimation:\", timeEstimation.text);\n\n  // Execute allocation task\n  const resourceAllocation = await CrewStyleHelpers.executeTask({\n    agent: crew.allocator,\n    task: tasks.resource_allocation(projectDetails),\n    previousTasks: {\n      projectPlan: projectPlan.text,\n      timeEstimation: timeEstimation.text,\n    },\n    schema: planSchema,\n  });\n  console.log(\n    \"Resource Allocation:\",\n    JSON.stringify(resourceAllocation.object, null, 2)\n  );\n\n  // Cleanup\n  await agent.close();\n}\n\nrunWorkflow().catch(console.error);\n```\n\n### Custom Tools\n\nYou can easily add custom tools directly to your agent:\n\n```typescript\nimport { AIAgent } from \"mcp-ai-agent\";\nimport { openai } from \"@ai-sdk/openai\";\nimport { z } from \"zod\";\n\n// Create an agent with custom tools\nconst calculatorAgent = new AIAgent({\n  name: \"Calculator Agent\",\n  description: \"An agent that can perform mathematical operations\",\n  model: openai(\"gpt-4o-mini\"),\n  toolsConfigs: [\n    {\n      type: \"tool\",\n      name: \"multiply\",\n      description: \"Multiplies two numbers\",\n      parameters: z.object({\n        number1: z.number(),\n        number2: z.number(),\n      }),\n      execute: async (args) =\u003e {\n        return args.number1 * args.number2;\n      },\n    },\n    {\n      type: \"tool\",\n      name: \"add\",\n      description: \"Adds two numbers\",\n      parameters: z.object({\n        number1: z.number(),\n        number2: z.number(),\n      }),\n      execute: async (args) =\u003e {\n        return args.number1 + args.number2;\n      },\n    },\n  ],\n});\n\n// Use the agent with custom tools\nconst response = await calculatorAgent.generateResponse({\n  prompt: \"What is 125 * 37?\",\n});\nconsole.log(response.text);\n```\n\n## Supported MCP"])</script><script>self.__next_f.push([1," Servers\n\nMCP AI Agent comes with preconfigured support for the following servers:\n\n- **Sequential Thinking**: Use to break down complex problems into steps\n- **Memory**: Persistent memory for conversation context\n- **AWS KB Retrieval**: Retrieve information from AWS Knowledge Bases\n- **Brave Search**: Perform web searches using Brave Search API\n- **Everart**: Create and manipulate images using AI\n- **Fetch**: Retrieve data from URLs\n- **Firecrawl MCP**: Web crawling and retrieval capabilities\n- **SQLite**: Query and manipulate SQLite databases\n\n### Using Supported Servers\n\nYou can easily use any supported server by importing it from the `Servers` namespace:\n\n```typescript\nimport { AIAgent, Servers } from \"mcp-ai-agent\";\n\n// Use single server\nconst agent1 = new AIAgent({\n  name: \"Sequential Thinking Agent\",\n  description: \"Agent for sequential thinking\",\n  toolsConfigs: [Servers.sequentialThinking],\n});\n\n// Combine multiple servers\nconst agent2 = new AIAgent({\n  name: \"Multi-Tool Agent\",\n  description: \"Agent with multiple capabilities\",\n  toolsConfigs: [\n    Servers.sequentialThinking,\n    Servers.memory,\n    Servers.braveSearch,\n  ],\n});\n```\n\n### Contributing New Servers\n\nWe welcome contributions to add support for additional MCP servers! To add a new server:\n\n1. Create a new file in the `src/servers` directory following the existing patterns\n2. Export your server configuration in the file\n3. Add your server to the `src/servers/index.ts` exports\n4. Submit a pull request with your changes\n\nExample server configuration format:\n\n```typescript\nimport { MCPAutoConfig } from \"../types.js\";\n\nexport const yourServerName: MCPAutoConfig = {\n  type: \"auto\",\n  name: \"your-server-name\",\n  description: \"Description of what your server does\",\n  toolsDescription: {\n    toolName1: \"Description of first tool\",\n    toolName2: \"Description of second tool\",\n  },\n  parameters: {\n    API_KEY: {\n      description: \"API key for your service\",\n      required: true,\n    },\n  },\n  mcpConfig: {\n    command: \"npx\",\n    args: [\"-y\", \"@your-org/your-mcp-server-package\"],\n  },\n};\n```\n\n### Using Multiple Servers\n\nYou can initialize the agent with multiple servers:\n\n```typescript\nimport { AIAgent, Servers } from \"mcp-ai-agent\";\nimport { openai } from \"@ai-sdk/openai\";\n\n// Combine multiple preconfigured servers\nconst agent = new AIAgent({\n  name: \"Multi-Capability Agent\",\n  description: \"An agent with multiple specialized capabilities\",\n  toolsConfigs: [Servers.sequentialThinking, Servers.memory, Servers.fetch],\n});\n\nconst response = await agent.generateResponse({\n  prompt: \"What is 25 * 25?\",\n  model: openai(\"gpt-4o-mini\"),\n});\nconsole.log(response.text);\n```\n\n### Using Stdio Tools Manually\n\n```typescript\nimport { AIAgent } from \"mcp-ai-agent\";\nimport { openai } from \"@ai-sdk/openai\";\n\nconst agent = new AIAgent({\n  name: \"Custom Server Agent\",\n  description: \"Agent using a manually configured sequential thinking server\",\n  model: openai(\"gpt-4o-mini\"),\n  toolsConfigs: [\n    {\n      mcpServers: {\n        \"sequential-thinking\": {\n          command: \"npx\",\n          args: [\"-y\", \"@modelcontextprotocol/server-sequential-thinking\"],\n        },\n      },\n    },\n  ],\n});\n\nconst response = await agent.generateResponse({\n  prompt: \"What is 25 * 25?\",\n});\nconsole.log(response.text);\n```\n\n### Using SSE Transport\n\nYou can also use Server-Sent Events (SSE) transport for connecting to MCP servers:\n\n```typescript\nimport { AIAgent } from \"mcp-ai-agent\";\nimport { openai } from \"@ai-sdk/openai\";\n\nconst agent = new AIAgent({\n  name: \"SSE Transport Agent\",\n  description: \"Agent using SSE transport for remote server connection\",\n  model: openai(\"gpt-4o-mini\"),\n  toolsConfigs: [\n    {\n      mcpServers: {\n        \"sequential-thinking\": {\n          type: \"sse\",\n          url: \"https://your-mcp-server.com/sequential-thinking\",\n          headers: {\n            \"x-api-key\": \"your-api-key\",\n          },\n        },\n      },\n    },\n  ],\n});\n\nconst response = await agent.generateResponse({\n  prompt: \"What is 25 * 25?\",\n});\nconsole.log(response.text);\n```\n\n## Advanced Examples\n\n### Wor"])</script><script>self.__next_f.push([1,"king with Images\n\nYou can include images in your messages:\n\n```typescript\nimport { AIAgent, Servers } from \"mcp-ai-agent\";\nimport { openai } from \"@ai-sdk/openai\";\nimport fs from \"fs\";\n\nconst agent = new AIAgent({\n  name: \"Image Processing Agent\",\n  description: \"Agent capable of processing images\",\n  model: openai(\"gpt-4o-mini\"),\n  toolsConfigs: [Servers.sequentialThinking],\n});\n\nconst response = await agent.generateResponse({\n  messages: [\n    {\n      role: \"user\",\n      content: [\n        {\n          type: \"text\",\n          text: \"Use sequential thinking to solve the following equation\",\n        },\n        {\n          type: \"image\",\n          image: fs.readFileSync(\"./path/to/equation.png\"),\n        },\n      ],\n    },\n  ],\n});\nconsole.log(response.text);\nawait agent.close();\n```\n\n### Working with PDFs\n\nYou can also process PDFs:\n\n```typescript\nimport { AIAgent, Servers } from \"mcp-ai-agent\";\nimport { openai } from \"@ai-sdk/openai\";\nimport fs from \"fs\";\n\nconst agent = new AIAgent({\n  name: \"PDF Processing Agent\",\n  description: \"Agent capable of processing PDF documents\",\n  model: openai(\"gpt-4o-mini\"),\n  toolsConfigs: [Servers.sequentialThinking],\n});\n\nconst response = await agent.generateResponse({\n  messages: [\n    {\n      role: \"user\",\n      content: [\n        {\n          type: \"text\",\n          text: \"Use sequential thinking to solve the following equation\",\n        },\n        {\n          type: \"file\",\n          data: fs.readFileSync(\"./path/to/equation.pdf\"),\n          filename: \"equation.pdf\",\n          mimeType: \"application/pdf\",\n        },\n      ],\n    },\n  ],\n});\nconsole.log(response.text);\nawait agent.close();\n```\n\n### Mixing Preconfigured and Custom Server Configurations\n\nYou can combine preconfigured servers with manually configured servers:\n\n```typescript\nimport { AIAgent, Servers } from \"mcp-ai-agent\";\nimport { openai } from \"@ai-sdk/openai\";\n\n// Create an agent with both preconfigured and custom servers\nconst agent = new AIAgent({\n  name: \"Hybrid Configuration Agent\",\n  description:\n    \"Agent that combines preconfigured and custom server configurations\",\n  model: openai(\"gpt-4o\"),\n  toolsConfigs: [\n    // Use a preconfigured server from the Servers namespace\n    Servers.sequentialThinking,\n\n    // Add a manually configured server\n    {\n      mcpServers: {\n        \"custom-api-server\": {\n          type: \"sse\",\n          url: \"https://api.example.com/mcp-endpoint\",\n          headers: {\n            Authorization: `Bearer ${process.env.API_TOKEN}`,\n            \"Content-Type\": \"application/json\",\n          },\n        },\n      },\n    },\n\n    // Add another preconfigured server\n    Servers.memory,\n  ],\n});\n\nconst response = await agent.generateResponse({\n  prompt:\n    \"Search for information about AI agents and store the results in memory\",\n  // Optionally filter which tools to use\n  filterMCPTools: (tool) =\u003e {\n    // Only use specific tools from the available servers\n    return [\"sequentialThinking\", \"memory\", \"customApiSearch\"].includes(\n      tool.name\n    );\n  },\n});\n\nconsole.log(response.text);\nawait agent.close();\n```\n\n### Creating a Custom Auto-Configured Server\n\nYou can also create and use your own auto-configured server library definition:\n\n```typescript\nimport { AIAgent, MCPAutoConfig } from \"mcp-ai-agent\";\nimport { openai } from \"@ai-sdk/openai\";\n\n// Define a custom server configuration\nconst customVectorDB: MCPAutoConfig = {\n  type: \"auto\",\n  name: \"vector-db\",\n  description: \"Vector database tools for semantic search and retrieval\",\n  toolsDescription: {\n    vectorSearch:\n      \"Search for semantically similar documents in the vector database\",\n    vectorStore: \"Store documents in the vector database with embeddings\",\n    vectorDelete: \"Delete documents from the vector database\",\n  },\n  parameters: {\n    VECTOR_DB_API_KEY: {\n      description: \"API key for the vector database service\",\n      required: true,\n    },\n    VECTOR_DB_URL: {\n      description: \"URL of the vector database service\",\n      required: true,\n    },\n  },\n  mcpConfig: {\n    command: \"npx\",\n    args: [\"-y\", \"@your-org/vector-db-"])</script><script>self.__next_f.push([1,"mcp-server\"],\n    env: {\n      VECTOR_DB_API_KEY: process.env.VECTOR_DB_API_KEY || \"\",\n      VECTOR_DB_URL: process.env.VECTOR_DB_URL || \"\",\n    },\n  },\n};\n\n// Create an agent with the custom server and a preconfigured server\nconst agent = new AIAgent({\n  name: \"Vector DB Agent\",\n  description: \"Agent with vector database capabilities\",\n  model: openai(\"gpt-4o\"),\n  toolsConfigs: [customVectorDB, Servers.sequentialThinking],\n});\n\nconst response = await agent.generateResponse({\n  prompt:\n    \"Use sequential thinking to analyze this document and store it in the vector database\",\n  messages: [\n    {\n      role: \"user\",\n      content: [\n        {\n          type: \"text\",\n          text: \"Analyze this document and store the key points in the vector database\",\n        },\n        {\n          type: \"file\",\n          data: fs.readFileSync(\"./path/to/document.pdf\"),\n          filename: \"document.pdf\",\n          mimeType: \"application/pdf\",\n        },\n      ],\n    },\n  ],\n});\n\nconsole.log(response.text);\nawait agent.close();\n```\n\n## Configuration\n\nThe `AIAgent` constructor accepts a name, description, and multiple configuration objects:\n\n```typescript\n/**\n * Create a new AIAgent with a name, description, and one or more configurations\n * @param name - The name of the agent\n * @param description - A description of the agent's capabilities\n * @param configs - Configuration objects for various MCP servers and sub-agents\n */\nconstructor(name: string, description: string, ...configs: WorkflowConfig[])\n```\n\n### Auto Configuration\n\nThe `MCPAutoConfig` interface defines auto-configuration for MCP servers:\n\n```typescript\n/**\n * Auto configuration for MCP servers\n */\ninterface MCPAutoConfig {\n  /**\n   * Type identifier for auto configuration\n   */\n  type: \"auto\";\n\n  /**\n   * The name of the MCP server\n   */\n  name: string;\n\n  /**\n   * The description of the MCP server\n   */\n  description: string;\n\n  /**\n   * Optional GitHub repository URL for the MCP server\n   */\n  gitHubRepo?: string;\n\n  /**\n   * The license of the MCP server\n   */\n  license?: string;\n\n  /**\n   * Description of tools provided by the server\n   */\n  toolsDescription: Record\u003cstring, string\u003e;\n\n  /**\n   * Required parameters for the server\n   */\n  parameters: Record\u003cstring, { description: string; required: boolean }\u003e;\n\n  /**\n   * The configuration for the MCP server\n   */\n  mcpConfig: MCPServerConfig | MCPServerConfig[];\n}\n```\n\n### Standard Configuration\n\nThe `AIAgentConfig` interface defines the configuration for the AIAgent:\n\n```typescript\n/**\n * Full configuration for AIAgent\n * Contains configuration for all MCP servers\n */\ninterface AIAgentConfig {\n  /**\n   * Map of server names to their configurations\n   */\n  mcpServers: Record\u003cstring, MCPServerConfig | MCPAutoConfig\u003e;\n}\n\n/**\n * Union type for different MCP server configuration types\n */\ntype MCPServerConfig = StdioConfig | SSEConfig;\n\n/**\n * Configuration for running an MCP server via command line\n */\ninterface StdioConfig {\n  /**\n   * Command to execute\n   */\n  command: string;\n\n  /**\n   * Command line arguments\n   */\n  args?: string[];\n\n  /**\n   * Environment variables to set\n   */\n  env?: Record\u003cstring, string\u003e;\n\n  /**\n   * Standard error handling configuration\n   */\n  stderr?: IOType | Stream | number;\n\n  /**\n   * Working directory for the command\n   */\n  cwd?: string;\n}\n\n/**\n * Type for handling I/O redirection\n * - overlapped: Overlapped I/O mode\n * - pipe: Pipe I/O to parent process\n * - ignore: Ignore I/O\n * - inherit: Inherit I/O from parent process\n */\ntype IOType = \"overlapped\" | \"pipe\" | \"ignore\" | \"inherit\";\n\n/**\n * Configuration for connecting to an MCP server via Server-Sent Events\n */\ninterface SSEConfig {\n  /**\n   * Type identifier for SSE configuration\n   */\n  type: \"sse\";\n\n  /**\n   * URL to connect to\n   */\n  url: string;\n\n  /**\n   * HTTP headers to include in the request\n   */\n  headers?: Record\u003cstring, string\u003e;\n}\n```\n\n## API Types\n\nThe following types are used throughout the API:\n\n```typescript\n/**\n * Arguments for generating text using an AI model\n */\ninterface GenerateTextArgs {\n  /**\n   * Language mo"])</script><script>self.__next_f.push([1,"del to use\n   */\n  model?: LanguageModel;\n\n  /**\n   * Tools to make available to the model\n   */\n  tools?: TOOLS;\n\n  /**\n   * Tool selection configuration\n   */\n  toolChoice?: ToolChoice\u003cTOOLS\u003e;\n\n  /**\n   * Maximum number of steps to execute\n   */\n  maxSteps?: number;\n\n  /**\n   * System message to include in the prompt\n   * Can be used with `prompt` or `messages`\n   */\n  system?: string;\n\n  /**\n   * A simple text prompt\n   * You can either use `prompt` or `messages` but not both\n   */\n  prompt?: string;\n\n  /**\n   * A list of messages\n   * You can either use `prompt` or `messages` but not both\n   */\n  messages?: Array\u003cCoreMessage\u003e | Array\u003cOmit\u003cMessage, \"id\"\u003e\u003e;\n\n  /**\n   * Provider-specific options\n   */\n  providerOptions?: ProviderMetadata;\n\n  /**\n   * Callback function that is called when a step finishes\n   */\n  onStepFinish?: GenerateTextOnStepFinishCallback\u003cTOOLS\u003e;\n\n  /**\n   * Function to filter which MCP tools should be available\n   * @param tool The tool to evaluate\n   * @returns Boolean indicating whether to include the tool\n   */\n  filterMCPTools?: (tool: TOOLS) =\u003e boolean;\n}\n\n/**\n * Tool definition returned by MCP servers\n * Represents a callable function that the agent can use\n */\ninterface MCPTool {\n  /**\n   * Name of the tool\n   */\n  name: string;\n\n  /**\n   * Description of what the tool does\n   */\n  description: string;\n\n  /**\n   * Parameter schema for the tool\n   */\n  parameters: {\n    /**\n     * JSON Schema type\n     */\n    type: string;\n\n    /**\n     * Properties of the tool parameters\n     */\n    properties: Record\u003cstring, any\u003e;\n  };\n}\n\n/**\n * Type definition for a collection of tools\n * Maps tool names to their definitions\n */\ntype ToolSet = Record\u003cstring, MCPTool\u003e;\n\n/**\n * Generic type for tool configurations\n */\ntype TOOLS = Record\u003cstring, any\u003e;\n\n/**\n * AI response type to handle OpenAI API responses\n */\ntype MCPResponse = GenerateTextResult\u003cany, any\u003e;\n```\n\n## API\n\n### `AIAgent`\n\nThe main class that simplifies agent configuration and offers enhanced features.\n\n#### Constructor\n\n```typescript\nconstructor({\n  name,\n  description,\n  systemPrompt,\n  model,\n  verbose,\n  toolsConfigs\n}: {\n  name: string;\n  description: string;\n  systemPrompt?: string;\n  model?: LanguageModel;\n  verbose?: boolean;\n  toolsConfigs?: WorkflowConfig[];\n})\n```\n\n#### Methods\n\n##### `generateResponse(args: GenerateTextArgs): Promise\u003cGenerateTextResult\u003cTOOLS, any\u003e\u003e`\n\nGenerates a response using the AI model and the tools from the MCP servers and custom tools.\n\nArguments:\n\n- `prompt` - The user's message to respond to\n- `model` - Optional: Override the default model\n- `system` - Optional: Override the default system prompt\n- `maxSteps` - Maximum number of steps for the AI to take (default: 20)\n- `tools` - Additional tools to make available to the model\n- `filterMCPTools` - Optional function to filter which MCP tools to use\n- `onStepFinish` - Optional callback function that is called after each step\n- `messages` - Alternative to prompt, allows for rich content messages\n\n##### `generateObject\u003cT\u003e(args: GenerateObjectArgs): Promise\u003cGenerateObjectResult\u003cTOOLS, T\u003e\u003e`\n\nGenerates a structured object response using a schema.\n\nArguments:\n\n- `prompt` - The user's message\n- `model` - Optional: Override the default model\n- `schema` - Zod schema or JSON schema for the expected response object\n- `schemaName` - Optional name for the schema\n- `schemaDescription` - Optional description for the schema\n\n##### `close(): Promise\u003cvoid\u003e`\n\nCloses all server connections.\n\n##### `getInfo(): { name: string; description: string; tools: string[]; agents?: string[]; model?: LanguageModel; }`\n\nReturns information about the agent, including its name, description, available tools, agents, and default model.\n\n## License\n\nMIT41:T5ec,## what is MCP AI Agent? \nMCP AI Agent is a TypeScript library that enables AI agents to leverage Model Context Protocol (MCP) servers for enhanced capabilities, integrating seamlessly with AI SDKs for AI-powered applications.\n\n## how to use MCP AI Agent? \nTo use MCP AI Agent, install it via npm with `npm install mcp-ai-agent`, then import and "])</script><script>self.__next_f.push([1,"initialize the agent with preconfigured servers or custom configurations to generate responses using AI models.\n\n## key features of MCP AI Agent? \n- Connect to multiple MCP servers using various transport methods (STDIO, SSE)\n- Automatic discovery and usage of tools from MCP servers\n- Integration with AI SDK for text generation with tool usage\n- Support for filtering and combining MCP tools with custom tools\n- Preconfigured servers for easy initialization and auto-configuration support\n\n## use cases of MCP AI Agent? \n1. Building AI-powered applications that require complex problem-solving capabilities.\n2. Integrating various AI tools and services into a single application.\n3. Creating multi-agent workflows for enhanced AI interactions.\n\n## FAQ from MCP AI Agent? \n- Can MCP AI Agent connect to any MCP server?\n\u003e Yes! MCP AI Agent can connect to multiple MCP servers using different transport methods.\n\n- Is MCP AI Agent free to use?\n\u003e Yes! MCP AI Agent is open-source and available for free.\n\n- How do I contribute to MCP AI Agent?\n\u003e Contributions are welcome! You can add support for new MCP servers by following the contribution guidelines in the repository.42:T4590,![Minions Logo](assets/Ollama_minionS_background.png)\n\n# Where On-Device and Cloud LLMs Meet\n\n[![Discord](https://img.shields.io/badge/Discord-7289DA?logo=discord\u0026logoColor=white)](https://discord.gg/jfJyxXwFVa)\n\n_What is this?_ Minions is a communication protocol that enables small on-device models to collaborate with frontier models in the cloud. By only reading long contexts locally, we can reduce cloud costs with minimal or no quality degradation. This repository provides a demonstration of the protocol. Get started below or see our paper and blogpost below for more information.\n\nPaper: [Minions: Cost-efficient Collaboration Between On-device and Cloud\nLanguage Models](https://arxiv.org/pdf/2502.15964)\n\nMinions Blogpost: https://hazyresearch.stanford.edu/blog/2025-02-24-minions\n\nSecure Minions Chat Blogpost: https://hazyresearch.stanford.edu/blog/2025-05-12-security\n\n## Table of Contents\n\n\u003e **Looking for Secure Minions Chat?** If you're interested in our end-to-end encrypted and chat system, please see the [Secure Minions Chat README](secure/README.md) for detailed setup and usage instructions.\n\n- [Setup](#setup)\n  - [Step 1: Clone and Install](#step-1-clone-the-repository-and-install-the-python-package)\n  - [Step 2: Install a Local Model Server](#step-2-install-a-server-for-running-the-local-model)\n  - [Step 3: Set Cloud LLM API Keys](#step-3-set-your-api-key-for-at-least-one-of-the-following-cloud-llm-providers)\n- [Minions Demo Application](#minions-demo-application)\n- [Minions WebGPU App](#minions-webgpu-app)\n- [Example Code](#example-code-minion-singular)\n  - [Minion (Singular)](#example-code-minion-singular)\n  - [Minions (Plural)](#example-code-minions-plural)\n- [Python Notebook](#python-notebook)\n- [Docker Support](#docker-support)\n- [Command Line Interface](#cli)\n- [Secure Minions Local-Remote Protocol](#secure-minions-local-remote-protocol)\n- [Secure Minions Chat](#secure-minions-chat)\n- [Inference Estimator](#inference-estimator)\n  - [Command Line Usage](#command-line-usage)\n  - [Python API Usage](#python-api-usage)\n- [Miscellaneous Setup](#miscellaneous-setup)\n  - [Using Azure OpenAI](#using-azure-openai-with-minions)\n- [Maintainers](#maintainers)\n\n## Setup\n\n_We have tested the following setup on Mac and Ubuntu with Python 3.10-3.11_ (Note: Python 3.13 is not supported)\n\n\u003cdetails\u003e\n  \u003csummary\u003eOptional: Create a virtual environment with your favorite package manager (e.g. conda, venv, uv)\u003c/summary\u003e\n\n```python\nconda create -n minions python=3.11\n```\n\n\u003c/details\u003e\u003cbr\u003e\n\n**Step 1:** Clone the repository and install the Python package.\n\n```bash\ngit clone https://github.com/HazyResearch/minions.git\ncd minions\npip install -e .  # installs the minions package in editable mode\n```\n\n_note_: for optional MLX-LM install the package with the following command:\n\n```bash\npip install -e \".[mlx]\"\n```\n\n_note_: for secure minions chat, install the package with the following"])</script><script>self.__next_f.push([1," command:\n\n```bash\npip install -e \".[secure]\"\n```\n\n_note_: for optional Cartesia-MLX install, pip install the basic package and then follow the instructions below.\n\n**Step 2:** Install a server for running the local model.\n\nWe support two servers for running local models: `ollama` and `tokasaurus`. You need to install at least one of these.\n\n- You should use `ollama` if you do not have access to NVIDIA GPUs. Install `ollama` following the instructions [here](https://ollama.com/download). To enable Flash Attention, run\n  `launchctl setenv OLLAMA_FLASH_ATTENTION 1` and, if on a mac, restart the ollama app.\n- You should use `tokasaurus` if you have access to NVIDIA GPUs and you are running the Minions protocol, which benefits from the high-throughput of `tokasaurus`. Install `tokasaurus` with the following command:\n\n```\nuv pip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ tokasaurus==0.0.1.post1\n```\n\n\u003cdetails\u003e\n  \u003csummary\u003eOptional: Install Cartesia-MLX (only available on Apple Silicon)\u003c/summary\u003e\n\n1. Download XCode\n2. Install the command line tools by running `xcode-select --install`\n3. Install the Nanobind🧮\n\n```\npip install nanobind@git+https://github.com/wjakob/nanobind.git@2f04eac452a6d9142dedb957701bdb20125561e4\n```\n\n4. Install the Cartesia Metal backend by running the following command:\n\n```\npip install git+https://github.com/cartesia-ai/edge.git#subdirectory=cartesia-metal\n```\n\n5. Install the Cartesia-MLX package by running the following command:\n\n```\npip install git+https://github.com/cartesia-ai/edge.git#subdirectory=cartesia-mlx\n```\n\n\u003c/details\u003e\u003cbr\u003e\n\n\u003cdetails\u003e\n    \u003csummary\u003eOptional: Install llama-cpp-python\u003c/summary\u003e\n\n# Installation\n\nFirst, install the llama-cpp-python package:\n\n```bash\n# CPU-only installation\npip install llama-cpp-python\n\n# For Metal on Mac (Apple Silicon/Intel)\nCMAKE_ARGS=\"-DGGML_METAL=on\" pip install llama-cpp-python\n\n# For CUDA on NVIDIA GPUs\nCMAKE_ARGS=\"-DGGML_CUDA=on\" pip install llama-cpp-python\n\n# For OpenBLAS CPU optimizations\nCMAKE_ARGS=\"-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS\" pip install llama-cpp-python\n```\n\nFor more installation options, see the [llama-cpp-python documentation](https://llama-cpp-python.readthedocs.io/en/latest/).\n\n## Basic Usage\n\nThe client follows the basic pattern from the llama-cpp-python library:\n\n```python\nfrom minions.clients import LlamaCppClient\n\n# Initialize the client with a local model\nclient = LlamaCppClient(\n    model_path=\"/path/to/model.gguf\",\n    chat_format=\"chatml\",     # Most modern models use \"chatml\" format\n    n_gpu_layers=35           # Set to 0 for CPU-only inference\n)\n\n# Run a chat completion\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What's the capital of France?\"}\n]\n\nresponses, usage, done_reasons = client.chat(messages)\nprint(responses[0])  # The generated response\n```\n\n## Loading Models from Hugging Face\n\nYou can easily load models directly from Hugging Face:\n\n```python\nclient = LlamaCppClient(\n    model_path=\"dummy\",  # Will be replaced by downloaded model\n    model_repo_id=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n    model_file_pattern=\"*Q4_K_M.gguf\",  # Optional - specify quantization\n    chat_format=\"chatml\",\n    n_gpu_layers=35     # Offload 35 layers to GPU\n)\n```\n\n\u003c/details\u003e\u003cbr\u003e\n\n**Step 3:** Set your API key for at least one of the following cloud LLM providers.\n\n_If needed, create an [OpenAI API Key](https://platform.openai.com/docs/overview) or [TogetherAI API key](https://docs.together.ai/docs/quickstart) or [DeepSeek API key](https://platform.deepseek.com/api_keys) for the cloud model._\n\n```bash\n# OpenAI\nexport OPENAI_API_KEY=\u003cyour-openai-api-key\u003e\nexport OPENAI_BASE_URL=\u003cyour-openai-base-url\u003e  # Optional: Use a different OpenAI API endpoint\n\n# Together AI\nexport TOGETHER_API_KEY=\u003cyour-together-api-key\u003e\n\n# OpenRouter\nexport OPENROUTER_API_KEY=\u003cyour-openrouter-api-key\u003e\nexport OPENROUTER_BASE_URL=\u003cyour-openrouter-base-url\u003e  # Optional: Use a different OpenRouter API endpoint\n\n# Perplexity\nexport PERPLEXITY_API_KEY"])</script><script>self.__next_f.push([1,"=\u003cyour-perplexity-api-key\u003e\nexport PERPLEXITY_BASE_URL=\u003cyour-perplexity-base-url\u003e  # Optional: Use a different Perplexity API endpoint\n\n# Tokasaurus\nexport TOKASAURUS_BASE_URL=\u003cyour-tokasaurus-base-url\u003e  # Optional: Use a different Tokasaurus API endpoint\n\n# DeepSeek\nexport DEEPSEEK_API_KEY=\u003cyour-deepseek-api-key\u003e\n\n# Anthropic\nexport ANTHROPIC_API_KEY=\u003cyour-anthropic-api-key\u003e\n\n# Mistral AI\nexport MISTRAL_API_KEY=\u003cyour-mistral-api-key\u003e\n```\n\n## Minions Demo Application\n\n[![Watch the video](https://img.youtube.com/vi/70Kot0_DFNs/0.jpg)](https://www.youtube.com/watch?v=70Kot0_DFNs)\n\nTo try the Minion or Minions protocol, run the following commands:\n\n```bash\npip install torch transformers\n\nstreamlit run app.py\n```\n\nIf you are seeing an error about the `ollama` client,\n\n```\nAn error occurred: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download\n```\n\ntry running the following command:\n\n```bash\nOLLAMA_FLASH_ATTENTION=1 ollama serve\n```\n\n## Minions WebGPU App\n\nThe Minions WebGPU app demonstrates the Minions protocol running entirely in the browser using WebGPU for local model inference and cloud APIs for supervision. This approach eliminates the need for local server setup while providing a user-friendly web interface.\n\n### Features\n\n- **Browser-based**: Runs entirely in your web browser with no local server required\n- **WebGPU acceleration**: Uses WebGPU for fast local model inference\n- **Model selection**: Choose from multiple pre-optimized models from [MLC AI](https://mlc.ai/models)\n- **Real-time progress**: See model loading progress and conversation logs in real-time\n- **Privacy-focused**: Your API key and data never leave your browser\n\n### Quick Start\n\n1. **Navigate to the WebGPU app directory:**\n   ```bash\n   cd apps/minions-webgpu\n   ```\n\n2. **Install dependencies:**\n   ```bash\n   npm install\n   ```\n\n3. **Start the development server:**\n   ```bash\n   npm start\n   ```\n\n4. **Open your browser** and navigate to the URL shown in the terminal (typically `http://localhost:5173`)\n\n## Example code: Minion (singular)\n\nThe following example is for an `ollama` local client and an `openai` remote client.\nThe protocol is `minion`.\n\n```python\nfrom minions.clients.ollama import OllamaClient\nfrom minions.clients.openai import OpenAIClient\nfrom minions.minion import Minion\n\nlocal_client = OllamaClient(\n        model_name=\"llama3.2\",\n    )\n\nremote_client = OpenAIClient(\n        model_name=\"gpt-4o\",\n    )\n\n# Instantiate the Minion object with both clients\nminion = Minion(local_client, remote_client)\n\n\ncontext = \"\"\"\nPatient John Doe is a 60-year-old male with a history of hypertension. In his latest checkup, his blood pressure was recorded at 160/100 mmHg, and he reported occasional chest discomfort during physical activity.\nRecent laboratory results show that his LDL cholesterol level is elevated at 170 mg/dL, while his HDL remains within the normal range at 45 mg/dL. Other metabolic indicators, including fasting glucose and renal function, are unremarkable.\n\"\"\"\n\ntask = \"Based on the patient's blood pressure and LDL cholesterol readings in the context, evaluate whether these factors together suggest an increased risk for cardiovascular complications.\"\n\n# Execute the minion protocol for up to two communication rounds\noutput = minion(\n    task=task,\n    context=[context],\n    max_rounds=2\n)\n```\n\n## Example Code: Minions (plural)\n\nThe following example is for an `ollama` local client and an `openai` remote client.\nThe protocol is `minions`.\n\n```python\nfrom minions.clients.ollama import OllamaClient\nfrom minions.clients.openai import OpenAIClient\nfrom minions.minions import Minions\nfrom pydantic import BaseModel\n\nclass StructuredLocalOutput(BaseModel):\n    explanation: str\n    citation: str | None\n    answer: str | None\n\nlocal_client = OllamaClient(\n        model_name=\"llama3.2\",\n        temperature=0.0,\n        structured_output_schema=StructuredLocalOutput\n)\n\nremote_client = OpenAIClient(\n        model_name=\"gpt-4o\",\n)\n\n\n# Instantiate the Minion object with both clients\nminion"])</script><script>self.__next_f.push([1," = Minions(local_client, remote_client)\n\n\ncontext = \"\"\"\nPatient John Doe is a 60-year-old male with a history of hypertension. In his latest checkup, his blood pressure was recorded at 160/100 mmHg, and he reported occasional chest discomfort during physical activity.\nRecent laboratory results show that his LDL cholesterol level is elevated at 170 mg/dL, while his HDL remains within the normal range at 45 mg/dL. Other metabolic indicators, including fasting glucose and renal function, are unremarkable.\n\"\"\"\n\ntask = \"Based on the patient's blood pressure and LDL cholesterol readings in the context, evaluate whether these factors together suggest an increased risk for cardiovascular complications.\"\n\n# Execute the minion protocol for up to two communication rounds\noutput = minion(\n    task=task,\n    doc_metadata=\"Medical Report\",\n    context=[context],\n    max_rounds=2\n)\n```\n\n## Python Notebook\n\nTo run Minion/Minions in a notebook, checkout `minions.ipynb`.\n\n## Docker support\n\n### Build the Docker Image\n\n```bash\ndocker build -t minions .\n```\n\n### Run the container\n\n```bash\n#without GPU support\ndocker run -p 8501:8501 --env OPENAI_API_KEY=\u003cyour-openai-api-key\u003e --env DEEPSEEK_API_KEY=\u003cyour-deepseek-api-key\u003e minions\n#with GPU support\ndocker run --gpus all -p 8501:8501 --env OPENAI_API_KEY=\u003cyour-openai-api-key\u003e --env DEEPSEEK_API_KEY=\u003cyour-deepseek-api-key\u003e minions\n```\n\n## CLI\n\nTo run Minion/Minions in a CLI, checkout `minions_cli.py`.\n\nSet your choice of local and remote models by running the following command. The format is `\u003cprovider\u003e/\u003cmodel_name\u003e`. Choice of providers are `ollama`, `openai`, `anthropic`, `together`, `perplexity`, `openrouter`, `groq`, and `mlx`.\n\n```bash\nexport MINIONS_LOCAL=ollama/llama3.2\nexport MINIONS_REMOTE=openai/gpt-4o\n```\n\n```bash\nminions --help\n```\n\n```bash\nminions --context \u003cpath_to_context\u003e --protocol \u003cminion|minions\u003e\n```\n\n## Secure Minions Local-Remote Protocol\n\nThe Secure Minions Local-Remote Protocol (`secure/minions_secure.py`) provides an end-to-end encrypted implementation of the Minions protocol that enables secure communication between a local worker model and a remote supervisor server. This protocol includes attestation verification, perfect forward secrecy, and replay protection.\n\n\n### Prerequisites\n\nInstall the secure dependencies:\n\n```bash\npip install -e \".[secure]\"\n```\n\n### Basic Usage\n\n#### Python API\n\n```python\nfrom minions.clients import OllamaClient\nfrom secure.minions_secure import SecureMinionProtocol\n\n# Initialize local client\nlocal_client = OllamaClient(model_name=\"llama3.2\")\n\n# Create secure protocol instance\nprotocol = SecureMinionProtocol(\n    supervisor_url=\"https://your-supervisor-server.com\",\n    local_client=local_client,\n    max_rounds=3,\n    system_prompt=\"You are a helpful AI assistant.\"\n)\n\n# Run a secure task\nresult = protocol(\n    task=\"Analyze this document for key insights\",\n    context=[\"Your document content here\"],\n    max_rounds=2\n)\n\nprint(f\"Final Answer: {result['final_answer']}\")\nprint(f\"Session ID: {result['session_id']}\")\nprint(f\"Log saved to: {result['log_file']}\")\n\n# Clean up the session\nprotocol.end_session()\n```\n\n#### Command Line Interfacec\n\n```bash\npython secure/minions_secure.py \\\n    --supervisor_url https://your-supervisor-server.com \\\n    --local_client_type ollama \\\n    --local_model llama3.2 \\\n    --max_rounds 3\n```\n\n## Secure Minions Chat\n\nTo install secure minions chat, install the package with the following command:\n\n```bash\npip install -e \".[secure]\"\n```\n\nSee the [Secure Minions Chat README](secure/README.md) for additional details on how to setup clients and run the secure chat protocol.\n\n## Inference Estimator\n\nMinions provides a utility to estimate LLM inference speed on your hardware. The inference estimator helps you:\n\n1. Analyze your hardware capabilities (GPU, MPS, or CPU)\n2. Calculate peak performance for your models\n3. Estimate tokens per second and completion time\n\n### Command Line Usage\n\nRun the estimator directly from the command line to check how fast a model will run:\n\n```bash\npython -m minions.utils.inference_estimator --mo"])</script><script>self.__next_f.push([1,"del llama3.2 --tokens 1000 --describe\n```\n\nArguments:\n\n- `--model`: Model name from the supported model list (e.g., llama3.2, mistral7b)\n- `--tokens`: Number of tokens to estimate generation time for\n- `--describe`: Show detailed hardware and model performance statistics\n- `--quantized`: Specify that the model is quantized\n- `--quant-bits`: Quantization bit-width (4, 8, or 16)\n\n### Python API Usage\n\nYou can also use the inference estimator in your Python code:\n\n```python\nfrom minions.utils.inference_estimator import InferenceEstimator\n\n# Initialize the estimator for a specific model\nestimator = InferenceEstimator(\n    model_name=\"llama3.2\",  # Model name\n    is_quant=True,          # Is model quantized?\n    quant_bits=4            # Quantization level (4, 8, 16)\n)\n\n# Estimate performance for 1000 tokens\ntokens_per_second, estimated_time = estimator.estimate(1000)\nprint(f\"Estimated speed: {tokens_per_second:.1f} tokens/sec\")\nprint(f\"Estimated time: {estimated_time:.2f} seconds\")\n\n# Get detailed stats\ndetailed_info = estimator.describe(1000)\nprint(detailed_info)\n\n# Calibrate with your actual model client for better accuracy\n# (requires a model client that implements a chat() method)\nestimator.calibrate(my_model_client, sample_tokens=32, prompt=\"Hello\")\n```\n\nThe estimator uses a roofline model that considers both compute and memory bandwidth limitations and applies empirical calibration to improve accuracy. The calibration data is cached at `~/.cache/ie_calib.json` for future use.\n\n## Miscellaneous Setup\n\n### Using Azure OpenAI with Minions\n\n#### Set Environment Variables\n\n```bash\nexport AZURE_OPENAI_API_KEY=your-api-key\nexport AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com/\nexport AZURE_OPENAI_API_VERSION=2024-02-15-preview\n```\n\n#### Example Code\n\nHere's an example of how to use Azure OpenAI with the Minions protocol in your own code:\n\n```python\nfrom minions.clients.ollama import OllamaClient\nfrom minions.clients.azure_openai import AzureOpenAIClient\nfrom minions.minion import Minion\n\nlocal_client = OllamaClient(\n    model_name=\"llama3.2\",\n)\n\nremote_client = AzureOpenAIClient(\n    model_name=\"gpt-4o\",  # This should match your deployment name\n    api_key=\"your-api-key\",\n    azure_endpoint=\"https://your-resource-name.openai.azure.com/\",\n    api_version=\"2024-02-15-preview\",\n)\n\n# Instantiate the Minion object with both clients\nminion = Minion(local_client, remote_client)\n```\n\n## Maintainers\n\n- Avanika Narayan (contact: avanika@cs.stanford.edu)\n- Dan Biderman (contact: biderman@stanford.edu)\n- Sabri Eyuboglu (contact: eyuboglu@cs.stanford.edu)43:T4da,## What is Minions? \nMinions is a communication protocol that enables small on-device models to collaborate with frontier models in the cloud, optimizing cloud costs while maintaining quality.\n\n## How to use Minions? \nTo use Minions, clone the repository, install the required Python packages, set up a local model server, and run the demo application using Streamlit.\n\n## Key features of Minions? \n- Collaboration between on-device and cloud language models\n- Cost-efficient processing by minimizing cloud usage\n- Support for multiple local model servers (ollama, tokasaurus)\n\n## Use cases of Minions? \n1. Reducing cloud costs for AI applications\n2. Enhancing performance of language models by leveraging local processing\n3. Facilitating complex tasks that require both local and cloud resources\n\n## FAQ from Minions? \n- What is the purpose of Minions?  \n\u003e Minions allows for efficient collaboration between local and cloud models, reducing costs and improving performance.\n\n- Is there a specific setup required?  \n\u003e Yes, it requires Python 3.10-3.11 and specific local model servers depending on your hardware.\n\n- Can I use Minions with any cloud provider?  \n\u003e Yes, Minions supports various cloud providers, including OpenAI and TogetherAI.44:Td42,# Osmosis-MCP-4B\n\n## [Blog Post](https://osmosis.ai/blog/applying-rl-mcp)\n\n## Prerequisites\n\n*   Python 3.x\n*   uvx\n*   Access to a model server (e.g., running locally via vLLM/lm studio)\n*   API Keys for:\n    *   Brave Search\n    *   "])</script><script>self.__next_f.push([1,"Google Maps\n    *   AccuWeather\n\n## Setup \u0026 Installation\n\n0. **Install LM studio [here](https://lmstudio.ai) and use it to run the model from a http endpoint**\n   1. find and download model in discover    \n   \u003cimg width=\"944\" alt=\"image\" src=\"https://github.com/user-attachments/assets/7a379cc3-fe3f-4e4c-bc0f-9f17f753c849\" /\u003e\n   2. select model to load and load an osmosis model\n   \u003cimg width=\"430\" alt=\"image\" src=\"https://github.com/user-attachments/assets/3d32833d-dcae-4f91-bb3f-47dd23e0570c\" /\u003e\n   3. start the web server with model loaded\n   \u003cimg width=\"1130\" alt=\"image\" src=\"https://github.com/user-attachments/assets/51d4fd79-75dc-4808-8dd7-3b9f8c43ff77\" /\u003e\n\n\n2.  **Clone the repository (if applicable):**\n    ```bash\n    git clone https://github.com/Gulp-AI/Osmosis-MCP-4B-demo\n    cd Osmosis-MCP-4B-demo\n    ```\n\n3.  **Install dependencies:**\n    It's recommended to use a virtual environment.\n    ```bash\n    python -m venv .venv\n    source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n    pip install -r requirements.txt\n    ```\n\n4.  **Set up Environment Variables:**\n    Create a `.env` file in the root of the project directory and add your API keys and model server configuration:\n    ```env\n    BRAVE_API_KEY=\"your_brave_api_key\"\n    GOOGLE_MAPS_API_KEY=\"your_google_maps_api_key\"\n    ACCUWEATHER_API_KEY=\"your_accuweather_api_key\"\n    APP_STYLE=\"gui\" # or \"tui\"\n    ```\n    if an api key is not provided, the tool will not be loaded.\n    If `APP_STYLE` is omitted, the application defaults to GUI mode.\n    The `main.py` script currently configures the model server URL directly. Ensure `http://localhost:1234/v1` is the correct endpoint for your Qwen model server (this is what lm studio uses).\n\n5.  **Serve local model:**\n    Use a tool like lm studio to provide a usable endpoint.\n\n## Environment Variables\n\nThe application uses the following environment variables (loaded from a `.env` file):\n\n*   `BRAVE_API_KEY`: Your API key for Brave Search.\n*   `GOOGLE_MAPS_API_KEY`: Your API key for Google Maps.\n*   `ACCUWEATHER_API_KEY`: Your API key for AccuWeather.\n*   `APP_STYLE`: 'tui' or 'gui'\n## Available Tools (MCP Servers)\n\nThe agent is configured to use the following tools via MCP:\n\n*   **Time:** Provides current time information.\n*   **Brave Search:** Enables web search capabilities.\n    *   Requires: `BRAVE_API_KEY`\n*   **Fetch:** Fetches content from URLs.\n*   **Google Maps:** Provides location-based services.\n    *   Requires: `GOOGLE_MAPS_API_KEY`\n*   **Weather:** Provides weather forecasts.\n    *   Requires: `ACCUWEATHER_API_KEY`\n*   **Code Interpreter:** A built-in tool for executing Python code snippets.\n\nThese servers need to be running and accessible for the agent to utilize their respective functionalities. The `main.py` script provides the commands to start these MCP servers.\n\n## How to Run Graphical User Interface (GUI):\n    This mode launches a web-based interface for interacting with the agent. This is the default mode.\n    ```bash\n    python main.py\n    ```\n\n    The web UI will be accessible at the address provided by the `WebUI` component upon startup (on `http://localhost:7860`).\n\n---45:T55e,## What is Osmosis-MCP-4B? \nOsmosis-MCP-4B is an open-source machine learning project designed for multi-channel processing (MCP) using a trained model.\n\n## How to use Osmosis-MCP-4B? \nTo use Osmosis-MCP-4B, clone the repository, install the required dependencies, set up your environment variables with necessary API keys, and run the application using the provided scripts.\n\n## Key features of Osmosis-MCP-4B? \n- Integration with various APIs for enhanced functionality (Brave Search, Google Maps, AccuWeather).\n- Supports both graphical user interface (GUI) and terminal user interface (TUI).\n- Built-in tools for fetching content, providing weather forecasts, and executing Python code snippets.\n\n## Use cases of Osmosis-MCP-4B? \n1. Conducting web searches using Brave Search.\n2. Fetching location-based data through Google Maps.\n3. Accessing real-time weather information from AccuWeather.\n\n## FAQ from Osmosis-MCP-4B? \n- Wh"])</script><script>self.__next_f.push([1,"at are the prerequisites for using Osmosis-MCP-4B?  \n\u003e You need Python 3.x, uvx, and access to a model server along with API keys for Brave Search, Google Maps, and AccuWeather.\n\n- Is there a graphical interface available?  \n\u003e Yes, you can run the application in GUI mode for easier interaction.\n\n- How do I set up the environment variables?  \n\u003e Create a `.env` file in the project directory and add your API keys and model server configuration.46:Ta30,# Groq Desktop\n\n[![Latest macOS Build](https://img.shields.io/github/v/release/groq/groq-desktop-beta?include_prereleases\u0026label=latest%20macOS%20.dmg%20build)](https://github.com/groq/groq-desktop-beta/releases/latest)\n\nGroq Desktop features MCP server support for all function calling capable models hosted on Groq. Now available for Windows, macOS, and Linux!\n\n\u003e **Note for macOS Users**: After installing on macOS, you may need to run this command to open the app:\n\u003e ```sh\n\u003e xattr -c /Applications/Groq\\ Desktop.app\n\u003e ```\n\n\u003cimg width=\"450\" alt=\"Screenshot 2025-04-14 at 11 53 18 PM\" src=\"https://github.com/user-attachments/assets/300abf8c-8b7f-4ef8-a5f9-174f93e39506\" /\u003e\u003cimg width=\"450\" alt=\"Screenshot 2025-04-14 at 11 53 35 PM\" src=\"https://github.com/user-attachments/assets/61641680-5b3d-4ca9-8da4-8e84779f97bb\" /\u003e\n\n## Unofficial Homebrew Installation (macOS)\n\nYou can install the latest release using [Homebrew](https://brew.sh/) via an unofficial tap:\n\n```sh\nbrew tap ricklamers/groq-desktop-unofficial\nbrew install --cask groq-desktop\n# Allow the app to run\nxattr -c /Applications/Groq\\ Desktop.app\n```\n\n## Features\n\n- Chat interface with image support\n- Local MCP servers\n\n## Prerequisites\n\n- Node.js (v18+)\n- pnpm package manager\n\n## Setup\n\n1. Clone this repository\n2. Install dependencies:\n   ```\n   pnpm install\n   ```\n3. Start the development server:\n   ```\n   pnpm dev\n   ```\n\n## Building for Production\n\nTo build the application for production:\n\n```\npnpm dist\n```\n\nThis will create installable packages in the `release` directory for your current platform.\n\n### Building for Specific Platforms\n\n```bash\n# Build for all supported platforms\npnpm dist\n\n# Build for macOS only\npnpm dist:mac\n\n# Build for Windows only\npnpm dist:win\n\n# Build for Linux only\npnpm dist:linux\n```\n\n### Testing Cross-Platform Support\n\nThis app now supports Windows, macOS, and Linux. Here's how to test cross-platform functionality:\n\n#### Running Cross-Platform Tests\n\nWe've added several test scripts to verify platform support:\n\n```bash\n# Run all platform tests (including Docker test for Linux)\npnpm test:platforms\n\n# Run basic path handling test only\npnpm test:paths\n\n# If on Windows, run the PowerShell test script\n.\\test-windows.ps1\n```\n\nThe testing scripts will check:\n- Platform detection\n- Script file resolution\n- Environment variable handling\n- Path separators\n- Command resolution\n\n## Configuration\n\nIn the settings page, add your Groq API key:\n\n```json\n{\n  \"GROQ_API_KEY\": \"your-api-key-here\"\n}\n```\n\nYou can obtain a Groq API key by signing up at [https://console.groq.com](https://console.groq.com).47:T485,## what is Groq Desktop? \nGroq Desktop is a local chat application that supports MCP (Model Control Protocol) for all function calling capable models hosted on Groq, available for Windows, macOS, and Linux.\n\n## how to use Groq Desktop? \nTo use Groq Desktop, install the application on your preferred operating system, configure it with your Groq API key, and start chatting with image support.\n\n## key features of Groq Desktop? \n- Chat interface with image support\n- Local MCP server functionality\n- Cross-platform support for Windows, macOS, and Linux\n\n## use cases of Groq Desktop? \n1. Engaging in real-time chat with support for images.\n2. Utilizing local MCP servers for enhanced performance.\n3. Collaborating across different operating systems seamlessly.\n\n## FAQ from Groq Desktop? \n- Is Groq Desktop available for all operating systems?\n\u003e Yes! Groq Desktop is available for Windows, macOS, and Linux.\n\n- How do I install Groq Desktop on macOS?\n\u003e You can install it using Homebrew or by downloading the latest release fro"])</script><script>self.__next_f.push([1,"m GitHub.\n\n- What do I need to run Groq Desktop?\n\u003e You need Node.js (v18+) and the pnpm package manager to set up the application.48:T168e,# Systemprompt Multimodal MCP Client\n\n\u003cdiv align=\"center\"\u003e\n   \n[![Discord](https://img.shields.io/discord/1255160891062620252?color=7289da\u0026label=discord)](https://discord.com/invite/wkAbSuPWpr)\n[![Twitter Follow](https://img.shields.io/twitter/follow/tyingshoelaces_?style=social)](https://twitter.com/tyingshoelaces_)\n[![Linkedin](https://i.sstatic.net/gVE0j.png)](https://www.linkedin.com/in/edjburton/)\n\n[Website](https://systemprompt.io) • [Documentation](https://systemprompt.io/documentation) • [Blog](https://tyingshoelaces.com) • [Get API Key](https://systemprompt.io/console)\n\n\u003c/div\u003e\n\nA modern voice-controlled AI interface powered by Google Gemini and Anthropic MCP (Model Control Protocol). Transform how you interact with AI through natural speech and multimodal inputs.\n\n\u003e **⚠️ Important Note**: This open source project is currently in development and in early access. It is not currently compatible with Safari but has been tested on Chrome with Linux, Windows, and MacOS. If you have any problems, please let us know on Discord or GitHub.\n\nIf you find this project useful, please consider:\n\n- ⭐ Starring it on GitHub\n- 🔄 Sharing it with others\n- 💬 Joining our [Discord community](https://discord.com/invite/wkAbSuPWpr)\n\n## 🌟 Overview\n\nA modern Vite + TypeScript application that enables voice-controlled AI workflows through MCP (Model Control Protocol). This project revolutionizes how you interact with AI systems by combining Google Gemini's multimodal capabilities with MCP's extensible tooling system.\n\nThe Client supports both custom (user provided and configured) and Systemprompt MCP servers. Systemprompt MCP servers can be installed through the UX with a Systemprompt API key (free).\n\nCustom MCP servers are not pre-configured and require a custom configuration file. \n\nCreate a local file `mcp.config.custom.json` in the `config` directory and add your MCP server configuration.\n\n```json\n{\n  \"mcpServers\": {\n    \"my-custom-server\": {\n      \"id\": \"my-custom-server\",\n      \"env\": {\n        \"xxx\": \"xxx\"\n      },\n      \"command\": \"node\",\n      \"args\": [\n        \"/my-custom-server/build/index.js\"\n      ]\n    }\n  }\n}\n```\n\n## 🎯 Why Systemprompt MCP?\n\nTransform your AI interactions with a powerful voice-first interface that combines:\n\n| Feature                             | Description                                                                     |\n| ----------------------------------- | ------------------------------------------------------------------------------- |\n| 🗣️ **Multimodal AI**                | Understand and process text, voice, and visual inputs naturally                 |\n| 🛠️ **MCP (Model Control Protocol)** | Execute complex AI workflows with a robust tooling system                       |\n| 🎙️ **Voice-First Design**           | Control everything through natural speech, making AI interaction more intuitive |\n\n**Perfect for**: Developers building voice-controlled AI applications and looking for innovative ways to use multimodal AI.\n\n## ✨ Core Features\n\n### 🎙️ Voice \u0026 Multimodal Intelligence\n\n- **Natural Voice Control**: Speak naturally to control AI workflows and execute commands\n- **Multimodal Understanding**: Process text, voice, and visual inputs simultaneously\n- **Real-time Voice Synthesis**: Get instant audio responses from your AI interactions\n\n### 🔄 AI Workflow Orchestration\n\n- **Extensible Tool System**: Add custom tools and workflows through MCP\n- **Workflow Automation**: Chain multiple AI operations with voice commands\n- **State Management**: Robust handling of complex, multi-step AI interactions\n\n### 💻 Developer Experience\n\n- **Modern Tech Stack**: Built with Vite, React, TypeScript, and NextUI\n- **Type Safety**: Full TypeScript support with comprehensive type definitions\n- **Hot Module Replacement**: Fast development with instant feedback\n- **Comprehensive Testing**: Built-in testing infrastructure with high cove"])</script><script>self.__next_f.push([1,"rage\n\n## 🚀 Getting Started\n\n### Prerequisites\n\n- Node.js 16.x or higher\n- npm 7.x or higher\n- A modern browser with Web Speech API support\n\n### Quick Start\n\n1. **Clone the repository**\n\n   ```bash\n   git clone https://github.com/Ejb503/multimodal-mcp-client.git\n   cd multimodal-mcp-client\n   ```\n\n2. **Install dependencies**\n\n   ```bash\n   npm install\n   cd proxy\n   npm install\n   ```\n\n3. **Configure the application**\n\n   ```bash\n   # Navigate to config directory\n   cd config\n\n   # Create local configuration files\n   cp mcp.config.example.json mcp.config.custom.json\n   ```\n\n   Required API Keys:\n\n   - [Google AI Studio](https://ai.google.dev/gemini-api/docs) - Gemini API key\n   - [systemprompt.io/console](https://systemprompt.io/console) - Systemprompt API key\n\n   Add keys to `.env` (see `.env.example` for reference). note that the `VITE_` prefix is required to share the keys with the MCP server and client.\n\n4. **Start development server**\n   ```bash\n   npm run dev\n   ```\n   Access the development server at `http://localhost:5173`\n\n## 🤝 Support \u0026 Community\n\n| Resource   | Link                                                                    |\n| ---------- | ----------------------------------------------------------------------- |\n| 💬 Discord | [Join our community](https://discord.com/invite/wkAbSuPWpr)             |\n| 🐛 Issues  | [GitHub Issues](https://github.com/Ejb503/multimodal-mcp-client/issues) |\n| 📚 Docs    | [Documentation](https://systemprompt.io/documentation)                  |\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 🔮 Future Development\n\nWe're actively working on expanding the capabilities of Systemprompt MCP Client with exciting new features and extensions. Stay tuned for updates!49:T5ee,## What is Systemprompt Multimodal MCP Client? \nSystemprompt Multimodal MCP Client is a modern voice-controlled AI interface that enables agentic workflows through natural speech and multimodal inputs, powered by Google Gemini and the Model Control Protocol (MCP).\n\n## How to use Systemprompt Multimodal MCP Client? \nTo use the client, clone the repository, install dependencies, configure the application with necessary API keys, and start the development server to access the interface.\n\n## Key features of Systemprompt Multimodal MCP Client? \n- **Natural Voice Control**: Control AI workflows using natural speech.\n- **Multimodal Understanding**: Process text, voice, and visual inputs simultaneously.\n- **Extensible Tool System**: Add custom tools and workflows through MCP.\n- **Real-time Voice Synthesis**: Get instant audio responses from AI interactions.\n\n## Use cases of Systemprompt Multimodal MCP Client? \n1. Building voice-controlled AI applications.\n2. Automating complex AI workflows with voice commands.\n3. Enhancing user interaction with AI through multimodal inputs.\n\n## FAQ from Systemprompt Multimodal MCP Client? \n- **Is the project compatible with all browsers?**  \n\u003e No, it is currently not compatible with Safari but works on Chrome with Linux, Windows, and MacOS.\n\n- **Is there a community for support?**  \n\u003e Yes, you can join the community on Discord for support and discussions.\n\n- **What are the prerequisites for using the client?**  \n\u003e You need Node.js 16.x or higher and npm 7.x or higher.4a:Td90,\u003cdiv align=\"center\"\u003e\n\n# `nerve`\n\n\u003ci\u003eThe Simple Agent Development Kit\u003c/i\u003e\n\n[![Documentation](https://img.shields.io/badge/documentation-blue)](https://github.com/evilsocket/nerve/blob/main/docs/index.md)\n[![Release](https://img.shields.io/github/release/evilsocket/nerve.svg?style=flat-square)](https://github.com/evilsocket/nerve/releases/latest)\n[![Package](https://img.shields.io/pypi/v/nerve-adk.svg)](https://pypi.org/project/nerve-adk)\n[![Docker](https://img.shields.io/docker/v/evilsocket/nerve?logo=docker)](https://hub.docker.com/r/evilsocket/nerve)\n[![CI](https://img.shields.io/github/actions/workflow/status/evilsocket/nerve/ci.yml)](https://github.com/evilsocket/nerve/actions/workflows/ci.yml)\n[![License](https://img.shields.io/bad"])</script><script>self.__next_f.push([1,"ge/license-GPL3-brightgreen.svg?style=flat-square)](https://github.com/evilsocket/nerve/blob/master/LICENSE.md)\n\n  \u003csmall\u003eJoin the project community on our server!\u003c/small\u003e\n  \u003cbr/\u003e\u003cbr/\u003e\n  \u003ca href=\"https://discord.gg/https://discord.gg/btZpkp45gQ\" target=\"_blank\" title=\"Join our community!\"\u003e\n    \u003cimg src=\"https://dcbadge.limes.pink/api/server/https://discord.gg/btZpkp45gQ\"/\u003e\n  \u003c/a\u003e\n\n\u003c/div\u003e\n\nNerve is a simple yet powerful Agent Development Kit (ADK) to build, run, evaluate, and orchestrate LLM-based agents using just YAML and a CLI. It’s designed for technical users who want programmable, auditable, and reproducible automation using large language models.\n\n## Key Features\n\n**📝 Declarative Agents**\n\nDefine agents using a clean YAML format: system prompt, task, tools, and variables — all in one file.\n\n**🔧 Built-in Tools \u0026 Extensibility**\n\nUse shell commands, Python functions, or remote tools to power your agents. Tools are fully typed and annotated.\n\n**🌐 Native MCP Support (Client \u0026 Server)**  \n\nNerve is the first framework to let you define **MCP servers in YAML** — and act as both **client and server**, enabling agent teams and [deep orchestration](https://github.com/evilsocket/nerve/blob/main/docs/mcp.md).\n\n**📊 Evaluation Mode**  \n\n[Benchmark your agents](https://github.com/evilsocket/nerve/blob/main/docs/evaluation.md) with YAML, Parquet, or folder-based test cases. Run reproducible tests, log structured outputs, and track regression or progress. \n\n**🔁 Workflows**  \n\nCompose agents into simple, linear pipelines to create multi-step automations with shared context.\n\n**🧪 LLM-Agnostic**  \n\nBuilt on [LiteLLM](https://docs.litellm.ai/), Nerve supports OpenAI, Anthropic, Ollama, [and dozens more](https://docs.litellm.ai/docs/providers) — switch models in one line.\n\n## Quick Start\n\n```bash\n# 🖥️ install the project with:\npip install nerve-adk\n\n# ⬇️ download and install an agent from a github repo with:\nnerve install evilsocket/changelog\n\n# 💡 or create an agent with a guided procedure:\nnerve create new-agent\n\n# 🚀 go!\nnerve run new-agent\n```\n\nRead the [documentation](https://github.com/evilsocket/nerve/blob/main/docs/index.md) and the [examples](https://github.com/evilsocket/nerve/tree/main/examples) for more.\n\n## Contributing\n\nWe welcome contributions! Check out our [contributing guidelines](https://github.com/evilsocket/nerve/blob/main/CONTRIBUTING.md) to get started and join our [Discord community](https://discord.gg/btZpkp45gQ) for help and discussion.\n\n## License\n\nNerve is released under the GPL 3 license.\n\n[![Star History Chart](https://api.star-history.com/svg?repos=evilsocket/nerve\u0026type=Date)](https://star-history.com/#evilsocket/nerve\u0026Date)4b:T43b,## what is Nerve? \nNerve is an Agent Development Kit (ADK) designed to create and execute LLM-based agents easily and effectively.\n\n## how to use Nerve? \nTo use Nerve, install it via pip, create an agent using a guided procedure, and run the agent using the command line interface (CLI).\n\n## key features of Nerve? \n- Define agents using simple YAML files.\n- User-friendly CLI for creating, installing, and running agents.\n- Library of predefined tools for common tasks.\n- Integration with various MCP servers and custom tool creation.\n- Support for any model provider.\n\n## use cases of Nerve? \n1. Automating system tasks using shell commands.\n2. Creating custom agents for specific workflows.\n3. Integrating with various model providers for enhanced functionality.\n\n## FAQ from Nerve? \n- Can I create custom agents?  \n\u003e Yes! You can define agents as YAML files and customize them as needed.\n\n- Is Nerve free to use?  \n\u003e Yes! Nerve is open-source and released under the GPL 3 license.\n\n- How do I get support?  \n\u003e You can join the Nerve community on Discord for help and discussions.4c:T1d36,# MCP CLI client\n\nA simple CLI program to run LLM prompt and implement [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) client.\n\nYou can use any [MCP-compatible servers](https://github.com/punkpeye/awesome-mcp-servers) from the convenience of your termi"])</script><script>self.__next_f.push([1,"nal.\n\nThis act as alternative client beside Claude Desktop. Additionally you can use any LLM provider like OpenAI, Groq, or local LLM model via [llama](https://github.com/ggerganov/llama.cpp).\n\n![C4 Diagram](https://raw.githubusercontent.com/adhikasp/mcp-client-cli/refs/heads/master/c4_diagram.png)\n\n## Setup\n\n1. Install via pip:\n   ```bash\n   pip install mcp-client-cli\n   ```\n\n2. Create a `~/.llm/config.json` file to configure your LLM and MCP servers:\n   ```json\n   {\n     \"systemPrompt\": \"You are an AI assistant helping a software engineer...\",\n     \"llm\": {\n       \"provider\": \"openai\",\n       \"model\": \"gpt-4\",\n       \"api_key\": \"your-openai-api-key\",\n       \"temperature\": 0.7,\n       \"base_url\": \"https://api.openai.com/v1\"  // Optional, for OpenRouter or other providers\n     },\n     \"mcpServers\": {\n       \"fetch\": {\n         \"command\": \"uvx\",\n         \"args\": [\"mcp-server-fetch\"],\n         \"requires_confirmation\": [\"fetch\"],\n         \"enabled\": true,  // Optional, defaults to true\n         \"exclude_tools\": []  // Optional, list of tool names to exclude\n       },\n       \"brave-search\": {\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"@modelcontextprotocol/server-brave-search\"],\n         \"env\": {\n           \"BRAVE_API_KEY\": \"your-brave-api-key\"\n         },\n         \"requires_confirmation\": [\"brave_web_search\"]\n       },\n       \"youtube\": {\n         \"command\": \"uvx\",\n         \"args\": [\"--from\", \"git+https://github.com/adhikasp/mcp-youtube\", \"mcp-youtube\"]\n       }\n     }\n   }\n   ```\n\n   Note: \n   - Use `requires_confirmation` to specify which tools need user confirmation before execution\n   - The LLM API key can also be set via environment variables `LLM_API_KEY` or `OPENAI_API_KEY`\n   - The config file can be placed in either `~/.llm/config.json` or `$PWD/.llm/config.json`\n   - You can comment the JSON config file with `//` if you like to switch around the configuration\n\n3. Run the CLI:\n   ```bash\n   llm \"What is the capital city of North Sumatra?\"\n   ```\n\n## Usage\n\n### Basic Usage\n\n```bash\n$ llm What is the capital city of North Sumatra?\nThe capital city of North Sumatra is Medan.\n```\n\nYou can omit the quotes, but be careful with bash special characters like `\u0026`, `|`, `;` that might be interpreted by your shell.\n\nYou can also pipe input from other commands or files:\n\n```bash\n$ echo \"What is the capital city of North Sumatra?\" | llm\nThe capital city of North Sumatra is Medan.\n\n$ echo \"Given a location, tell me its capital city.\" \u003e instructions.txt\n$ cat instruction.txt | llm \"West Java\"\nThe capital city of West Java is Bandung.\n```\n\n### Image Input\n\nYou can pipe image files to analyze them with multimodal LLMs:\n\n```bash\n$ cat image.jpg | llm \"What do you see in this image?\"\n[LLM will analyze and describe the image]\n\n$ cat screenshot.png | llm \"Is there any error in this screenshot?\"\n[LLM will analyze the screenshot and point out any errors]\n```\n\n### Using Prompt Templates\n\nYou can use predefined prompt templates by using the `p` prefix followed by the template name and its arguments:\n\n```bash\n# List available prompt templates\n$ llm --list-prompts\n\n# Use a template\n$ llm p review  # Review git changes\n$ llm p commit  # Generate commit message\n$ llm p yt url=https://youtube.com/...  # Summarize YouTube video\n```\n\n### Triggering a tool\n\n```bash\n$ llm What is the top article on hackernews today?\n\n================================== Ai Message ==================================\nTool Calls:\n  brave_web_search (call_eXmFQizLUp8TKBgPtgFo71et)\n Call ID: call_eXmFQizLUp8TKBgPtgFo71et\n  Args:\n    query: site:news.ycombinator.com\n    count: 1\nBrave Search MCP Server running on stdio\n\n# If the tool requires confirmation, you'll be prompted:\nConfirm tool call? [y/n]: y\n\n================================== Ai Message ==================================\nTool Calls:\n  fetch (call_xH32S0QKqMfudgN1ZGV6vH1P)\n Call ID: call_xH32S0QKqMfudgN1ZGV6vH1P\n  Args:\n    url: https://news.ycombinator.com/\n================================= Tool Message =================================\nName: fetch\n\n[TextContent(type='text', text='Contents [REDACTED]]\n"])</script><script>self.__next_f.push([1,"================================== Ai Message ==================================\n\nThe top article on Hacker News today is:\n\n### [Why pipes sometimes get \"stuck\": buffering](https://jvns.ca)\n- **Points:** 31\n- **Posted by:** tanelpoder\n- **Posted:** 1 hour ago\n\nYou can view the full list of articles on [Hacker News](https://news.ycombinator.com/)\n```\n\nTo bypass tool confirmation requirements, use the `--no-confirmations` flag:\n\n```bash\n$ llm --no-confirmations \"What is the top article on hackernews today?\"\n```\n\nTo use in bash scripts, add the --no-intermediates, so it doesn't print intermediate messages, only the concluding end message.\n```bash\n$ llm --no-intermediates \"What is the time in Tokyo right now?\"\n```\n\n### Continuation\n\nAdd a `c ` prefix to your message to continue the last conversation.\n\n```bash\n$ llm asldkfjasdfkl\nIt seems like your message might have been a typo or an error. Could you please clarify or provide more details about what you need help with?\n$ llm c what did i say previously?\nYou previously typed \"asldkfjasdfkl,\" which appears to be a random string of characters. If you meant to ask something specific or if you have a question, please let me know!\n```\n\n### Clipboard Support\n\nYou can use content from your clipboard using the `cb` command:\n\n```bash\n# After copying text to clipboard\n$ llm cb\n[LLM will process the clipboard text]\n\n$ llm cb \"What language is this code written in?\"\n[LLM will analyze the clipboard text with your question]\n\n# After copying an image to clipboard\n$ llm cb \"What do you see in this image?\"\n[LLM will analyze the clipboard image]\n\n# You can combine it with continuation\n$ llm cb c \"Tell me more about what you see\"\n[LLM will continue the conversation about the clipboard content]\n```\n\nThe clipboard feature works in:\n- Native Windows/macOS/Linux environments\n  - Windows: Uses PowerShell\n  - macOS: Uses `pbpaste` for text, `pngpaste` for images (optional)\n  - Linux: Uses `xclip` (required for clipboard support)\n- Windows Subsystem for Linux (WSL)\n  - Accesses the Windows clipboard through PowerShell\n  - Works with both text and images\n  - Make sure you have access to `powershell.exe` from WSL\n\nRequired tools for clipboard support:\n- Windows: PowerShell (built-in)\n- macOS: \n  - `pbpaste` (built-in) for text\n  - `pngpaste` (optional) for images: `brew install pngpaste`\n- Linux: \n  - `xclip`: `sudo apt install xclip` or equivalent\n\nThe CLI automatically detects if the clipboard content is text or image and handles it appropriately.\n\n### Additional Options\n\n```bash\n$ llm --list-tools                # List all available tools\n$ llm --list-prompts              # List available prompt templates\n$ llm --no-tools                  # Run without any tools\n$ llm --force-refresh             # Force refresh tool capabilities cache\n$ llm --text-only                 # Output raw text without markdown formatting\n$ llm --show-memories             # Show user memories\n$ llm --model gpt-4               # Override the model specified in config\n```\n\n## Contributing\n\nFeel free to submit issues and pull requests for improvements or bug fixes.4d:T4fd,## What is MCP CLI Client? \nMCP CLI Client is a command-line interface (CLI) tool designed to run prompts for large language models (LLMs) and implement the Model Context Protocol (MCP) client.\n\n## How to use MCP CLI Client? \nTo use the MCP CLI Client, install it via pip, configure your LLM and MCP servers in a JSON file, and run commands directly from your terminal.\n\n## Key features of MCP CLI Client? \n- Supports multiple LLM providers including OpenAI and local models.\n- Allows image input for analysis with multimodal LLMs.\n- Provides predefined prompt templates for common tasks.\n- Clipboard support for processing text and images directly from the clipboard.\n\n## Use cases of MCP CLI Client? \n1. Running queries to retrieve information from LLMs.\n2. Analyzing images and screenshots for insights.\n3. Generating summaries or reviews using predefined templates.\n\n## FAQ from MCP CLI Client? \n- Can I use any LLM provider with MCP CLI Client?  \n\u003e Yes! You can use any "])</script><script>self.__next_f.push([1,"MCP-compatible server and various LLM providers.\n\n- Is there a way to bypass tool confirmation?  \n\u003e Yes, you can use the `--no-confirmations` flag to bypass confirmation prompts.\n\n- How do I configure the client?  \n\u003e You need to create a `~/.llm/config.json` file with your LLM and MCP server settings.4e:Tc20,\\# Playwright MCP Server\r\n\r\nA Model Context Protocol server that provides browser automation capabilities using Playwright. This server enables LLMs to interact with web pages, take screenshots, and execute JavaScript in a real browser environment.\r\n\r\n## Screenshot\r\n!\\[Playwright + Claude\\](image/playwright\\_claude.png)\r\n\r\n## Installation\r\n\r\nYou can install the package using either npm or mcp-get:\r\n\r\nUsing npm:\r\n\\`\\`\\`bash\r\nnpm install -g @executeautomation/playwright-mcp-server\r\n\\`\\`\\`\r\n\r\nUsing mcp-get:\r\n\\`\\`\\`bash\r\nnpx @michaellatman/mcp-get@latest install @executeautomation/playwright-mcp-server\r\n\\`\\`\\`\r\n\r\n## Building from Source\r\n\r\nIf you want to build the code from source:\r\n\r\n\\*\\*Clone the repository\\*\\*\r\n\\`\\`\\`bash\r\ngit clone https://github.com/executeautomation/mcp-playwright.git\r\n\\`\\`\\`\r\n\r\n\\*\\*Build the code\\*\\*\r\nFirst run npm install\r\n\r\n\\`\\`\\`bash\r\nnpm install\r\n\\`\\`\\`\r\nThen run build and link\r\n\r\n\\`\\`\\`bash\r\nnpm run build\r\nnpm link\r\n\\`\\`\\`\r\n\r\n## Configuration to use Playwright Server\r\nHere's the Claude Desktop configuration to use the Playwright server:\r\n\r\n\\`\\`\\`json\r\n{\r\n  \"mcpServers\": {\r\n    \"playwright\": {\r\n      \"command\": \"npx\",\r\n      \"args\": \\[\"-y\", \"@executeautomation/playwright-mcp-server\"\\]\r\n    }\r\n  }\r\n}\r\n\\`\\`\\`\r\n\r\n\r\n## Components\r\n\r\n### Tools\r\n\r\n- \\*\\*playwright\\_navigate\\*\\*\r\n  - Navigate to any URL in the browser\r\n  - Input: \\`url\\` (string)\r\n\r\n- \\*\\*playwright\\_screenshot\\*\\*\r\n  - Capture screenshots of the entire page or specific elements\r\n  - Inputs:\r\n    - \\`name\\` (string, required): Name for the screenshot\r\n    - \\`selector\\` (string, optional): CSS selector for element to screenshot\r\n    - \\`width\\` (number, optional, default: 800): Screenshot width\r\n    - \\`height\\` (number, optional, default: 600): Screenshot height\r\n\r\n- \\*\\*playwright\\_click\\*\\*\r\n  - Click elements on the page\r\n  - Input: \\`selector\\` (string): CSS selector for element to click\r\n\r\n- \\*\\*playwright\\_hover\\*\\*\r\n  - Hover elements on the page\r\n  - Input: \\`selector\\` (string): CSS selector for element to hover\r\n\r\n- \\*\\*playwright\\_fill\\*\\*\r\n  - Fill out input fields\r\n  - Inputs:\r\n    - \\`selector\\` (string): CSS selector for input field\r\n    - \\`value\\` (string): Value to fill\r\n\r\n- \\*\\*playwright\\_select\\*\\*\r\n  - Select an element with SELECT tag\r\n  - Inputs:\r\n    - \\`selector\\` (string): CSS selector for element to select\r\n    - \\`value\\` (string): Value to select\r\n\r\n- \\*\\*playwright\\_evaluate\\*\\*\r\n  - Execute JavaScript in the browser console\r\n  - Input: \\`script\\` (string): JavaScript code to execute\r\n\r\n### Resources\r\n\r\nThe server provides access to two types of resources:\r\n\r\n1. \\*\\*Console Logs\\*\\* (\\`console://logs\\`)\r\n   - Browser console output in text format\r\n   - Includes all console messages from the browser\r\n\r\n2. \\*\\*Screenshots\\*\\* (\\`screenshot://\u003cname\\\u003e\\`)\r\n   - PNG images of captured screenshots\r\n   - Accessible via the screenshot name specified during capture\r\n\r\n## Key Features\r\n\r\n- Browser automation\r\n- Console log monitoring\r\n- Screenshot capabilities\r\n- JavaScript execution\r\n- Basic web interaction (navigation, clicking, form filling)4f:T58e,## what is Playwright MCP? \nPlaywright MCP is a Model Context Protocol server that facilitates browser automation and web scraping using Playwright, allowing LLMs to interact with web pages, take screenshots, and execute JavaScript in a real browser environment.\n\n## how to use Playwright MCP? \nTo use Playwright MCP, install it via npm or mcp-get. Configuration is required to connect to the server and use the provided tools for browser interactions.\n\n## key features of Playwright MCP? \n- Automated browser navigation and interactions \n- Screenshot capture of full pages or specific elements \n- Console log monitoring from the browser \n- JavaScript code execution \n- Inte"])</script><script>self.__next_f.push([1,"raction capabilities such as clicking, hovering, and filling input fields \n\n## use cases of Playwright MCP? \n1. Automating web testing for applications \n2. Scraping data from websites for analysis \n3. Taking screenshots of webpages for documentation \n4. Executing scripts within the browser environment for dynamic content \n\n## FAQ from Playwright MCP? \n- What technologies does Playwright MCP support? \n\u003e It supports all major browsers like Chromium, Firefox, and WebKit.\n\n- Can I run multiple scripts concurrently with Playwright MCP? \n\u003e Yes, you can run multiple instances simultaneously to handle various tasks.\n\n- Is there a limit on the number of screenshots I can take? \n\u003e No, you can take as many screenshots as your server resources allow.50:Tabd,# Ollama MCP (Model Context Protocol)\n\nOllama MCP is a tool for connecting Ollama-based language models with external tools and services using the Model Context Protocol (MCP). This integration enables LLMs to interact with various systems like Git repositories, shell commands, and other tool-enabled services.\n\n## Features\n\n- Seamless integration between Ollama language models and MCP servers\n- Support for Git operations through MCP Git server\n- Extensible tool management system\n- Interactive command-line assistant interface\n- Interactive Ollama model selection at startup from available local models\n\n## Installation\n\n1. Ensure you have Python 3.13+ installed\n2. Clone this repository\n3. Install dependencies:\n\n```bash\n# Create a virtual environment\nuv add ruff check\n# Activate the virtual environment\nsource .venv/bin/activate\n# Install the package in development mode\nuv pip install -e .\n```\n\n## Usage\n\n### Ollama Model Selection\n\nBefore the main application starts, you will be prompted to select an Ollama model to use.\n\n1.  **Prerequisites**:\n    *   Ensure Ollama is installed and running.\n    *   You must have at least one model pulled locally (e.g., via `ollama pull llama3.1:8b`). A list of models that support tool usage can be found on the [Ollama website](https://ollama.com/search?c=tools).\n\n2.  **Startup Process**:\n    *   The application will automatically detect and list all Ollama models available on your local machine.\n    *   You will be prompted to type the name of the model you wish to use from the displayed list.\n    *   If you enter an invalid model name, you will be prompted again until a valid selection is made.\n    *   The chosen model will then be used by the agent for all subsequent operations.\n\n### Running the Git Assistant\n\n```bash\nuv run main.py\n```\n\n### To run tests\n```bash\npytest -xvs tests/test_ollama_toolmanager.py\n```\n\nThis will start an interactive CLI where you can ask the assistant to perform Git operations.\n\n### Extending with Custom Tools\n\nYou can extend the system by:\n\n1. Creating new tool wrappers\n2. Registering them with the `OllamaToolManager`\n3. Connecting to different MCP servers\n\n## Components\n\n- **OllamaToolManager**: Manages tool registrations and execution\n- **MCPClient**: Handles communication with MCP servers\n- **OllamaAgent**: Orchestrates Ollama LLM and tool usage\n\n## Examples\n\n```python\n# Creating a Git-enabled agent\ngit_params = StdioServerParameters(\n    command=\"uvx\",\n    args=[\"mcp-server-git\", \"--repository\", \"/path/to/repo\"],\n    env=None\n)\n\n# Connect and register tools\nasync with MCPClient(git_params) as client:\n    # Register tools with the agent\n    # Use the agent for Git operations\n```\n\n## Requirements\n\n- Python 3.13+\n- MCP 1.5.0+\n- Ollama 0.4.7+51:T579,## What is Ollama MCP? \nOllama MCP (Model Control Protocol) is a tool designed to connect Ollama-based language models with external tools and services, enabling interaction through the Model Control Protocol (MCP). This allows LLMs to work with systems like Git repositories and shell commands.\n\n## How to use Ollama MCP? \nTo use Ollama MCP, clone the repository, install the necessary dependencies, and run the interactive command-line assistant to perform Git operations. Ensure to replace the repository path in the configuration file before running the project.\n\n## Key features"])</script><script>self.__next_f.push([1," of Ollama MCP? \n- Seamless integration with Ollama language models and MCP servers\n- Support for Git operations via MCP Git server\n- Extensible tool management system\n- Interactive command-line assistant interface\n\n## Use cases of Ollama MCP? \n1. Managing Git repositories through a language model interface.\n2. Automating shell commands using LLMs.\n3. Extending functionality with custom tools for various applications.\n\n## FAQ from Ollama MCP? \n- What are the system requirements for Ollama MCP?\n\u003e Python 3.13+ and MCP 1.5.0+ are required to run Ollama MCP.\n\n- Can I extend Ollama MCP with my own tools?\n\u003e Yes! You can create new tool wrappers and register them with the OllamaToolManager.\n\n- Is there a command-line interface for Ollama MCP?\n\u003e Yes, Ollama MCP provides an interactive CLI for performing operations.52:T9fa,# DrissionPage MCP Server -- 骚神出品\n\n基于DrissionPage和FastMCP的浏览器自动化MCP服务器，提供丰富的浏览器操作API供AI调用。\n\n## 项目简介\n![logo](img/DrissionPageMCP-logo.png)\n\nDrissionPage MCP  是一个基于 DrissionPage 和 FastMCP 的浏览器自动化MCP server服务器，它提供了一系列强大的浏览器操作 API，让您能够轻松通过AI实现网页自动化操作。\n\n### 主要特性\n\n- 支持浏览器的打开、关闭和连接管理\n- 提供丰富的页面元素操作方法\n- 支持 JavaScript 代码执行\n- 支持 CDP 协议操作\n- 提供便捷的文件下载功能\n- 支持键盘按键模拟\n- 支持页面截图功能\n- 增加 网页后台监听数据包的功能\n- 增加自动上传下载文件功能\n\n#### Python要求\n- Python \u003e= 3.9\n- pip（最新版本）\n- uv （最新版本）\n\n\n#### 浏览器要求\n- Chrome 浏览器（推荐 90 及以上版本）\n\n\n#### 必需的Python包\n- drissionpage \u003e= 4.1.0.18\n- fastmcp \u003e= 2.4.0\n- uv\n\n## 安装说明\n把本仓库git clone到本地，核心文件是main.py：\n\n### 安装到Cursor编辑器\n\n![安装说明](img/install_to_Cursor1.png)\n![安装说明](img/install_to_cursor2.png)\n\n### 安装到vscode编辑器\n\n![安装说明](img/install_to_vscode0.png)\n![安装说明](img/install_to_vscode1.png)\n![安装说明](img/install_to_vscode2.png)\n\n\n请将以下配置代码粘贴到编辑器的`mcpServers`设置中（请填写`你自己电脑上 main.py 文件的绝对路径`）：\n\n```json\n{\n  \"mcpServers\": {\n    \"DrssionPageMCP\": {\n      \"type\": \"stdio\",\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"D:\\\\test10\\\\DrssionPageMCP\\\\main.py\"\n      ]\n    }\n  }\n}\n```\n新增mcp配置 ，填写下面的配置：\n``` json\n\"DrssionPageMCP\": {\n      \"type\": \"stdio\",\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"D:\\\\test10\\\\DrssionPageMCP\\\\main.py\"\n      ]\n    } \n```\n\n注意事项：\n- 请根据实际路径修改`args`中的路径\n- Windows中路径中的反斜杠需要转义（使用`\\\\`）\n- 确保`uv`命令在系统PATH中可用\n- [《MCP安装参考教程》](https://docs.trae.ai/ide/model-context-protocol)\n\n\n## 调试命令\n\n调试\n```\nnpx -y @modelcontextprotocol/inspector uv run D:\\\\test10\\\\DrssionPageMCP\\\\main.py\n```\n或者\n```\nmcp dev  D:\\\\test10\\\\DrssionPageMCP\\\\main.py\n```\n\n## 更新日志\n### v0.1.3\n增加 自动上传下载文件功能\n### v0.1.2\n增加 网页后台监听数据包的功能\n\n### v0.1.0\n\n- 初始版本发布\n- 实现基本的浏览器控制功能\n- 提供元素操作 API53:T57a,## what is DrissionPage MCP? \nDrissionPage MCP is a browser automation server based on DrissionPage and FastMCP, providing a rich set of browser operation APIs for AI calls.\n\n## how to use DrissionPage MCP? \nTo use DrissionPage MCP, clone the repository to your local machine and run the main.py file using the specified command in your editor's configuration.\n\n## key features of DrissionPage MCP? \n- Supports browser opening, closing, and connection management\n- Provides extensive page element operation methods\n- Supports JavaScript code execution\n- Compatible with CDP protocol operations\n- Offers convenient file download functionality\n- Simulates keyboard key presses\n- Supports page screenshot functi"])</script><script>self.__next_f.push([1,"onality\n- Includes background data packet listening for web pages\n- Automates file upload and download functionality\n\n## use cases of DrissionPage MCP? \n1. Automating web scraping tasks\n2. Testing web applications through automated browser interactions\n3. Monitoring web page changes and data extraction\n\n## FAQ from DrissionPage MCP? \n- What are the system requirements for DrissionPage MCP?\n\u003e Requires Python \u003e= 3.9 and Chrome browser (recommended version 90 and above).\n\n- Is DrissionPage MCP free to use?\n\u003e Yes! DrissionPage MCP is open-source and free to use.\n\n- How can I contribute to DrissionPage MCP?\n\u003e You can contribute by submitting issues or pull requests on the GitHub repository.54:Td76,# tinyagents\nTinyAgents: LLM + MCP Tools\n\n**TinyAgents** is a minimalist implementation of agents powered by LLMs and [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) tools.\n\nThis project is inspired by the [MCP Client Quickstart](https://modelcontextprotocol.io/quickstart/client#python) and provides a lightweight foundation for building LLM-based agent workflows.\n\n## Agent Implementations\n\nThe repository includes two different agent implementations:\n\n### TinyToolCallingAgent\n\n`TinyToolCallingAgent` is a general-purpose agent that can solve tasks by calling external tools. It:\n\n- Connects to Python or JavaScript MCP servers\n- Processes user queries using the Qwen2.5-Coder-32B-Instruct model\n- Dynamically discovers and calls tools provided by the MCP server\n- Handles tool call results and continues the conversation\n- Provides an interactive chat loop for user interaction\n\nUsage:\n```python\npython tinytoolcallingagent.py \u003cpath_to_server_script\u003e\n```\n\n### TinyCodeAgent\n\n`TinyCodeAgent` is designed to solve tasks using Python code. It connects to an MCP server to access tools and can:\n\n- Connect to Python or JavaScript MCP servers\n- Process user queries using the Qwen2.5-Coder-32B-Instruct model\n- Generate Python code solutions\n- Execute Python code and display the results (TODO)\n- Provide an interactive chat loop for user interaction\n\nUsage:\n```python\npython tinycodeagent.py \u003cpath_to_server_script\u003e\n```\n\n## Common Features\n\nBoth agents share these capabilities:\n- Asynchronous operation using Python's asyncio\n- Connection to MCP servers via stdio\n- Interactive chat interface\n- Dynamic tool discovery\n- Integration with Hugging Face's InferenceClient\n\n## Included Example: Weather Server\n\nThe repository includes an example MCP server implementation in the `servers/weather` directory. This server provides tools for accessing weather data from the National Weather Service API:\n\n- `get_alerts`: Retrieves weather alerts for a specified US state\n- `get_forecast`: Gets a detailed weather forecast for a location based on latitude and longitude\n\nTo use the weather server with one of the agents:\n\n```bash\n# With TinyToolCallingAgent\npython tinytoolcallingagent.py servers/weather/weather.py\n\n# With TinyCodeAgent\npython tinycodeagent.py servers/weather/weather.py\n```\n\n## Future Enhancements\n\n### Python Code Execution\n\nThe TinyCodeAgent should include a basic Python code executor that:\n- Automatically extracts Python code blocks from the LLM's response\n- Executes the code in a controlled environment\n- Captures and displays standard output and error streams\n- Reports execution status (success or failure)\n\nThis feature will enable users to immediately see the results of code solutions provided by the agent, making it more interactive and useful for programming tasks.\n\n## Requirements\n\n- Python 3.10+\n- mcp \u003e= 1.9.0\n- huggingface-hub \u003e= 0.31.2\n- httpx (for the weather server example)\n\n## Getting Started\n\n1. Install the required dependencies\n2. Set up an MCP server (use the included weather server or create your own)\n3. Run one of the agent implementations pointing to your server script\n\nExample queries for the weather server:\n- \"What are the current weather alerts in New York and California?\"\n- \"What's the forecast for latitude 37.7749 and longitude -122.4194?\"\n\n\u003e **Note**: The [MCP server](https://modelcontextprotocol.io/quickstart/server) is"])</script><script>self.__next_f.push([1," intended for testing and development purposes only.55:T4bf,## what is TinyAgents? \nTinyAgents is a minimalist implementation of agents powered by LLMs and Model Context Protocol (MCP) tools, designed to facilitate the creation of LLM-based agent workflows.\n\n## how to use TinyAgents? \nTo use TinyAgents, set up an MCP server and run one of the agent implementations, such as TinyToolCallingAgent or TinyCodeAgent, pointing to your server script.\n\n## key features of TinyAgents? \n- General-purpose and code-specific agent implementations\n- Asynchronous operation using Python's asyncio\n- Dynamic tool discovery and integration with Hugging Face's InferenceClient\n- Interactive chat interface for user interaction\n\n## use cases of TinyAgents? \n1. Accessing weather data through an MCP server\n2. Solving programming tasks using Python code\n3. Building custom LLM-based workflows for various applications\n\n## FAQ from TinyAgents? \n- What is the purpose of TinyAgents?\n\u003e TinyAgents provides a lightweight foundation for building LLM-based agent workflows using MCP tools.\n\n- What programming language is required?\n\u003e TinyAgents requires Python 3.10 or higher.\n\n- Can I create my own MCP server?\n\u003e Yes! You can create your own MCP server or use the included weather server example.56:T10bd,# Coolify MCP Workflow\n\n🚀 **Automate Coolify Management with n8n!**\n\nThis workflow leverages the [Community n8n MCP Client](https://github.com/n8n-nodes-mcp) and my new [Coolify MCP Server](https://github.com/coolify-mcp-server) to interact with your Coolify infrastructure using **MCP (Model Context Protocol)**. It enables seamless management of teams, servers, services, applications, and deployments directly within your n8n workflows. \n\n## 🎯 Why I Created This Setup\nI built this workflow to **automate the management of my Coolify infrastructure using AI**. With AI-assisted parameter filling and automated execution, this setup simplifies and streamlines infrastructure management without extensive manual setup.\n\nAdditionally, I plan to **release another workflow integrating Telegram**, allowing interaction with a Telegram bot for even more convenient Coolify management.\n\n## 🔥 Coolify MCP Server – The Backbone of This Setup\nTo make this workflow possible, I put together the **Coolify MCP Server**. I used the documentation from [Coolify](https://coolify.io/docs/api-reference/authorization) and used the API endpoints I know I would use most to build out the tooling. This server acts as a bridge between Coolify and n8n, enabling:\n\n- **Seamless API communication** between Coolify and n8n\n- **Efficient command execution** for managing servers, services, and applications\n- **Scalability** to handle multiple infrastructure tasks simultaneously\n- **AI-driven decision-making**, allowing smart automation without manual input\n\nThis server is the heart of this automation setup, ensuring that every command runs smoothly and effectively. If you're using Coolify and want to unlock true automation potential, this server is a game-changer! 🚀\n\n## 📌 Features\n\nThis workflow provides **25 powerful tools** to control various aspects of Coolify:\n\n### 🔍 Version \u0026 Health Checks\n- **Get Version** - Retrieve Coolify version details\n- **Health Check** - Verify Coolify API health status\n\n### 👥 Teams Management\n- **List Teams** - Display all teams\n- **Get Team** - Fetch details of a specific team\n- **Get Current Team** - Retrieve details of the active team\n- **Get Current Team Members** - List members in the current team\n\n### 🖥️ Server Management\n- **List Servers** - Show all available servers\n- **Create Server** - Deploy a new server\n- **Validate Server** - Check server configuration validity\n- **Get Server Resources** - Monitor server resource usage\n- **Get Server Domains** - List domains associated with a server\n\n### ⚙️ Service Management\n- **List Services** - Show all registered services\n- **Create Service** - Deploy a new service\n- **Start Service** - Launch a service\n- **Stop Service** - Halt a running service\n- **Restart Service** - Restart a service\n\n###"])</script><script>self.__next_f.push([1," 📦 Application Management\n- **List Applications** - Show all applications\n- **Create Application** - Deploy a new application\n- **Start Application** - Launch an application\n- **Stop Application** - Stop an application\n- **Restart Application** - Restart an application\n- **Execute Command Application** - Run commands inside an application container\n\n### 🚀 Deployment Management\n- **List Deployments** - Show all deployments\n- **Get Deployment** - Fetch deployment details\n\n### 🔐 Security\n- **List Private Keys** - Show all stored private keys\n- **Create Private Key** - Generate a new private key\n\n## 🔧 Setup Instructions\n\n1. **Install Required Nodes:**\n   - [Community n8n MCP Client](https://www.npmjs.com/package/n8n-nodes-mcp)\n   - [Coolify MCP Server](https://www.npmjs.com/package/coolify-mcp-server)\n\n2. **Import Configuration:**\n   - Add this workflow to your n8n instance.\n\n3. **Configure Coolify MCP Credentials:**\n   - Ensure your Coolify instance has API access and set up the required authentication credentials in n8n.\n\n\n## 🛠️ Requirements\n\n- [**Community n8n MCP Client**](https://www.npmjs.com/package/n8n-nodes-mcp)\n- [**Coolify MCP Server**](https://www.npmjs.com/package/coolify-mcp-server)\n- [**Coolify**](https://coolify.io) with API access\n\n---\n\nWith this setup, you can effortlessly manage your Coolify infrastructure directly from n8n! 🎯 Stay tuned for the upcoming **Telegram bot integration** for even easier automation. 🤖57:T55d,## What is Coolify MCP Workflow? \nCoolify MCP Workflow is an automation tool that integrates the Community n8n MCP Client with the Coolify MCP Server to manage Coolify infrastructure using the Model Context Protocol (MCP).\n\n## How to use Coolify MCP Workflow? \nTo use this workflow, install the required nodes, import the configuration into your n8n instance, and configure your Coolify MCP credentials for API access.\n\n## Key features of Coolify MCP Workflow? \n- Seamless API communication between Coolify and n8n\n- Efficient management of teams, servers, services, applications, and deployments\n- AI-driven decision-making for smart automation\n- 25 powerful tools for various management tasks\n\n## Use cases of Coolify MCP Workflow? \n1. Automating server management tasks in Coolify.\n2. Managing application deployments without manual intervention.\n3. Monitoring and maintaining the health of Coolify infrastructure.\n\n## FAQ from Coolify MCP Workflow? \n- What is required to set up the workflow?\n\u003e You need the Community n8n MCP Client, Coolify MCP Server, and a Coolify instance with API access.\n\n- Can I manage multiple Coolify instances?\n\u003e Yes, the workflow is designed to handle multiple infrastructure tasks simultaneously.\n\n- Is there a planned integration with other platforms?\n\u003e Yes, there are plans to integrate with Telegram for enhanced management capabilities.58:T3c0b,# Proteus Workflow Engine\n\n一个强大的、可扩展的多智能体工作流引擎，支持Multi-Agent系统、auto-workflow、MCP-SERVER接入等功能，支持多种工具和资源，提供智能代理和自动化服务执行。\n\n## 项目名称由来\n\nProteus（普罗透斯）源自希腊神话中的海神，他以能够随意改变自己的形态而闻名。这个名字完美契合了本项目的核心特性：\n- 强大的可变性：就像普罗透斯能够变化成任何形态，本引擎可以通过不同节点类型的组合实现各种复杂的工作流\n- 智能适应：普罗透斯具有预知未来的能力，类似地，我们的Agent系统能够智能地选择最适合的工具和执行路径\n- 灵活性：如同海神能够掌控海洋的变化，本引擎能够灵活处理各种任务场景和数据流\n\n## 实际效果\n详见  **examples** 文件夹中的研究报告示例：\n- `中美人工智能发展报告.md`\n- `印巴空战5.7研究报告.md`\n- `细胞膜结构与功能研究进展.md`\n- `美俄军力报告.md`\n\n## 项目介绍\n\nProteus 是一个基于 Python 和 FastAPI 构建的现代化工作流引擎，它提供了以下核心特性：\n\n- 🚀 基于 FastAPI 的高性能 API 服务\n- 🤖"])</script><script>self.__next_f.push([1," 内置智能 Agent 系统（支持Chain-of-Thought推理）\n- 🔌 丰富的节点类型支持（20+种内置节点，包括新增的handoff交接节点）\n- 📊 实时执行状态监控（基于SSE的实时通信）\n- 🌐 Web 可视化界面（多种模式：工作流、智能体、多智能体等）\n- 🐳 Docker 支持（包含完整的容器化部署方案）\n- 🔄 MCP（Model Context Protocol）支持，可扩展外部工具和资源\n- 🛡️ 安全沙箱环境（用于安全执行代码节点）\n\n## 快速开始\n\n### 环境要求\n\n- Python 3.11+\n- Docker (可选，用于容器化部署)\n- LLM API密钥 (支持多种LLM服务，默认配置为Deepseek Chat)\n\n### 安装步骤\n\n1. 克隆项目\n```bash\ngit clone https://github.com/yourusername/proteus-ai.git\ncd proteus-ai\n```\n\n2. 创建并激活虚拟环境\n```bash\npython -m venv venv\nsource venv/bin/activate  # Linux/macOS\nvenv\\Scripts\\activate     # Windows\n```\n\n3. 安装依赖\n```bash\npip install -r proteus/requirements.txt\n```\n\n4. 配置环境变量\n```bash\ncp .env.example .env\n# 编辑 .env 文件，设置必要的环境变量\n```\n\n5. 浏览器自动化需要执行如下安装\n```shell\nplaywright install\n```\n\n### 启动服务\n\n#### 本地开发环境\n```bash\n# 确保已安装所有依赖\npip install -r proteus/requirements.txt\n\n# 如果需要浏览器自动化功能，安装playwright\nplaywright install\n\n# 启动服务\npython proteus/main.py\n```\n\n#### 使用 Docker\n```bash\n# 构建Docker镜像（在项目根目录执行）\ndocker build -t proteus -f proteus/Dockerfile .\n\n# 使用Docker Compose启动所有服务\ndocker-compose -f proteus/docker/docker-compose.yml up -d\n```\n\n服务启动后，访问 http://localhost:8000 即可打开 Web 界面。\n也可以通过 https://localhost:9443 访问Nginx代理后的HTTPS服务。\n\n#### 配置说明\n主要配置项在`.env`文件中，您需要从`.env.example`复制并配置：\n\n- `API_KEY`: LLM API密钥（必填）\n- `MODEL_NAME`: 使用的模型名称（默认为deepseek-chat）\n- `REASONER_MODEL_NAME`: 推理模型名称（可选）\n- `SERPER_API_KEY`: 用于Web搜索的Serper API密钥（可选）\n- `MCP_CONFIG_PATH`: MCP配置文件路径（默认为./proteus/proteus_mcp_config.json）\n\n## 功能特点\n\n### 1. 多样化节点类型\n- API 调用节点 (api_call): 调用外部API服务\n- 文件操作节点 (file_read, file_write): 读写本地文件\n- 数据库操作节点 (db_query, db_execute): 执行SQL查询和更新操作\n- 搜索节点 (duckduckgo_search, arxiv_search, serper_search): 从不同搜索引擎获取信息\n- Python 代码执行节点 (python_execute): 动态执行Python代码\n- Web 爬虫节点 (web_crawler, web_crawler_local): 抓取网页内容，支持远程API和本地浏览器\n- 天气预报节点 (weather_forecast): 获取天气信息\n- 用户输入节点 (user_input): 获取用户交互输入\n- 工作流嵌套节点 (workflow_node): 支持工作流嵌套\n- 循环节点 (loop_node): 支持循环执行\n- 聊天节点 (chat): 与LLM进行对话\n- 浏览器代理节点 (browser_agent): 自动化浏览器操作\n- MCP客户端节点 (mcp_client): 与MCP服务器交互\n- 交接节点 (handoff): 任务交接给其他Agent\n\n### 2. 智能 Agent 系统\n- 基于 Chain-of-Thought 的推理能力，支持复杂任务分解\n- 自动工具选择和执行，可动态调用最适合的工具\n- 多轮对话支持，保持上下文连贯性\n- 历史记录管理，包括查询、结果存储和摘要生成\n- 多种Agent模式：\n  * 超级智能体 (super-agent): 综合能力的智能体\n  * 自动工作流 (workflow): 自动生成和执行工作流\n  * MCP智能体 (mcp-agent): 支持MCP协议的智能体\n  * 多智能体 (multi-agent): 多个智能体协作\n  * 深度研究 (research): 专注于研究任务的智能体\n  * 浏览器智能体 (browser-agent): 专注于浏览器自动化任务\n\n### 3. 实时状态监控\n- SSE (Server-Sent Events) 实时通信\n- 节点执行状态实时更新\n- 执行结果即时反馈\n- 支持工作流暂停、恢复和取消操作\n\n### 4. Web 可视化界面\n- 直观的工作"])</script><script>self.__next_f.push([1,"流展示，支持节点和边的可视化\n- 实时执行状态可视化，通过颜色和图标展示节点状态\n- 历史记录查看和管理，支持会话恢复\n- 多种交互模式：工作流模式、智能体模式、研究模式等\n- 弹幕功能，支持实时显示执行过程中的思考和操作\n\n## TODO 功能项\n\n1. 核心功能增强\n   - [ ] 支持工作流的暂停和恢复\n   - [ ] 添加工作流模板系统\n   - [ ] 实现工作流版本控制\n   - [ ] 增加节点执行超时机制\n\n2. 节点类型扩展\n   - [ ] 添加更多 AI 模型集成节点\n   - [ ] 实现文件格式转换节点\n   - [ ] 添加邮件发送节点\n   - [ ] 集成更多第三方服务 API\n\n3. Agent 系统优化\n   - [ ] 优化 Chain-of-Thought 推理\n   - [ ] 添加多 Agent 协作机制\n   - [ ] 实现 Agent 记忆系统\n   - [ ] 增强错误处理和恢复能力\n\n4. 用户体验改进\n   - [ ] 优化 Web 界面交互\n   - [ ] 添加工作流调试工具\n   - [ ] 实现工作流执行日志导出\n   - [ ] 添加性能监控面板\n\n5. 部署和运维\n   - [ ] 添加集群部署支持\n   - [ ] 实现自动化测试\n   - [ ] 优化资源使用效率\n   - [ ] 增加监控告警机制\n\n## 项目结构\n\n```\nproteus/\n├── src/                    # 源代码目录\n│   ├── agent/             # 智能Agent相关实现\n│   │   ├── agent_engine.py # Agent引擎核心实现\n│   │   ├── agent_manager.py # Agent管理器\n│   │   ├── agent_router.py # Agent API路由\n│   │   ├── danmaku_router.py # 弹幕功能路由\n│   │   ├── multi_agent.py # 多智能体实现\n│   │   ├── parse_xml.py   # XML解析工具\n│   │   ├── task_manager.py # 任务管理器\n│   │   └── prompt/        # Agent提示词模板\n│   ├── api/               # API服务实现\n│   │   ├── config.py      # API配置\n│   │   ├── events.py      # 事件处理\n│   │   ├── history_service.py # 历史记录服务\n│   │   ├── llm_api.py     # LLM API集成\n│   │   ├── stream_manager.py # 流管理器\n│   │   ├── utils.py       # 工具函数\n│   │   └── workflow_service.py # 工作流服务\n│   ├── core/              # 核心功能实现\n│   │   ├── engine.py      # 工作流引擎\n│   │   ├── enums.py       # 枚举定义\n│   │   ├── executor.py    # 节点执行器\n│   │   ├── models.py      # 数据模型\n│   │   ├── node_config.py # 节点配置管理\n│   │   ├── params.py      # 参数处理\n│   │   └── validator.py   # 验证器\n│   ├── exception/         # 异常处理\n│   ├── manager/           # 管理器\n│   │   └── mcp_manager.py # MCP管理器\n│   ├── nodes/             # 节点类型实现（20+种节点）\n│   │   ├── base.py        # 基础节点类\n│   │   ├── node_config.yaml # 节点配置文件\n│   │   ├── agent_node_config.yaml # Agent节点配置\n│   │   └── [各种节点实现]\n│   └── utils/             # 工具函数\n├── static/                # 前端静态资源\n│   ├── agent/            # Agent页面资源\n│   ├── icon/             # 图标资源\n│   ├── superagent/       # 超级Agent页面资源\n│   ├── card-styles.css   # 卡片样式\n│   ├── card-view.js      # 卡片视图脚本\n│   ├── danmaku.css       # 弹幕样式\n│   ├── danmaku.js        # 弹幕脚本\n│   ├── index.html        # 主页面\n│   ├── main.js           # 主脚本\n│   ├── marked.min.js     # Markdown渲染库\n│   └── styles.css        # 主样式表\n├── docker/                # Docker相关配置\n│   ├── docker-compose.yml # Docker Compose配置\n│   └── volumes/          # 卷配置\n├── .env.example          # 环境变量示例\n├── Dockerfile            # Docker构建文件\n├── generate-ssl-cert.sh  # SSL证书生成脚"])</script><script>self.__next_f.push([1,"本\n├── LICENSE               # 许可证文件\n├── main.py               # 应用入口\n├── proteus_mcp_config.json # MCP配置文件\n└── requirements.txt      # 依赖项列表\n```\n\n## 核心功能详解\n\n### 1. 工作流引擎 (core/engine.py)\n- 工作流生命周期管理\n- 节点依赖关系处理\n- 异步执行调度\n- 状态追踪和错误处理\n- 支持流式执行和实时状态更新\n- 提供工作流暂停、恢复和取消功能\n\n### 2. 节点系统 (nodes/)\n- 统一的节点接口定义\n- 丰富的内置节点类型\n- 节点参数验证\n- 节点执行状态管理\n- 支持同步和异步执行\n- 错误重试机制\n\n### 3. Agent系统 (agent/)\n- 基于CoT的推理实现\n- 动态工具调用\n- 上下文管理\n- 多轮对话处理\n- 支持多种Agent模式\n- 用户输入交互\n- 历史记录管理\n\n### 4. API服务 (api/)\n- RESTful API设计\n- 实时通信(SSE)支持\n- 工作流生成和执行\n- 历史记录管理\n- 事件驱动架构\n- 认证和授权\n\n### 5. MCP系统 (manager/mcp_manager.py)\n- Model Context Protocol (MCP) 标准支持\n- 动态加载和管理外部工具和资源\n- 与远程MCP服务器集成（支持SSE通信）\n- 扩展智能体能力，提供更丰富的交互方式\n- 标准化的工具描述格式，便于模型理解和使用\n\n## 使用示例\n\n### 示例1: 创建简单研究工作流\n\n以下是一个简单的研究工作流示例，用于收集和分析主题信息：\n\n```yaml\nname: 简单研究流程\ndescription: 一个简单的研究工作流示例\nnodes:\n  - type: user_input\n    id: input\n    params:\n      question: \"请输入研究主题\"\n  - type: duckduckgo_search\n    id: search\n    params:\n      query: \"{{input.output}}\"\n      max_results: 5\n  - type: web_crawler\n    id: crawler\n    params:\n      urls: \"{{search.output}}\"\n  - type: chat\n    id: analysis\n    params:\n      prompt: |\n        请基于以下内容进行分析总结：\n        {{crawler.output}}\n  - type: file_write\n    id: output\n    params:\n      path: \"./research_output.md\"\n      content: \"{{analysis.output}}\"\n```\n\n### 示例2: 使用MCP工具查询天气\n\n```yaml\nname: 天气查询\ndescription: 使用MCP工具查询天气\nnodes:\n  - type: user_input\n    id: input\n    params:\n      question: \"请输入城市名称\"\n  - type: mcp_client\n    id: weather\n    params:\n      server_name: \"amap-maps\"\n      tool_name: \"maps_weather\"\n      arguments: |\n        {\n          \"city\": \"{{input.output}}\"\n        }\n  - type: chat\n    id: format\n    params:\n      prompt: |\n        将天气数据格式化为友好回复：\n        {{weather.output}}\n```\n\n## 开发指南\n\n### 1. 添加新节点类型\n\n1. 在 `src/nodes/` 目录下创建新的节点文件\n2. 继承 `BaseNode` 类并实现必要方法：\n```python\nfrom src.nodes.base import BaseNode\n\nclass MyCustomNode(BaseNode):\n    def __init__(self, node_id: str, params: dict):\n        super().__init__(node_id, params)\n        \n    async def execute(self, context: dict) -\u003e dict:\n        # 实现节点逻辑\n        result = await self._process_data(context)\n        return {\"success\": True, \"data\": result}\n```\n3. 在 `node_config.yaml` 中注册节点配置：\n```yaml\nMyCustomNode:\n  type: my_custom_node\n  description: \"自定义节点描述\"\n  params:\n    param1:\n      type: string\n      required: true\n      description: \"参数1描述\"\n```\n\n### 2. 扩展Agent功能\n\n1. 在 `src/agent/prompt/` 中添加新的提示词模板\n2. 修改 `src/agent/agent.py` 实现新的推理方法\n3. 注册新的工具到Agent系统\n\n### 3. 前端开发\n\n1. 静态资源位于 `static/` 目录\n2. 使用 SSE 接收实时更新：\n```javascript\nconst eventSource = new EventSource(`/stream/${chatId}`);\neventSource.onmessage = (event) =\u003e {\n    const data = JSON.parse(event.data);\n    // 处理实时数据\n};\n```\n\n### 4. 测试指南\n\n1. 单元测试：\n```bash\npython -m pytest tests/unit/\n```\n\n2. 集成测试：\n```bash\npython -m pytest tests/integration/\n```\n\n3. 端到端测试：\n```bash\npython -m pytest tests/e2e/\n```\n\n### 5. 使用MCP功能\n\n1. 配置MCP服务器\n```json\n{\n    \"mcp"])</script><script>self.__next_f.push([1,"Servers\": {\n        \"server-name\": {\n            \"type\": \"sse\",\n            \"url\": \"https://your-mcp-server-url\"\n        }\n    }\n}\n```\n\n2. 在代码中使用MCP管理器\n```python\nfrom src.manager.mcp_manager import get_mcp_manager, initialize_mcp_manager\n\n# 初始化MCP管理器\nawait initialize_mcp_manager()\n\n# 获取MCP管理器实例\nmcp_manager = get_mcp_manager()\n\n# 获取所有工具\ntools = mcp_manager.get_all_tools()\n\n# 获取所有资源\nresources = mcp_manager.get_all_resources()\n```\n\n3. 在Agent中使用MCP工具\n   - 选择\"MCP智能体\"模式\n   - 或在自定义Agent中配置MCP工具\n\n## 贡献指南\n\n欢迎提交 Issue 和 Pull Request 来帮助改进项目。在提交代码前，请确保：\n\n1. 代码符合项目的编码规范\n2. 添加了必要的测试用例\n3. 更新了相关文档\n4. 遵循Git提交规范\n5. 通过所有CI检查\n\n## 接下来需要做的事情\n\n基于对项目代码和结构的分析，以下是接下来可能需要优先考虑的工作：\n\n1. **完善MCP集成功能**\n   - 增强MCP工具的错误处理和重试机制\n   - 优化MCP资源的缓存策略\n   - 添加更多MCP服务器示例和文档\n\n2. **增强多智能体协作能力**\n   - 完善智能体间的通信机制\n   - 实现智能体记忆和知识共享\n   - 添加智能体角色定制功能\n\n3. **优化工作流执行引擎**\n   - 实现工作流的暂停和恢复功能\n   - 添加工作流执行的监控和日志记录\n   - 优化节点执行的并行处理能力\n\n4. **改进用户界面**\n   - 增强工作流可视化编辑功能\n   - 优化移动端适配\n   - 添加更多交互反馈和提示\n\n5. **扩展节点类型**\n   - 添加更多AI模型集成节点\n   - 实现文件格式转换节点\n   - 添加邮件和消息通知节点\n\n## 许可证\n\n本项目采用 MIT 许可证，详见 [LICENSE](LICENSE) 文件。59:T609,## What is Proteus Workflow Engine? \nProteus Workflow Engine is a powerful and scalable multi-agent workflow engine that supports Multi-Agent systems, auto-workflow, and MCP-SERVER integration, providing intelligent agents and automated service execution.\n\n## How to use Proteus Workflow Engine? \nTo use Proteus, clone the project from GitHub, set up a Python environment, install dependencies, configure environment variables, and start the service either locally or using Docker.\n\n## Key features of Proteus Workflow Engine? \n- High-performance API service based on FastAPI\n- Built-in intelligent agent system with Chain-of-Thought reasoning\n- Support for over 20 types of built-in nodes\n- Real-time execution status monitoring\n- Web visualization interface for workflows and agents\n- Docker support for containerized deployment\n- MCP (Model Context Protocol) support for external tools and resources\n\n## Use cases of Proteus Workflow Engine? \n1. Automating complex workflows in various applications.\n2. Integrating multiple tools and resources for enhanced functionality.\n3. Supporting research tasks with intelligent agents.\n\n## FAQ from Proteus Workflow Engine? \n- Can Proteus handle all types of workflows?  \n\u003e Yes! Proteus is designed to support a wide range of workflows and can be customized to fit specific needs.\n\n- Is Proteus free to use?  \n\u003e Yes! Proteus is open-source and free to use for everyone.\n\n- How can I contribute to Proteus?  \n\u003e Contributions are welcome! You can submit issues and pull requests on the GitHub repository.5a:T1d38,# Mattermost MCP Host\n\nA Mattermost integration that connects to Model Context Protocol (MCP) servers, leveraging a LangGraph-based AI agent to provide an intelligent interface for interacting with users and executing tools directly within Mattermost.\n\n![Version](https://img.shields.io/badge/version-0.1.0-blue)\n![Python](https://img.shields.io/badge/python-3.13.1%2B-blue)\n![License](https://img.shields.io/badge/license-MIT-green)\n![Package Manager](https://img.shields.io/badge/package%20manager-uv-purple)\n\n\n\n## Demo\n\n### 1. Github Agent in support channel - searches the existing issues and PRs and creates a new issue if not found\n![Description of your GIF](./demo/demo-3.gif) "])</script><script>self.__next_f.push([1,"  \n\n\n### 2. Search internet and post to a channel using Mattermost-MCP-server\n![Description of your GIF](./demo/demo-2.gif)\n\n#### Scroll below for full demo in YouTube\n\n## Features\n\n- 🤖 **Langgraph Agent Integration**: Uses a LangGraph agent to understand user requests and orchestrate responses.\n- 🔌 **MCP Server Integration**: Connects to multiple MCP servers defined in `mcp-servers.json`.\n- 🛠️ **Dynamic Tool Loading**: Automatically discovers tools from connected MCP servers and makes them available to the AI agent. Converts MCP tools to langchain structured tools.\n- 💬 **Thread-Aware Conversations**: Maintains conversational context within Mattermost threads for coherent interactions.\n- 🔄 **Intelligent Tool Use**: The AI agent can decide when to use available tools (including chaining multiple calls) to fulfill user requests.\n- 🔍 **MCP Capability Discovery**: Allows users to list available servers, tools, resources, and prompts via direct commands.\n- #️⃣ **Direct Command Interface**: Interact directly with MCP servers using a command prefix (default: `#`).\n\n\n## Overview\n\nThe integration works as follows:\n\n1.  **Mattermost Connection (`mattermost_client.py`)**: Connects to the Mattermost server via API and WebSocket to listen for messages in a specified channel.\n2.  **MCP Connections (`mcp_client.py`)**: Establishes connections (primarily `stdio`) to each MCP server defined in `src/mattermost_mcp_host/mcp-servers.json`. It discovers available tools on each server.\n3.  **Agent Initialization (`agent/llm_agent.py`)**: A `LangGraphAgent` is created, configured with the chosen LLM provider and the dynamically loaded tools from all connected MCP servers.\n4.  **Message Handling (`main.py`)**:\n    *   If a message starts with the command prefix (`#`), it's parsed as a direct command to list servers/tools or call a specific tool via the corresponding `MCPClient`.\n    *   Otherwise, the message (along with thread history) is passed to the `LangGraphAgent`.\n5.  **Agent Execution**: The agent processes the request, potentially calling one or more MCP tools via the `MCPClient` instances, and generates a response.\n6.  **Response Delivery**: The final response from the agent or command execution is posted back to the appropriate Mattermost channel/thread.\n\n## Setup\n1.  **Clone the repository:**\n    ```bash\n    git clone \u003crepository-url\u003e\n    cd mattermost-mcp-host\n    ```\n\n2.  **Install:**\n    *   Using uv (recommended):\n        ```bash\n        # Install uv if you don't have it yet\n        # curl -LsSf https://astral.sh/uv/install.sh | sh \n\n        # Activate venv\n        source .venv/bin/activate\n        \n        # Install the package with uv\n        uv sync\n\n        # To install dev dependencies\n        uv sync --dev --all-extras\n        ```\n\n3.  **Configure Environment (`.env` file):**\n    Copy the `.env.example` and fill in the values or\n    Create a `.env` file in the project root (or set environment variables):\n    ```env\n    # Mattermost Details\n    MATTERMOST_URL=http://your-mattermost-url\n    MATTERMOST_TOKEN=your-bot-token # Needs permissions to post, read channel, etc.\n    MATTERMOST_TEAM_NAME=your-team-name\n    MATTERMOST_CHANNEL_NAME=your-channel-name # Channel for the bot to listen in\n    # MATTERMOST_CHANNEL_ID= # Optional: Auto-detected if name is provided\n\n    # LLM Configuration (Azure OpenAI is default)\n    DEFAULT_PROVIDER=azure\n    AZURE_OPENAI_ENDPOINT=your-azure-endpoint\n    AZURE_OPENAI_API_KEY=your-azure-api-key\n    AZURE_OPENAI_DEPLOYMENT=your-deployment-name # e.g., gpt-4o\n    # AZURE_OPENAI_API_VERSION= # Optional, defaults provided\n\n    # Optional: Other providers (install with `[all]` extra)\n    # OPENAI_API_KEY=...\n    # ANTHROPIC_API_KEY=...\n    # GOOGLE_API_KEY=...\n\n    # Command Prefix\n    COMMAND_PREFIX=# \n    ```\n    See `.env.example` for more options.\n\n4.  **Configure MCP Servers:**\n    Edit `src/mattermost_mcp_host/mcp-servers.json` to define the MCP servers you want to connect to. See `src/mattermost_mcp_host/mcp-servers-example.json`.\n    Depending on the server configura"])</script><script>self.__next_f.push([1,"tion, you might `npx`, `uvx`, `docker` installed in your system and in path.\n\n5.  **Start the Integration:**\n    ```bash\n    mattermost-mcp-host\n    ```\n\n\n## Prerequisites\n\n- Python 3.13.1+\n- uv package manager\n- Mattermost server instance\n- Mattermost Bot Account with API token\n- Access to a LLM API (Azure OpenAI)\n\n### Optional\n- One or more MCP servers configured in `mcp-servers.json` \n- Tavily web search requires `TAVILY_API_KEY` in `.env` file\n\n\n## Usage in Mattermost\n\nOnce the integration is running and connected:\n\n1.  **Direct Chat:** Simply chat in the configured channel or with the bot. The AI agent will respond, using tools as needed. It maintains context within message threads.\n2.  **Direct Commands:** Use the command prefix (default `#`) for specific actions:\n    *   `#help` - Display help information.\n    *   `#servers` - List configured and connected MCP servers.\n    *   `#\u003cserver_name\u003e tools` - List available tools for `\u003cserver_name\u003e`.\n    *   `#\u003cserver_name\u003e call \u003ctool_name\u003e \u003cjson_arguments\u003e` - Call `\u003ctool_name\u003e` on `\u003cserver_name\u003e` with arguments provided as a JSON string.\n        *   Example: `#my-server call echo '{\"message\": \"Hello MCP!\"}'`\n    *   `#\u003cserver_name\u003e resources` - List available resources for `\u003cserver_name\u003e`.\n    *   `#\u003cserver_name\u003e prompts` - List available prompts for `\u003cserver_name\u003e`.\n\n\n\n## Next Steps\n- ⚙️ **Configurable LLM Backend**: Supports multiple AI providers (Azure OpenAI default, OpenAI, Anthropic Claude, Google Gemini) via environment variables.\n\n## Mattermost Setup\n\n1. **Create a Bot Account**\n- Go to Integrations \u003e Bot Accounts \u003e Add Bot Account\n- Give it a name and description\n- Save the access token in the .env file\n\n2. **Required Bot Permissions**\n- post_all\n- create_post\n- read_channel\n- create_direct_channel\n- read_user\n\n3. **Add Bot to Team/Channel**\n- Invite the bot to your team\n- Add bot to desired channels\n\n### Troubleshooting\n\n1. **Connection Issues**\n- Verify Mattermost server is running\n- Check bot token permissions\n- Ensure correct team/channel names\n\n2. **AI Provider Issues**\n- Validate API keys\n- Check API quotas and limits\n- Verify network access to API endpoints\n\n3. **MCP Server Issues**\n- Check server logs\n- Verify server configurations\n- Ensure required dependencies are installed and env variables are defined\n\n\n## Demos\n\n### Create issue via chat using Github MCP server\n![Description of your GIF](./demo/demo-1.gif)  \n\n### (in YouTube)\n[![AI Agent in Action in Mattermost](./demo/supercut-thumbnail.png)](https://youtu.be/s6CZY81DRrU)\n\n\n## Contributing\n\nPlease feel free to open a PR.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.5b:T6e4,## What is Mattermost MCP Host? \nMattermost MCP Host is an integration that connects Mattermost with Model Context Protocol (MCP) servers, utilizing AI language models to create an intelligent interface for managing and executing tools within Mattermost.\n\n## How to use Mattermost MCP Host? \nTo use Mattermost MCP Host, install the package via pip, configure your environment with the necessary Mattermost and AI provider credentials, and start the integration using Python.\n\n## Key features of Mattermost MCP Host? \n- AI-Powered Assistance with multiple AI providers (Azure OpenAI, OpenAI, Anthropic Claude, Google Gemini)\n- MCP Server Integration for connecting to any Model Context Protocol server\n- Tool Management for accessing and executing tools from connected servers\n- Thread-Based Conversations to maintain context within Mattermost threads\n- Tool Chaining to allow AI to call multiple tools in sequence\n- Resource Discovery to list available tools and resources from MCP servers\n- Multiple Provider Support for easy configuration changes.\n\n## Use cases of Mattermost MCP Host? \n1. Managing AI tools and resources directly within Mattermost.\n2. Executing complex tasks by chaining multiple AI tools.\n3. Facilitating team collaboration through intelligent tool management.\n\n## FAQ from Mattermost MCP Host? \n- What are the prerequisites for using Mattermost MCP Host?  \n\u003e You nee"])</script><script>self.__next_f.push([1,"d Python 3.13.1+, a Mattermost server, a bot account with permissions, and access to at least one LLM API.\n\n- How do I start the integration?  \n\u003e After installation and configuration, run the command `python -m mattermost_mcp_host` to start the integration.\n\n- Can I use different AI providers?  \n\u003e Yes! You can choose your preferred AI provider by changing the configuration.5c:T1feb,# codemcp\n\nMake Claude Desktop a pair programming assistant by installing codemcp.  With\nit, you can directly ask Claude to implement features, fix bugs and do\nrefactors on a codebase on your computer; Claude will directly edit files and\nrun tests.  Say goodbye to copying code in and out of Claude's chat window!\n\n![Screenshot of Claude Desktop with codemcp](static/screenshot.png?raw=true)\n\ncodemcp offers similar functionality to other AI coding software (Claude Code,\nCursor, Cline, Aider), but it occupies a unique point in the design space:\n\n1. It's intended to be used with **Claude Pro**, Anthropic's $20/mo\n   subscription offering.  I like paying for my usage with a subscription plan\n   because it means **zero marginal cost** for agent actions; no more feeling\n   bad that you wasted five bucks on a changeset that doesn't work.\n\n   Note that if you have Claude Max ($100/mo), Claude Code can also be used\n   with subscription based pricing.  The value proposition for codemcp is\n   murkier in this case (and it is definitely inferior to Claude Code in some\n   respects), but you can still use codemcp with Claude Max if you prefer some\n   of the other UI decisions it makes.  (Also, it's open source, so you can\n   change it if you don't like it, unlike Claude Code!)\n\n2. It's built around **auto-accept by default**.  I want my agent to get as\n   far as it can without my supervision, so I can review everything in one go at\n   the end.  There are two key things that codemcp does differently than most\n   coding agents: we **forbid unrestricted shell**, instead requiring you to\n   predeclare commands the agent can use in ``codemcp.toml``, and we **Git\n   version all LLM edits**, so you can roll back agent changes on a\n   fine-grained basis and don't have to worry about forgetting to commit\n   changes.\n\n3. It's **IDE agnostic**: you ask Claude to make changes, it makes them, and\n   then you can use your favorite IDE setup to review the changes and make\n   further edits.  I use vim as my daily driver editor, and coding environments\n   that require VSCode or a specific editor are a turn off for me.\n\n## IMPORTANT: For master users - Major changes for token efficiency\n\nTo improve codemcp's token efficiency, on master I am in the process of\nchanging codemcp back into a multi-tool tool (instead of a single tool whose\ninstructions are blatted into chat when you InitProject).  This means you have\nto manually approve tool use.  Because tool use approval is persistent across\nmultiple chats, I think this is a reasonable tradeoff to make, but if you\nreally don't like, file a bug at\n[refined-claude](https://github.com/ezyang/refined-claude/issues) browser\nextension for supporting auto-approve tool use.\n\n## Installation\n\nI recommend this specific way of installing and using codemcp:\n\n1. Install `uv` and install git, if they are not installed already.\n\n2. Install [claude-mcp](https://chromewebstore.google.com/detail/mcp-for-claudeai/jbdhaamjibfahpekpnjeikanebpdpfpb) on your browser.\n   This enables you to connect to SSE MCP servers directly from the website,\n   which means you don't need to use Claude Desktop and can easily have\n   multiple chat windows going in parallel.  We expect this extension should\n   be soon obsoleted by the rollout of\n   [Integrations](https://www.anthropic.com/news/integrations).  At time of\n   writing, however, Integrations have not yet arrived for Claude Pro subscribers.\n\n3. Run codemcp using ``uvx --from git+https://github.com/ezyang/codemcp@prod codemcp serve``.\n   You can add ``--port 1234`` if you need it to listen on a non-standard port.\n\n   Pro tip: if you like to live dangerously, you can change `prod` to `main`.  If\n   you want to pin"])</script><script>self.__next_f.push([1," to a specific release, replace it with `0.3.0` or similar.\n\n   Pro tip: you can run codemcp remotely!  If you use\n   [Tailscale](https://tailscale.com/) and trust all devices on your Tailnet,\n   you can do this securely by passing ``--host 100.101.102.103`` (replace the\n   IP with the Tailscale IP address of your node.  This IP typically lives in\n   the 100.64.0.0/10 range.)  **WARNING:** Anyone with access to this MCP can perform\n   arbitrary code execution on your computer, it is **EXTREMELY** unlikely you want to\n   bind to 0.0.0.0.\n\n4. Configure claude-mcp with URL: ``http://127.0.0.1:8000/sse`` (replace the port if needed.)\n\n5. Unfortunately, the web UI inconsistently displays the hammer icon.  However, you can verify\n   that the MCP server is working by looking for \"[MCP codemcp] SSE connection opened\" in the\n   Console, or by asking Claude what tools it has available (it should say\n   tools from codemcp are available.)\n\nIf you prefer to use Claude Desktop or have unusual needs, check out [INSTALL.md](INSTALL.md) for\ninstallation instructions for a variety of non-standard situations.\n\n## Usage\n\nFirst, you must create a `codemcp.toml` file in the Git repository checkout\nyou want to work on.  If you want the agent to be able to do things like run\nyour formatter or run tests, add the commands to execute them in the commands\nsection (note: these commands need to appropriately setup any virtual\nenvironment they need):\n\n```toml\nformat = [\"./run_format.sh\"]\ntest = [\"./run_test.sh\"]\n```\n\nThe ``format`` command is special; it is always run after every file edit.\n\nNext, in Claude Desktop, we recommend creating a Project and putting this in\nthe Project Instructions:\n\n```\nInitialize codemcp with $PROJECT_DIR\n```\n\nWhere `$PROJECT_DIR` is the path to the project you want to work on.\n\nThen chat with Claude about what changes you want to make to the project.\nEvery time codemcp makes a change to your code, it will generate a commit.\n\nTo see some sample transcripts using this tool, check out:\n\n- [Implement a new feature](https://claude.ai/share/a229d291-6800-4cb8-a0df-896a47602ca0)\n- [Fix failing tests](https://claude.ai/share/2b7161ef-5683-4261-ad45-fabc3708f950)\n- [Do a refactor](https://claude.ai/share/f005b43c-a657-43e5-ad9f-4714a5cd746f)\n\ncodemcp will generate a commit per chat and amend it as it is working on your feature.\n\n## Philosophy\n\n- When you get rate limited, take the time to do something else (review\n  Claude's code, review someone else's code, make plans, do some meetings)\n\n- This is *not* an autonomous agent.  At minimum, you have to intervene after\n  every chat to review the changes and request the next change.  While you\n  *can* ask for a long list of things to be done in a single chat, you will\n  likely hit Claude Desktop's output limit and have to manually \"continue\" the\n  agent anyway.  Embrace it, and use the interruptions to make sure Claude is\n  doing the right thing.\n\n- When Claude goes off the rails, it costs you time rather than dollars.\n  Behave accordingly: if time is the bottleneck, watch Claude's incremental\n  output carefully.\n\n## Configuration\n\nHere are all the config options supported by `codemcp.toml`:\n\n```toml\nproject_prompt = \"\"\"\nBefore beginning work on this feature, write a short haiku.  Do this only once.\n\"\"\"\n\n[commands]\nformat = [\"./run_format.sh\"]\ntest = [\"./run_test.sh\"]\n```\n\nThe `project_prompt` will be loaded when you initialize the project in chats.\n\nThe `commands` section allows you to configure commands for specific tools.  The\nnames are told to the LLM, who will decide when it wants to run them.  You can add\ninstructions how to use tools in the `project_prompt`; we also support a more verbose\nsyntax where you can give specific instructions on a tool-by-tool basis:\n\n```\n[commands.test]\ncommand = [\"./run_test.sh\"]\ndoc = \"Accepts a pytest-style test selector as an argument to run a specific test.\"\n```\n\n## Troubleshooting\n\nTo run the server with inspector, use:\n\n```\nPYTHONPATH=. mcp dev codemcp/__main__.py\n```\n\nLogs are written to `~/.codemcp/codemcp.log`. The log level can be set in a "])</script><script>self.__next_f.push([1,"global configuration file at `~/.codemcprc`:\n\n```toml\n[logger]\nverbosity = \"INFO\"  # Can be DEBUG, INFO, WARNING, ERROR, or CRITICAL\n```\n\nLogging is not configurable on a per project basis, but this shouldn't matter\nmuch because it's difficult to use Claude Desktop in parallel on multiple\nprojects anyway.\n\n## Contributing\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md).5d:T536,## what is codemcp? \ncodemcp is a coding assistant designed to enhance the Claude Desktop experience by enabling direct interaction with the AI for coding tasks such as implementing features, fixing bugs, and refactoring code.\n\n## how to use codemcp? \nTo use codemcp, install the necessary dependencies, configure the `claude_desktop_config.json` file, and create a `codemcp.toml` file in your project repository. Then, interact with Claude to make code changes directly.\n\n## key features of codemcp? \n- Direct integration with Claude Desktop for coding tasks\n- Safe agentic AI that follows best practices\n- IDE agnostic, allowing users to work in their preferred environment\n- Generates commits for each change made by the AI\n\n## use cases of codemcp? \n1. Implementing new features in a software project\n2. Fixing bugs and running tests automatically\n3. Refactoring existing code for better performance\n\n## FAQ from codemcp? \n- Is codemcp free to use?  \n\u003e Yes! codemcp is free to use with Claude Pro subscription.\n\n- What programming languages does codemcp support?  \n\u003e codemcp is designed to work with any language as long as the appropriate commands are configured.\n\n- How does codemcp ensure code safety?  \n\u003e codemcp uses Git version control to track changes, allowing users to review and accept changes before they are finalized.5e:T430,## what is Axiom? \nAxiom is a documentation AI Agent built with LangGraph, MCP Docs, Chainlit, and Gemini, designed to assist users in creating various projects using natural language.\n\n## how to use Axiom? \nTo use Axiom, simply interact with the agent through natural language commands to generate documentation and project outlines.\n\n## key features of Axiom? \n- Natural language processing for documentation generation\n- Integration with multiple AI frameworks\n- User-friendly interface for project creation\n\n## use cases of Axiom? \n1. Creating project documentation effortlessly\n2. Assisting in software development with automated documentation\n3. Generating outlines for technical reports\n\n## FAQ from Axiom? \n- Can Axiom handle all types of documentation?  \n\u003e Yes! Axiom is designed to assist with a wide range of documentation needs across various projects.\n\n- Is Axiom free to use?  \n\u003e Yes! Axiom is open-source and free for everyone to use.\n\n- What technologies does Axiom use?  \n\u003e Axiom utilizes LangGraph, MCP Docs, Chainlit, and Gemini for its functionalities.5f:T501,# Anirudh's Chat\n\n[![Anirudh's Chat Demo](./public/demo.png)](https://screen.studio/share/r5ljIy7R)\n\n\u003ccenter\u003e\u003ca href=\"https://screen.studio/share/r5ljIy7R\"\u003eWatch the video!\u003c/a\u003e\u003c/center\u003e\n\n## Background\n\nMCP servers have come a long way, and it's been easier than ever to create and run MCP servers, but the only way to use them is through Claude Desktop or their code editor. There's no \"ChatGPT for MCP\" yet.\n\nThis was my attempt at creating a simple open source MCP playground, where I could test out MCP servers and see how they work. At its full potential, if MCP is the internet for agents, what does Chromium look like?\n\nI also was heavily inspired by [t3.chat](https://t3.chat), which has some great features that I wanted to replicate, like the tree-based chat history.\n\n## Core Features\n\n- Fully [Vercel AI SDK](https://sdk.vercel.ai/) compatible\n- Git-based chat history.\n- No database/auth required, just Node.js.\n\n## Getting Started\n\n### Prerequisites\n\n- [Node.js](https://nodejs.org/en/download/) (v20 or higher)\n- [pnpm](https://pnpm.io/installation) (a faster alternative to npm)\n\n### Running the project\n\n```bash\n# Clone the repository\ngit clone https://github.com/kamath/chat\ncd chat\n\n# Install dependencies\npnpm install\n\n# Start the development server\npnpm dev\n```60:T4f8,## what is An"])</script><script>self.__next_f.push([1,"irudh's Chat? \nAnirudh's Chat is a simple open-source client for MCP (Multi-Chat Protocol) servers built with NextJS, designed to facilitate testing and interaction with MCP servers in a user-friendly environment.\n\n## how to use Anirudh's Chat? \nTo use Anirudh's Chat, clone the repository from GitHub, install the necessary dependencies using pnpm, and start the development server to begin interacting with MCP servers.\n\n## key features of Anirudh's Chat? \n- Fully compatible with the Vercel AI SDK.\n- Git-based chat history for easy tracking of conversations.\n- No database or authentication required, making it lightweight and easy to set up.\n\n## use cases of Anirudh's Chat? \n1. Testing and experimenting with MCP servers.\n2. Developing and debugging chat applications using the MCP framework.\n3. Learning about the functionalities of MCP in a practical environment.\n\n## FAQ from Anirudh's Chat? \n- Is Anirudh's Chat free to use?  \n\u003e Yes! Anirudh's Chat is open-source and free for everyone to use.\n\n- What are the prerequisites for running Anirudh's Chat?  \n\u003e You need Node.js (v20 or higher) and pnpm installed on your machine.\n\n- Can I contribute to Anirudh's Chat?  \n\u003e Absolutely! Contributions are welcome, and you can find the repository on GitHub.61:T2ce8,# 高德地址解析服务\n\n基于Claude AI和高德地图MCP的智能地址解析服务，提供强大的地址解析、地理编码和位置信息查询功能。\n\n## 🚀 功能特性\n\n- **智能地址解析**: 使用Claude AI理解自然语言地址查询\n- **高德地图集成**: 通过MCP协议集成高德地图API\n- **多种查询方式**: 支持地址转坐标、坐标转地址、POI搜索等\n- **多轮工具调用**: 支持复杂的多轮次工具调用，完成连续任务\n- **多种LLM支持**: 支持Claude和OpenAI等大模型\n- **RESTful API**: 提供完整的HTTP API接口\n- **异步处理**: 高性能异步架构，支持并发请求\n- **健康监控**: 完善的健康检查和监控机制\n- **错误处理**: 完善的异常处理和错误恢复机制\n\n## 📋 系统要求\n\n- Python 3.10+\n- Node.js 16+ (用于高德MCP服务器)\n- 高德地图API密钥\n- Claude API密钥\n\n## 🛠️ 安装配置\n\n### 1. 克隆项目\n\n```bash\ngit clone \u003crepository-url\u003e\ncd address-parser-service\n```\n\n### 2. 创建Python环境\n\n#### 方式一：使用pip（推荐）\n\n```bash\n# 创建虚拟环境\npython -m venv address-parser-env\n\n# 激活虚拟环境\n# Windows:\naddress-parser-env\\Scripts\\activate\n# macOS/Linux:\nsource address-parser-env/bin/activate\n\n# 安装依赖\npip install -r requirements.txt\n```\n\n#### 方式二：使用conda\n\n```bash\n# 方法1: 使用environment.yml文件（推荐）\nconda env create -f environment.yml\nconda activate address-parser\n\n# 方法2: 手动创建环境\nconda create -n address-parser python=3.11\nconda activate address-parser\n\n# 安装依赖\npip install -r requirements.txt\n\n# 或者使用conda安装部分依赖\nconda install fastapi uvicorn pydantic\npip install mcp anthropic structlog pydantic-settings\n```\n\n### 3. 配置环境变量\n\n复制环境变量模板：\n\n```bash\ncp .env.example .env\n```\n\n编辑 `.env` 文件，填入你的API密钥：\n\n```env\n# Claude API配置\nANTHROPIC_API_KEY=your_anthropic_api_key_here\n\n# 高德地图API配置  \nAMAP_MAPS_API_KEY=your_amap_api_key_here\n\n# 其他配置保持默认即可\n```\n\n### 4. 获取API密钥\n\n#### 高德地图API密钥\n1. 访问 [高德开放平台](https://lbs.amap.com/)\n2. 注册账号并创建应用\n3. 获取Web服务API密钥\n\n#### Claude API密钥\n1. 访问 [Anthropic Console](https://console.anthropic.com/)\n2. 创建账号并获取API密钥\n\n## 🚀 快速开始\n\n### 方式一：使用示例脚本\n\n```bash\n# 激活环境（如果使用虚拟环境）\n# pip环境:\nsource address-parser-env/bin/activate  # macOS/Linux\n# 或\naddress-parser-env\\Scripts\\activate     # Windows\n\n# conda环境:\nconda activate address-parser\n\n# 运行基础使用示例\npython examples/basic_usage.py\n```\n\n### 方式二：启动API服务\n\n```bash\n# 启动FastAPI服务\npython api/main.py\n```\n\n服务启动后访问：\n- API文"])</script><script>self.__next_f.push([1,"档: http://localhost:8000/docs\n- 健康检查: http://localhost:8000/api/v1/health\n\n### 方式三：直接使用MCP客户端\n\n```python\nimport asyncio\nfrom src.mcp_client import AmapMCPClient, ClaudeHandler\n\nasync def main():\n    # 创建MCP客户端\n    async with AmapMCPClient() as amap_client:\n        # 创建Claude处理器\n        claude_handler = ClaudeHandler(amap_client)\n        \n        # 处理查询\n        result = await claude_handler.process_query(\n            \"请帮我解析这个地址：北京市朝阳区三里屯太古里\"\n        )\n        \n        print(result[\"final_answer\"])\n\nasyncio.run(main())\n```\n\n## 📖 API文档\n\n### 地址解析接口\n\n**POST** `/api/v1/address/parse`\n\n请求体：\n```json\n{\n    \"address\": \"北京市朝阳区三里屯太古里\",\n    \"context\": {\n        \"city\": \"北京\",\n        \"preferences\": \"详细地址信息\"\n    },\n    \"system_prompt\": \"请提供详细的地址解析结果\"\n}\n```\n\n响应：\n```json\n{\n    \"success\": true,\n    \"request_id\": \"1642123456789_abc12345\",\n    \"data\": {\n        \"formatted_address\": \"北京市朝阳区三里屯太古里\",\n        \"location\": \"116.397428,39.90923\",\n        \"province\": \"北京市\",\n        \"city\": \"北京市\",\n        \"district\": \"朝阳区\"\n    },\n    \"response\": \"根据您提供的地址，我已经成功解析出详细信息...\",\n    \"tool_calls\": [...],\n    \"processing_time\": 2.5\n}\n```\n\n### 健康检查接口\n\n**GET** `/api/v1/health`\n\n响应：\n```json\n{\n    \"status\": \"healthy\",\n    \"mcp_connected\": true,\n    \"claude_available\": true,\n    \"tools_count\": 5,\n    \"uptime\": 3600.0\n}\n```\n\n### 工具列表接口\n\n**GET** `/api/v1/tools`\n\n响应：\n```json\n{\n    \"success\": true,\n    \"tools\": [\n        {\n            \"name\": \"geocode\",\n            \"description\": \"地理编码，将地址转换为坐标\",\n            \"input_schema\": {...}\n        }\n    ],\n    \"count\": 1\n}\n```\n\n## 🔧 高级配置\n\n### MCP服务器管理\n\n使用内置的服务器管理脚本：\n\n```bash\n# 启动高德MCP服务器\npython scripts/start_amap_server.py start\n\n# 停止服务器\npython scripts/start_amap_server.py stop\n\n# 重启服务器\npython scripts/start_amap_server.py restart\n\n# 查看状态\npython scripts/start_amap_server.py status\n\n# 监控模式（自动重启）\npython scripts/start_amap_server.py monitor\n```\n\n### 配置参数说明\n\n| 参数 | 默认值 | 说明 |\n|------|--------|------|\n| `LLM_PROVIDER` | claude | 大语言模型提供商（claude或openai） |\n| `ANTHROPIC_API_KEY` | 必填 | Claude API密钥 |\n| `CLAUDE_MODEL` | claude-3-7-sonnet-20250219 | Claude模型版本 |\n| `CLAUDE_MAX_TOKENS` | 1000 | Claude最大输出token数量 |\n| `ENABLE_TOKEN_EFFICIENT_TOOLS` | false | 是否启用Claude token高效工具调用 |\n| `TOOL_MAX_ITERATIONS` | 10 | 工具调用最大迭代次数 |\n| `OPENAI_API_KEY` | 可选 | OpenAI API密钥 |\n| `OPENAI_MODEL` | gpt-4 | OpenAI模型版本 |\n| `OPENAI_MAX_TOKENS` | 1000 | OpenAI最大输出token数量 |\n| `MCP_SERVER_TIMEOUT` | 30 | MCP服务器超时时间（秒） |\n| `MCP_RETRY_COUNT` | 3 | 重试次数 |\n| `MCP_RETRY_DELAY` | 1.0 | 重试延迟（秒） |\n| `LOG_LEVEL` | INFO | 日志级别 |\n| `API_HOST` | 127.0.0.1 | API服务主机 |\n| `API_PORT` | 8000 | API服务端口 |\n\n## 🧪 测试\n\n### 运行单元测试\n\n```bash\n# 运行所有测试\npytest tests/\n\n# 运行特定测试文件\npytest tests/test_mcp_client.py -v\n\n# 运行集成测试（需要API密钥）\npytest tests/ -m integration\n\n# 运行多轮工具调用测试\npython tests/run_tests.py --claude    # 仅运行Claude测试\npython tests/run_tests.py --openai    # 仅运行OpenAI测试\npython tests/run_tests.py --complex   # 仅运行复杂多轮测试\npython tests/run_tests.py --all       # 运行所有测试\n```\n\n### 测试覆盖率\n\n```bash\n# 生成测试覆盖率报告\npytest --cov=src tests/\n```\n\n## 📁 项目结构\n\n```\naddress-parser-service/\n├── src/                    # 核心源代码\n│   ├── mcp_client/        # MCP客户端模块\n│   │   ├── amap_client.py # 高德MCP客户端\n│   │   ├── cl"])</script><script>self.__next_f.push([1,"aude_handler.py # Claude处理器\n│   │   └── openai_handler.py # OpenAI处理器\n│   ├── core/              # 核心模块\n│   │   ├── config.py      # 配置管理\n│   │   ├── logger.py      # 日志配置\n│   │   └── exceptions.py  # 异常定义\n│   └── utils/             # 工具函数\n│       └── helpers.py     # 辅助函数\n├── api/                   # API服务模块\n│   ├── main.py           # FastAPI主应用\n│   ├── routes/           # API路由\n│   └── schemas/          # 数据模型\n├── tests/                # 测试代码\n│   ├── test_mcp_client.py # MCP客户端测试\n│   ├── test_multi_tool_calls.py # 多轮工具调用测试\n│   ├── test_complex_multi_tool_calls.py # 复杂多轮工具调用测试\n│   └── test_openai_multi_tool_calls.py # OpenAI多轮工具调用测试\n├── scripts/              # 脚本文件\n├── examples/             # 使用示例\n├── logs/                 # 日志文件\n├── requirements.txt      # Python依赖\n├── .env.example         # 环境变量模板\n└── README.md            # 项目文档\n```\n\n## 🔍 使用示例\n\n### 基础地址解析\n\n```python\n# 地址转坐标\nquery = \"北京市朝阳区三里屯太古里\"\nresult = await claude_handler.process_query(query)\n\n# 坐标转地址\nquery = \"116.397428,39.90923 这个坐标对应的地址是什么？\"\nresult = await claude_handler.process_query(query)\n\n# POI搜索\nquery = \"帮我查找北京大学的地理坐标\"\nresult = await claude_handler.process_query(query)\n```\n\n### 多轮工具调用示例\n\n```python\n# 多轮次工具调用示例 - 天气比较\nquery = \"查询一下北京市和上海市今天的天气，并告诉我在这两个城市中，哪个城市更适合户外活动？\"\nresult = await claude_handler.process_query(query)\n\n# 复杂多轮工具调用 - 旅行规划\nquery = \"\"\"请帮我规划一次从北京到上海的旅行路线：\n1. 首先查询北京和上海的天气情况\n2. 然后查询北京市区到上海市区的驾车路线\n3. 再查询途经城市杭州市的著名景点\n4. 最后给我一个综合考虑天气和路线的详细旅行计划建议\n\"\"\"\nresult = await claude_handler.process_query(query)\n```\n\n### 批量处理\n\n```bash\ncurl -X POST \"http://localhost:8000/api/v1/address/batch\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '[\n       {\"address\": \"北京市朝阳区三里屯\"},\n       {\"address\": \"上海市浦东新区陆家嘴\"},\n       {\"address\": \"广州市天河区珠江新城\"}\n     ]'\n```\n\n## 🐛 故障排除\n\n### 常见问题\n\n1. **MCP连接失败**\n   - 检查 `AMAP_MAPS_API_KEY` 是否正确设置\n   - 确认网络连接正常\n   - 查看日志文件获取详细错误信息\n\n2. **Claude API调用失败**\n   - 检查 `ANTHROPIC_API_KEY` 是否正确设置\n   - 确认API密钥有足够的配额\n   - 检查网络连接\n\n3. **工具调用超时**\n   - 增加 `MCP_SERVER_TIMEOUT` 配置值\n   - 检查高德地图API服务状态\n\n4. **依赖包冲突**\n   - 使用虚拟环境隔离依赖：`python -m venv address-parser-env`\n   - 或使用conda环境：`conda create -n address-parser python=3.11`\n   - 清理pip缓存：`pip cache purge`\n\n5. **conda环境问题**\n   - 确保激活了正确的环境：`conda activate address-parser`\n   - 如果遇到依赖冲突，尝试混合安装：\n     ```bash\n     conda install fastapi uvicorn pydantic\n     pip install mcp anthropic structlog pydantic-settings\n     ```\n\n### 日志查看\n\n```bash\n# 查看应用日志\ntail -f logs/app.log\n\n# 查看API访问日志\ntail -f logs/api.log\n```\n\n## 🤝 贡献指南\n\n1. Fork 项目\n2. 创建功能分支 (`git checkout -b feature/AmazingFeature`)\n3. 提交更改 (`git commit -m 'Add some AmazingFeature'`)\n4. 推送到分支 (`git push origin feature/AmazingFeature`)\n5. 开启 Pull Request\n\n## 📄 许可证\n\n本项目采用 MIT 许可证 - 查看 [LICENSE](LICENSE) 文件了解详情。\n\n## 🙏 致谢\n\n- [Anthropic](https://www.anthropic.com/) - "])</script><script>self.__next_f.push([1,"Claude AI API\n- [高德地图](https://lbs.amap.com/) - 地图服务API\n- [Model Context Protocol](https://modelcontextprotocol.io/) - MCP协议\n- [FastAPI](https://fastapi.tiangolo.com/) - Web框架\n\n## 📞 支持\n\n如果你遇到问题或有建议，请：\n\n1. 查看 [FAQ](#-故障排除)\n2. 搜索现有的 [Issues](../../issues)\n3. 创建新的 [Issue](../../issues/new)\n\n---\n\n**注意**: 本项目仅供学习和研究使用，请遵守相关API服务的使用条款。62:T674,## What is LLM-Amap-mcp? \nLLM-Amap-mcp is an intelligent address parsing service that combines Claude AI with Amap's MCP to provide powerful address parsing, geocoding, and location information query functionalities.\n\n## How to use LLM-Amap-mcp? \nTo use LLM-Amap-mcp, clone the project repository, set up the Python environment, configure your API keys, and run the FastAPI service to access the API endpoints for address parsing and geocoding.\n\n## Key features of LLM-Amap-mcp? \n- Intelligent address parsing using Claude AI for natural language address queries.\n- Integration with Amap's API via MCP protocol.\n- Multiple query methods including address-to-coordinate, coordinate-to-address, and POI search.\n- Support for complex multi-turn tool calls to complete sequential tasks.\n- RESTful API for easy integration.\n- Asynchronous processing for high-performance concurrent requests.\n\n## Use cases of LLM-Amap-mcp? \n1. Converting natural language addresses into geographic coordinates.\n2. Retrieving detailed address information from coordinates.\n3. Searching for points of interest (POI) based on user queries.\n4. Planning travel routes and providing location-based recommendations.\n\n## FAQ from LLM-Amap-mcp? \n- **What APIs does LLM-Amap-mcp provide?**  \n\u003e It provides APIs for address parsing, health checks, and tool listings.\n\n- **Is there a limit on the number of queries?**  \n\u003e The limits depend on the API keys used; please check the respective API documentation for details.\n\n- **Can I use it for commercial purposes?**  \n\u003e This project is intended for learning and research purposes; please adhere to the terms of service of the APIs used.63:T54e,## what is Install MCP CLI? \nInstall MCP CLI is a command-line interface (CLI) tool designed to simplify the installation and management of MCP servers for various clients.\n\n## how to use Install MCP CLI? \nTo use Install MCP CLI, run the command `npx install-mcp '\u003ccommand\u003e' --client \u003cclient\u003e` or use SSE URLs with the command `npx install-mcp '\u003curl\u003e' --client \u003cclient\u003e`, where `\u003cclient\u003e` can be one of the specified options like `claude`, `cline`, `roo-cline`, etc.\n\n## key features of Install MCP CLI? \n- Simplifies the installation process of MCP servers.\n- Supports multiple client options for flexibility.\n- Easy to use with straightforward command syntax.\n\n## use cases of Install MCP CLI? \n1. Quickly setting up MCP servers for development environments.\n2. Managing multiple MCP server installations across different clients.\n3. Streamlining the installation process for teams working with MCP technology.\n\n## FAQ from Install MCP CLI? \n- What clients are supported by Install MCP CLI?  \n\u003e Install MCP CLI supports clients such as `claude`, `cline`, `roo-cline`, `windsurf`, `witsy`, and `enconvo`.\n\n- Is there a license for Install MCP CLI?  \n\u003e Yes, Install MCP CLI is licensed under the MIT license.\n\n- How can I contribute to Install MCP CLI?  \n\u003e You can contribute by visiting the GitHub repository at https://github.com/supermemoryai/install-mcp.64:T445,## what is Oneshot? \nOneshot is an early prototype client for Anthropic's MCP designed specifically for macOS, allowing users to interact with the MCP API.\n\n## how to use Oneshot? \nTo use Oneshot, you need to install the necessary dependencies and run both the API server and the user interface using the provided commands.\n\n## key features of Oneshot? \n- Bring your own API key for personalized access\n- Built-in tool discovery for easy navigation\n- One-click tool installation for convenience\n\n## use cases of Oneshot? \n1. Developers can integrate MCP functionalities into their macOS"])</script><script>self.__next_f.push([1," applications.\n2. Users can experiment with the MCP API in a user-friendly environment.\n3. Quick testing and prototyping of features using the MCP.\n\n## FAQ from Oneshot? \n- Is Oneshot stable for production use?  \n\u003e No, Oneshot is an early prototype and may have stability issues.\n\n- What are the system requirements?  \n\u003e You need to have `bun` and Python's `uv` installed locally to run Oneshot.\n\n- Can I contribute to the development of Oneshot?  \n\u003e Yes! Contributions are welcome on the GitHub repository.65:T1a78,# MCPHost 🤖\n\nA CLI host application that enables Large Language Models (LLMs) to interact with external tools through the Model Context Protocol (MCP). Currently supports both Claude 3.5 Sonnet and Ollama models.\n\nDiscuss the Project on [Discord](https://discord.gg/RqSS2NQVsY)\n\n## Overview 🌟\n\nMCPHost acts as a host in the MCP client-server architecture, where:\n- **Hosts** (like MCPHost) are LLM applications that manage connections and interactions\n- **Clients** maintain 1:1 connections with MCP servers\n- **Servers** provide context, tools, and capabilities to the LLMs\n\nThis architecture allows language models to:\n- Access external tools and data sources 🛠️\n- Maintain consistent context across interactions 🔄\n- Execute commands and retrieve information safely 🔒\n\nCurrently supports:\n- Claude 3.5 Sonnet (claude-3-5-sonnet-20240620)\n- Any Ollama-compatible model with function calling support\n- Google Gemini models\n- Any OpenAI-compatible local or online model with function calling support\n\n## Features ✨\n\n- Interactive conversations with support models\n- Support for multiple concurrent MCP servers\n- Dynamic tool discovery and integration\n- Tool calling capabilities for both model types\n- Configurable MCP server locations and arguments\n- Consistent command interface across model types\n- Configurable message history window for context management\n\n## Requirements 📋\n\n- Go 1.23 or later\n- For Claude: An Anthropic API key\n- For Ollama: Local Ollama installation with desired models\n- For Google/Gemini: Google API key (see https://aistudio.google.com/app/apikey)\n- One or more MCP-compatible tool servers\n\n## Environment Setup 🔧\n\n1. Anthropic API Key (for Claude):\n```bash\nexport ANTHROPIC_API_KEY='your-api-key'\n```\n\n2. Ollama Setup:\n- Install Ollama from https://ollama.ai\n- Pull your desired model:\n```bash\nollama pull mistral\n```\n- Ensure Ollama is running:\n```bash\nollama serve\n```\n\nYou can also configure the Ollama client using standard environment variables, such as `OLLAMA HOST` for the Ollama base URL.\n\n3. Google API Key (for Gemini):\n```bash\nexport GOOGLE_API_KEY='your-api-key'\n```\n\n4. OpenAI compatible online Setup\n- Get your api server base url, api key and model name\n\n## Installation 📦\n\n```bash\ngo install github.com/mark3labs/mcphost@latest\n```\n\n## Configuration ⚙️\n\n### MCP-server\nMCPHost will automatically create a configuration file at `~/.mcp.json` if it doesn't exist. You can also specify a custom location using the `--config` flag.\n\n#### STDIO\nThe configuration for an STDIO MCP-server should be defined as the following:\n```json\n{\n  \"mcpServers\": {\n    \"sqlite\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mcp-server-sqlite\",\n        \"--db-path\",\n        \"/tmp/foo.db\"\n      ]\n    },\n    \"filesystem\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@modelcontextprotocol/server-filesystem\",\n        \"/tmp\"\n      ]\n    }\n  }\n}\n```\n\nEach STDIO entry requires:\n- `command`: The command to run (e.g., `uvx`, `npx`) \n- `args`: Array of arguments for the command:\n  - For SQLite server: `mcp-server-sqlite` with database path\n  - For filesystem server: `@modelcontextprotocol/server-filesystem` with directory path\n\n### Server Side Events (SSE) \n\nFor SSE the following config should be used:\n```json\n{\n  \"mcpServers\": {\n    \"server_name\": {\n      \"url\": \"http://some_jhost:8000/sse\",\n      \"headers\":[\n        \"Authorization: Bearer my-token\"\n       ]\n    }\n  }\n}\n```\n\nEach SSE entry requires:\n- `url`: The URL where the MCP server is accessible. \n- `headers`: (Optional) Array of heade"])</script><script>self.__next_f.push([1,"rs that will be attached to the requests\n\n### System-Prompt\n\nYou can specify a custom system prompt using the `--system-prompt` flag. The system prompt should be a JSON file containing the instructions and context you want to provide to the model. For example:\n\n```json\n{\n    \"systemPrompt\": \"You're a cat. Name is Neko\"\n}\n```\n\nUsage:\n```bash\nmcphost --system-prompt ./my-system-prompt.json\n```\n\n\n## Usage 🚀\n\nMCPHost is a CLI tool that allows you to interact with various AI models through a unified interface. It supports various tools through MCP servers.\n\n### Available Models\nModels can be specified using the `--model` (`-m`) flag:\n- Anthropic Claude (default): `anthropic:claude-3-5-sonnet-latest`\n- OpenAI or OpenAI-compatible: `openai:gpt-4`\n- Ollama models: `ollama:modelname`\n- Google: `google:gemini-2.0-flash`\n\n### Examples\n```bash\n# Use Ollama with Qwen model\nmcphost -m ollama:qwen2.5:3b\n\n# Use OpenAI's GPT-4\nmcphost -m openai:gpt-4\n\n# Use OpenAI-compatible model\nmcphost --model openai:\u003cyour-model-name\u003e \\\n--openai-url \u003cyour-base-url\u003e \\\n--openai-api-key \u003cyour-api-key\u003e\n```\n\n### Flags\n- `--anthropic-url string`: Base URL for Anthropic API (defaults to api.anthropic.com)\n- `--anthropic-api-key string`: Anthropic API key (can also be set via ANTHROPIC_API_KEY environment variable)\n- `--config string`: Config file location (default is $HOME/.mcp.json)\n- `--system-prompt string`: system-prompt file location\n- `--debug`: Enable debug logging\n- `--message-window int`: Number of messages to keep in context (default: 10)\n- `-m, --model string`: Model to use (format: provider:model) (default \"anthropic:claude-3-5-sonnet-latest\")\n- `--openai-url string`: Base URL for OpenAI API (defaults to api.openai.com)\n- `--openai-api-key string`: OpenAI API key (can also be set via OPENAI_API_KEY environment variable)\n- `--google-api-key string`: Google API key (can also be set via GOOGLE_API_KEY environment variable)\n\n\n### Interactive Commands\n\nWhile chatting, you can use:\n- `/help`: Show available commands\n- `/tools`: List all available tools\n- `/servers`: List configured MCP servers\n- `/history`: Display conversation history\n- `/quit`: Exit the application\n- `Ctrl+C`: Exit at any time\n\n### Global Flags\n- `--config`: Specify custom config file location\n- `--message-window`: Set number of messages to keep in context (default: 10)\n\n## MCP Server Compatibility 🔌\n\nMCPHost can work with any MCP-compliant server. For examples and reference implementations, see the [MCP Servers Repository](https://github.com/modelcontextprotocol/servers).\n\n## Contributing 🤝\n\nContributions are welcome! Feel free to:\n- Submit bug reports or feature requests through issues\n- Create pull requests for improvements\n- Share your custom MCP servers\n- Improve documentation\n\nPlease ensure your contributions follow good coding practices and include appropriate tests.\n\n## License 📄\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments 🙏\n\n- Thanks to the Anthropic team for Claude and the MCP specification\n- Thanks to the Ollama team for their local LLM runtime\n- Thanks to all contributors who have helped improve this tool66:T5a0,# MCPHost 🤖\n\n## what is MCPHost? \nMCPHost is a CLI host application designed to enable Large Language Models (LLMs) to interact with external tools through the Model Context Protocol (MCP). It provides a framework for LLM applications to manage connections, access tools, and maintain context.\n\n## how to use MCPHost? \nTo use MCPHost, install it via Go, configure it with your preferred MCP server settings, and execute basic commands to interact with either Claude 3.5 or Ollama models.\n\n## key features of MCPHost? \n- Interactive conversations with LLMs like Claude 3.5 and Ollama.\n- Support for multiple concurrent MCP servers.\n- Dynamic tool discovery and integration.\n- Tool calling capabilities across model types.\n- Customizable message history for context management.\n\n## use cases of MCPHost? \n1. Facilitating interactive sessions with LLMs for application development or research"])</script><script>self.__next_f.push([1,".\n2. Connecting LLMs with various tools for enhanced functionality.\n3. Managing context and tools for maintaining effective dialogue with users.\n\n## FAQ from MCPHost? \n- What platforms does MCPHost support?\n\u003e MCPHost supports any MCP-compliant server and can work with both Claude and Ollama models.\n\n- Is there a local installation requirement?\n\u003e Yes, for Ollama, a local installation is necessary along with compatible models.\n\n- Can I customize the command interface?\n\u003e Yes, MCPHost allows extensive configuration for commands and server setups.7:[[\"$\",\"div\",null,{\"className\":\"mb-4 w-full max-w-2xl overflow-hidden\",\"children\":[\"$undefined\",[\"$\",\"h1\",null,{\"className\":\"text-2xl font-bold mb-2\",\"children\":\"MCP Clients\"}],[\"$\",\"p\",null,{\"className\":\"text-sm text-muted-foreground\",\"children\":\"A list of MCP Clients.\"}],\"$undefined\",[\"$\",\"$L26\",null,{\"tabs\":[{\"title\":\"All\",\"url\":\"/clients\",\"is_active\":true},{\"title\":\"Featured\",\"url\":\"/clients?tag=featured\",\"is_active\":false},{\"title\":\"Latest\",\"url\":\"/clients?tag=latest\",\"is_active\":false}]}]]}],[\"$\",\"$L27\",null,{\"projects\":[{\"id\":15939,\"uuid\":\"fe0310cb-3f78-4803-b3a4-188beaa46b35\",\"name\":\"php-mcp-client\",\"title\":\"PHP MCP Client\",\"description\":\"Model Context Protocol client implementation for PHP\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/8734305?s=200\u0026v=4\",\"created_at\":\"$D2025-06-17T13:24:41.217Z\",\"updated_at\":\"$D2025-06-17T13:29:58.759Z\",\"status\":\"created\",\"author_name\":\"SWIS\",\"author_avatar_url\":null,\"tags\":\"PHP\",\"category\":null,\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/swisnl/mcp-client\",\"target\":\"_self\",\"content\":\"\",\"summary\":\"$28\",\"img_url\":null,\"type\":\"client\",\"metadata\":null,\"user_uuid\":\"6658a37e-0e35-4b29-a37b-06c0aee7b605\",\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":\"\",\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":5133,\"uuid\":\"bfd1ae8e-0a84-400a-a125-90c22c662717\",\"name\":\"mistr-agent\",\"title\":\"Mistr. Agent\",\"description\":\"A MCP client that enables Mistral AI models to autonomously execute complex tasks across web and local environments through standardized agentic capabilities.\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/151766858?v=4\",\"created_at\":\"$D2025-04-01T14:09:36.989Z\",\"updated_at\":\"$D2025-04-01T14:32:33.421Z\",\"status\":\"created\",\"author_name\":\"itisaevalex\",\"author_avatar_url\":\"https://avatars.githubusercontent.com/u/151766858?v=4\",\"tags\":\"[]\",\"category\":\"research-and-data\",\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/itisaevalex/mistr-agent\",\"target\":\"_self\",\"content\":\"$29\",\"summary\":\"$2a\",\"img_url\":\"https://github.com/itisaevalex/mistr-agent/raw/main/assets/images/main-interface.png\",\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"8\\\",\\\"license\\\":\\\"MIT license\\\",\\\"language\\\":\\\"TypeScript\\\",\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-04-01 15:45:00\\\"}\",\"user_uuid\":null,\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":null,\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":8809,\"uuid\":\"905c66c1-234a-4a1a-af2b-b152596c8134\",\"name\":\"install-mcp\",\"title\":\"Install MCP CLI\",\"description\":\"A simple CLI to install MCP servers into any client\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/63950637?v=4\",\"created_at\":\"$D2025-04-15T00:53:06.497Z\",\"updated_at\":\"$D2025-04-15T00:56:51.021Z\",\"status\":\"created\",\"author_name\":\"Dhravya\",\"author_avatar_url\":\"https://avatars.githubusercontent.com/u/63950637?v=4\",\"tags\":\"[]\",\"category\":\"developer-tools\",\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/dhravya/install-mcp\",\"target\":\"_self\",\"content\":\"# Install MCP CLI\\n\\n### A CLI tool to install and manage MCP servers.\\n\\nInstalling MCPs is a huge pain, so I made a CLI tool to make it easier.\\n\\n## Usage\\n\\nJust run\\n`npx install-mcp '\u003ccommand\u003e' --client \u003cclient\u003e`\\n\\nAlso works with SSE URLs\\"])</script><script>self.__next_f.push([1,"n`npx install-mcp '\u003curl\u003e' --client \u003cclient\u003e`\\n\\nwhere `\u003cclient\u003e` is one of the following:\\n\\n- `claude`\\n- `cline`\\n- `roo-cline`\\n- `windsurf`\\n- `witsy`\\n- `enconvo`\\n\\n## License\\n\\nMIT\",\"summary\":\"$2b\",\"img_url\":null,\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"50\\\",\\\"license\\\":\\\"MIT license\\\",\\\"language\\\":\\\"TypeScript\\\",\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-05-28 14:57:25\\\"}\",\"user_uuid\":null,\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":null,\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":2081,\"uuid\":\"962c380c-cff4-440a-8968-d8ff7965caaa\",\"name\":\"fast-agent\",\"title\":\"Fast Agent\",\"description\":\"Define, Prompt and Test MCP enabled Agents and Workflows\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/1936278?v=4\",\"created_at\":\"$D2025-03-09T02:44:14.287Z\",\"updated_at\":\"$D2025-03-12T10:20:54.707Z\",\"status\":\"created\",\"author_name\":\"evalstate\",\"author_avatar_url\":\"https://avatars.githubusercontent.com/u/1936278?v=4\",\"tags\":\"[]\",\"category\":\"developer-tools\",\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/evalstate/fast-agent\",\"target\":\"_self\",\"content\":\"$2c\",\"summary\":\"$2d\",\"img_url\":\"https://camo.githubusercontent.com/b60dabfbee044cc85054d321e20d067a9c1d1cd9ae8f627ddba968a35880d2c9/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f666173742d6167656e742d6d63703f636f6c6f723d253233333444303538266c6162656c3d70797069\",\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"2248\\\",\\\"license\\\":\\\"Apache-2.0 license\\\",\\\"language\\\":\\\"Python\\\",\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-05-30 16:50:12\\\"}\",\"user_uuid\":null,\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":null,\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":5029,\"uuid\":\"75cce3da-1e9c-4187-8669-b28635e90399\",\"name\":\"toolbase\",\"title\":\"Toolbase\",\"description\":\"A desktop application that adds powerful tools to Claude and AI platforms\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/191925333?v=4\",\"created_at\":\"$D2025-03-30T20:32:12.886Z\",\"updated_at\":\"$D2025-03-30T20:38:16.099Z\",\"status\":\"created\",\"author_name\":\"Toolbase-AI\",\"author_avatar_url\":\"https://avatars.githubusercontent.com/u/191925333?v=4\",\"tags\":\"tools,ai,mcp,claude,model-context-protocol\",\"category\":\"developer-tools\",\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/toolbase-ai/toolbase\",\"target\":\"_self\",\"content\":\"$2e\",\"summary\":\"$2f\",\"img_url\":\"https://private-user-images.githubusercontent.com/3330119/397458154-d60edc11-cabb-49c8-ba52-1a1418a9ebea.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDkwNTk4MzYsIm5iZiI6MTc0OTA1OTUzNiwicGF0aCI6Ii8zMzMwMTE5LzM5NzQ1ODE1NC1kNjBlZGMxMS1jYWJiLTQ5YzgtYmE1Mi0xYTE0MThhOWViZWEucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDYwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA2MDRUMTc1MjE2WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MmEzOTk3NjNhMDgyNWMwOGJhOGI4MzU5ZmMwMDQwODM5NTFkYTNkYTI4NjljYjRkNDc0Y2FjM2JjYzBiOTFjOCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.LcAhTwpGOXDoN0oV4AxRQCSvY91ojFOidFhYvs50ZUI\",\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"138\\\",\\\"license\\\":\\\"Apache-2.0 license\\\",\\\"language\\\":\\\"TypeScript\\\",\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-02-19 00:20:46\\\"}\",\"user_uuid\":null,\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":null,\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":13891,\"uuid\":\"cb464da6-6298-4e5c-95e8-c1e777082f9e\",\"name\":\"zola\",\"title\":\"Zola\",\"description\":\"The open-source interface for AI chat. Self-hostable, developer-first, and model-a"])</script><script>self.__next_f.push([1,"gnostic.\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/14288396?v=4\",\"created_at\":\"$D2025-06-01T02:00:40.628Z\",\"updated_at\":\"$D2025-06-01T04:15:40.048Z\",\"status\":\"created\",\"author_name\":\"ibelick\",\"author_avatar_url\":\"https://avatars.githubusercontent.com/u/14288396?v=4\",\"tags\":\"chat,open-source,typescript,ai,nextjs,multi-model,supabase,shadcn-ui,prompt-kit\",\"category\":\"research-and-data\",\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/ibelick/zola\",\"target\":\"_self\",\"content\":\"$30\",\"summary\":\"$31\",\"img_url\":\"https://camo.githubusercontent.com/fb2e2717ce64adfd92bc6ecd9350ab16041af45d968a3a0a7dbfb9c76c124560/68747470733a2f2f7a6f6c612e636861742f627574746f6e2f6769746875622e737667\",\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"767\\\",\\\"license\\\":\\\"Apache-2.0 license\\\",\\\"language\\\":\\\"TypeScript\\\",\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-06-04 16:00:35\\\"}\",\"user_uuid\":null,\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":null,\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":1582,\"uuid\":\"96b3433a-8a16-4a17-a6d0-e43cd6abb092\",\"name\":\"chat-mcp\",\"title\":\"MCP Chat Desktop App\",\"description\":\"A Desktop Chat App that leverages MCP(Model Context Protocol) to interface with other LLMs.\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/173930331?v=4\",\"created_at\":\"$D2025-02-23T09:39:31.460Z\",\"updated_at\":\"$D2025-02-23T09:48:49.805Z\",\"status\":\"created\",\"author_name\":\"AI-QL\",\"author_avatar_url\":\"https://avatars.githubusercontent.com/u/173930331?v=4\",\"tags\":\"electron,mcp,model-context-protocol,mcp-client\",\"category\":\"developer-tools\",\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/ai-ql/chat-mcp\",\"target\":\"_self\",\"content\":\"$32\",\"summary\":\"$33\",\"img_url\":\"https://camo.githubusercontent.com/4e04acac059df773f9fff095109dd036cc7f86f1c226f07505df2441bef554bf/68747470733a2f2f67636f72652e6a7364656c6976722e6e65742f67682f41492d514c2f2e6769746875622f7075626c69632f636861742d6d63702f64656d6f2d6d756c74696d6f64616c2e706e67\",\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"212\\\",\\\"license\\\":\\\"Apache-2.0 license\\\",\\\"language\\\":\\\"HTML\\\",\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-05-02 15:33:52\\\"}\",\"user_uuid\":null,\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":null,\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":4141,\"uuid\":\"7027f0c9-dfeb-4fcd-81b1-8c7fad71210c\",\"name\":\"EasyMCP\",\"title\":\"EasyMCP\",\"description\":\"A beginner-friendly client for the MCP (Model Context Protocol). Connect to SSE, NPX, and UV servers, and integrate with OpenAI for dynamic tool interactions. Perfect for exploring server connections and chat enhancements.\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/76538971?v=4\",\"created_at\":\"$D2025-03-23T14:42:30.770Z\",\"updated_at\":\"$D2025-03-23T15:07:47.294Z\",\"status\":\"created\",\"author_name\":\"mshojaei77\",\"author_avatar_url\":\"https://avatars.githubusercontent.com/u/76538971?v=4\",\"tags\":\"mcp,mcp-client\",\"category\":\"developer-tools\",\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/mshojaei77/easymcp\",\"target\":\"_self\",\"content\":\"$34\",\"summary\":\"$35\",\"img_url\":null,\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"15\\\",\\\"license\\\":\\\"\\\",\\\"language\\\":\\\"Python\\\",\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-03-23 16:28:37\\\"}\",\"user_uuid\":null,\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":null,\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":13903,\"uuid\":\"1f98e528-8fab-4646-94a4-9df65df03d8c\",\"name\":\"evo-ai\",\"title\":\"Evo AI - AI Agents Platform\",\"description\":\"Evo AI is an open-source platform for creating and managing AI agents, enabling integration with different AI models and services.\",\"avatar_url\":\"https://avat"])</script><script>self.__next_f.push([1,"ars.githubusercontent.com/u/136080052?v=4\",\"created_at\":\"$D2025-06-01T02:18:06.895Z\",\"updated_at\":\"$D2025-06-01T02:33:35.638Z\",\"status\":\"created\",\"author_name\":\"EvolutionAPI\",\"author_avatar_url\":\"https://avatars.githubusercontent.com/u/136080052?v=4\",\"tags\":\"python,agent,ai,mcp,adk,crewai,langgraph,agentic-workflow,agentic-ai,a2a-protocol\",\"category\":\"research-and-data\",\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/evolutionapi/evo-ai\",\"target\":\"_self\",\"content\":\"$36\",\"summary\":\"$37\",\"img_url\":\"https://camo.githubusercontent.com/742129aff65b0f5e7ae12e79771dc91a60efc2f9a0c31e176198b6c1eaa553fa/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f47726f75702d57686174734170702d253233323242433138\",\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"332\\\",\\\"license\\\":\\\"Apache-2.0 license\\\",\\\"language\\\":\\\"TypeScript\\\",\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-05-24 11:01:14\\\"}\",\"user_uuid\":null,\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":null,\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":4855,\"uuid\":\"0486a998-fd05-4d4f-9980-0f50cbb2f851\",\"name\":\"mcp_chatbot\",\"title\":\"MCPChatbot Example\",\"description\":\"A chatbot implementation compatible with MCP (terminal / streamlit supported)\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/103916249?v=4\",\"created_at\":\"$D2025-03-27T01:25:13.246Z\",\"updated_at\":\"$D2025-03-27T01:45:40.792Z\",\"status\":\"created\",\"author_name\":\"keli-wen\",\"author_avatar_url\":\"https://avatars.githubusercontent.com/u/103916249?v=4\",\"tags\":\"python,mcp,chatbot,streamlit,mcp-server,mcp-host\",\"category\":\"developer-tools\",\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/keli-wen/mcp_chatbot\",\"target\":\"_self\",\"content\":\"$38\",\"summary\":\"$39\",\"img_url\":\"https://github.com/keli-wen/mcp_chatbot/raw/master/assets/mcp_chatbot_logo.png\",\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"144\\\",\\\"license\\\":\\\"\\\",\\\"language\\\":\\\"Python\\\",\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-05-22 14:35:46\\\"}\",\"user_uuid\":null,\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":null,\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":4872,\"uuid\":\"127983c4-99df-4bf2-b2a4-e70d76a7bdc0\",\"name\":\"ClaudeR\",\"title\":\"ClaudeR\",\"description\":\"R MCP Integration for Claude\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/77558626?v=4\",\"created_at\":\"$D2025-03-27T03:32:00.362Z\",\"updated_at\":\"$D2025-03-27T03:42:18.657Z\",\"status\":\"created\",\"author_name\":\"IMNMV\",\"author_avatar_url\":\"https://avatars.githubusercontent.com/u/77558626?v=4\",\"tags\":\"[]\",\"category\":\"developer-tools\",\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/imnmv/clauder\",\"target\":\"_self\",\"content\":\"$3a\",\"summary\":\"$3b\",\"img_url\":\"https://camo.githubusercontent.com/6b96488ab7e718b8aec601f921473d330402b2f38d04529ad449de571edaf37d/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f4b534b63757852535a44592f302e6a7067\",\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"31\\\",\\\"license\\\":\\\"MIT license\\\",\\\"language\\\":\\\"R\\\",\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-04-05 17:19:12\\\"}\",\"user_uuid\":null,\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":null,\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":4432,\"uuid\":\"22f4feab-da48-403b-ab15-c69e8a3ea77d\",\"name\":\"prodex-js\",\"title\":\"prodex-js\",\"description\":\"The ultimate vide coding MCP!\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/9470158?v=4\",\"created_at\":\"$D2025-03-24T17:13:34.216Z\",\"updated_at\":\"$D2025-03-24T17:30:55.362Z\",\"status\":\"created\",\"author_name\":\"tarasyarema\",\"author_avatar_url\":\"https://avatars.githubusercontent.com/u/9470158?v=4\",\"tags\":\"[]\",\"category\":\"developer-tools\",\"is_featured\":false,\"sort\":1,\"url\":\""])</script><script>self.__next_f.push([1,"https://github.com/tarasyarema/prodex-js\",\"target\":\"_self\",\"content\":\"$3c\",\"summary\":\"$3d\",\"img_url\":null,\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"6\\\",\\\"license\\\":\\\"MIT license\\\",\\\"language\\\":\\\"JavaScript\\\",\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-03-26 18:06:31\\\"}\",\"user_uuid\":null,\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":null,\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":1576,\"uuid\":\"394732d4-beff-4562-9722-02a1724062b7\",\"name\":\"goose\",\"title\":\"codename goose\",\"description\":\"an open source, extensible AI agent that goes beyond code suggestions - install, execute, edit, and test with any LLM\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/185116535?v=4\",\"created_at\":\"$D2025-02-23T09:37:11.643Z\",\"updated_at\":\"$D2025-02-23T09:48:49.847Z\",\"status\":\"created\",\"author_name\":\"block\",\"author_avatar_url\":\"https://avatars.githubusercontent.com/u/185116535?v=4\",\"tags\":\"[]\",\"category\":\"developer-tools\",\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/block/goose\",\"target\":\"_self\",\"content\":\"$3e\",\"summary\":\"$3f\",\"img_url\":\"https://camo.githubusercontent.com/5ce2e21e84680df1ab24807babebc3417d27d66e0826a350eb04ab57f4c8f3e5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4170616368655f322e302d626c75652e737667\",\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"13542\\\",\\\"license\\\":\\\"Apache-2.0 license\\\",\\\"language\\\":\\\"Rust\\\",\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-06-04 20:50:08\\\"}\",\"user_uuid\":null,\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":null,\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":8081,\"uuid\":\"82ab7270-e588-4ed0-aad8-8d76186bc354\",\"name\":\"mcp-ai-agent\",\"title\":\"MCP AI Agent\",\"description\":\"A TypeScript library that enables AI agents to leverage MCP (Model Context Protocol) servers for enhanced capabilities. This library integrates with the AI SDK to provide a seamless way to connect to MCP servers and use their tools in AI-powered applications.\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/98839443?v=4\",\"created_at\":\"$D2025-04-12T10:29:03.437Z\",\"updated_at\":\"$D2025-04-12T10:31:59.857Z\",\"status\":\"created\",\"author_name\":\"fkesheh\",\"author_avatar_url\":\"https://avatars.githubusercontent.com/u/98839443?v=4\",\"tags\":\"autogen,ai-agent,crewai,agentic-ai,mcp-client\",\"category\":\"developer-tools\",\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/fkesheh/mcp-ai-agent\",\"target\":\"_self\",\"content\":\"$40\",\"summary\":\"$41\",\"img_url\":null,\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"14\\\",\\\"license\\\":\\\"MIT license\\\",\\\"language\\\":\\\"TypeScript\\\",\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-04-14 05:41:38\\\"}\",\"user_uuid\":null,\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":null,\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":1683,\"uuid\":\"89bbb777-cd14-4761-ae1f-bad05e0d719e\",\"name\":\"minions\",\"title\":\"Where On-Device and Cloud LLMs Meet\",\"description\":\"Big \u0026 Small LLMs working together\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/2165246?v=4\",\"created_at\":\"$D2025-03-05T04:43:25.119Z\",\"updated_at\":\"$D2025-03-12T10:19:28.872Z\",\"status\":\"created\",\"author_name\":\"HazyResearch\",\"author_avatar_url\":\"https://avatars.githubusercontent.com/u/2165246?v=4\",\"tags\":\"[]\",\"category\":\"research-and-data\",\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/hazyresearch/minions\",\"target\":\"_self\",\"content\":\"$42\",\"summary\":\"$43\",\"img_url\":\"https://github.com/HazyResearch/minions/raw/main/assets/Ollama_minionS_background.png\",\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"892\\\",\\\"license\\\":\\\"MIT license\\\",\\\"language\\\":\\\"Python\\\",\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-06-02 22:35"])</script><script>self.__next_f.push([1,":25\\\"}\",\"user_uuid\":null,\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":null,\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":13453,\"uuid\":\"528eccc2-317c-47c6-a92d-a93facaa00ba\",\"name\":\"Osmosis-MCP-4B-demo\",\"title\":\"Osmosis-MCP-4B\",\"description\":\"An Open Source SLM Trained for MCP\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/190666434?v=4\",\"created_at\":\"$D2025-05-09T17:28:25.117Z\",\"updated_at\":\"$D2025-05-09T17:46:42.625Z\",\"status\":\"created\",\"author_name\":\"Gulp-AI\",\"author_avatar_url\":\"https://avatars.githubusercontent.com/u/190666434?v=4\",\"tags\":\"[]\",\"category\":\"research-and-data\",\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/gulp-ai/osmosis-mcp-4b-demo\",\"target\":\"_self\",\"content\":\"$44\",\"summary\":\"$45\",\"img_url\":\"https://private-user-images.githubusercontent.com/17764452/444656439-7a379cc3-fe3f-4e4c-bc0f-9f17f753c849.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDkwNjE2MTksIm5iZiI6MTc0OTA2MTMxOSwicGF0aCI6Ii8xNzc2NDQ1Mi80NDQ2NTY0MzktN2EzNzljYzMtZmUzZi00ZTRjLWJjMGYtOWYxN2Y3NTNjODQ5LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA2MDQlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNjA0VDE4MjE1OVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTFmZjU2Nzg1YTc4ZjBkZjEyMTUxNTJmNzk0MTlmN2UyZDdjZWM2OTkxY2NhYmM0YzliNzc4YjRiZDliZjNlMWQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.SMueOXgojmhFnKM7UzXFp1DU1iMvrJva7iuS9bcRsEk\",\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"17\\\",\\\"license\\\":\\\"MIT license\\\",\\\"language\\\":\\\"Python\\\",\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-05-18 13:30:46\\\"}\",\"user_uuid\":null,\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":null,\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":13892,\"uuid\":\"b185cd98-c971-482f-a6b9-fe02715dfd17\",\"name\":\"groq-desktop-beta\",\"title\":\"Groq Desktop\",\"description\":\"Local Groq Desktop chat app with MCP support\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/7464134?v=4\",\"created_at\":\"$D2025-06-01T02:00:40.913Z\",\"updated_at\":\"$D2025-06-01T05:23:16.921Z\",\"status\":\"created\",\"author_name\":\"groq\",\"author_avatar_url\":\"https://avatars.githubusercontent.com/u/7464134?v=4\",\"tags\":\"managed-by-terraform\",\"category\":\"communication\",\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/groq/groq-desktop-beta\",\"target\":\"_self\",\"content\":\"$46\",\"summary\":\"$47\",\"img_url\":\"https://camo.githubusercontent.com/50ada1db680439f2666b3da072dc6f0259a1d0b36c8a34ba2a7ab092c8ffb96d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f67726f712f67726f712d6465736b746f702d626574613f696e636c7564655f70726572656c6561736573266c6162656c3d6c61746573742532306d61634f532532302e646d672532306275696c64\",\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"268\\\",\\\"license\\\":\\\"MIT license\\\",\\\"language\\\":\\\"JavaScript\\\",\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-05-15 15:15:37\\\"}\",\"user_uuid\":null,\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":null,\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":1577,\"uuid\":\"a9cab802-3ee9-454d-af35-34f29b04ea71\",\"name\":\"multimodal-mcp-client\",\"title\":\"Systemprompt Multimodal MCP Client\",\"description\":\"A Multi-modal MCP client for voice powered agentic workflows\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/4939114?v=4\",\"created_at\":\"$D2025-02-23T09:37:11.636Z\",\"updated_at\":\"$D2025-02-23T09:48:49.860Z\",\"status\":\"created\",\"author_name\":\"Ejb503\",\"author_avatar_url\":\"https://avatars.githubusercontent.com/u/4939114?v=4\",\"tags\":\"mcp,gemini,voice-assistant,model-context-protocol"])</script><script>self.__next_f.push([1,"\",\"category\":\"research-and-data\",\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/ejb503/multimodal-mcp-client\",\"target\":\"_self\",\"content\":\"$48\",\"summary\":\"$49\",\"img_url\":\"https://camo.githubusercontent.com/40f03207d8d2eca54bc5ea3699e27a2053cc19e4204b21202457124f3c59c6c0/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f313235353136303839313036323632303235323f636f6c6f723d373238396461266c6162656c3d646973636f7264\",\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"183\\\",\\\"license\\\":\\\"MIT license\\\",\\\"language\\\":\\\"TypeScript\\\",\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-02-03 22:39:20\\\"}\",\"user_uuid\":null,\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":null,\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":6121,\"uuid\":\"6bb42915-8ae7-41ad-afa4-6f61f6574273\",\"name\":\"nerve\",\"title\":\"Nerve\",\"description\":\"The Simple Agent Development Kit.\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/86922?v=4\",\"created_at\":\"$D2025-04-03T11:58:34.755Z\",\"updated_at\":\"$D2025-04-03T11:58:43.607Z\",\"status\":\"created\",\"author_name\":\"evilsocket\",\"author_avatar_url\":\"https://avatars.githubusercontent.com/u/86922?v=4\",\"tags\":\"ai,mcp,agent-based-modeling,agents,adk,llm,model-context-protocol,mcp-client,agent-development-kit\",\"category\":\"developer-tools\",\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/evilsocket/nerve\",\"target\":\"_self\",\"content\":\"$4a\",\"summary\":\"$4b\",\"img_url\":\"https://camo.githubusercontent.com/81bf64618acd68acb6a107ab472995890348609a6675172834a9aae1b097e175/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63756d656e746174696f6e2d626c7565\",\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"1062\\\",\\\"license\\\":\\\"View license\\\",\\\"language\\\":\\\"Python\\\",\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-05-30 17:35:57\\\"}\",\"user_uuid\":\"5dbc7931-a7cd-4aa8-a75b-cabddab7daa8\",\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":\"\",\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":3466,\"uuid\":\"3ff1606c-d034-4b02-8997-4742223cc635\",\"name\":\"mcp-client-cli\",\"title\":\"MCP CLI client\",\"description\":\"A simple CLI to run LLM prompt and implement MCP client.\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/153226281?v=4\",\"created_at\":\"$D2025-03-19T17:12:40.745Z\",\"updated_at\":\"$D2025-03-19T17:17:53.206Z\",\"status\":\"created\",\"author_name\":\"williamvd4\",\"author_avatar_url\":\"https://avatars.githubusercontent.com/u/153226281?v=4\",\"tags\":\"[]\",\"category\":\"developer-tools\",\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/williamvd4/mcp-client-cli\",\"target\":\"_self\",\"content\":\"$4c\",\"summary\":\"$4d\",\"img_url\":\"https://raw.githubusercontent.com/adhikasp/mcp-client-cli/refs/heads/master/c4_diagram.png\",\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"1\\\",\\\"license\\\":\\\"MIT license\\\",\\\"language\\\":\\\"\\\\n          \\\",\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-03-03 20:58:10\\\"}\",\"user_uuid\":null,\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":null,\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":62,\"uuid\":\"04a5a2fb-e5b4-48f8-ad91-ae12e00e5fbb\",\"name\":\"playwright-mcp\",\"title\":\"PLAYWRIGHT-MCP\",\"description\":\"This MCP Server will help you run browser automation and webscraping using Playwright\",\"avatar_url\":\"https://playwright.dev/img/playwright-logo.svg\",\"created_at\":\"$D2024-12-05T16:27:19.593Z\",\"updated_at\":\"$D2025-10-29T06:24:49.645Z\",\"status\":\"created\",\"author_name\":\"\",\"author_avatar_url\":\"\",\"tags\":\"playwright,browser-automation,webscraping\",\"category\":null,\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/microsoft/playwright-mcp\",\"target\":\"_self\",\"content\":\"$4e\",\"summary\":\"$4f\",\"img_url\":null,\"type\":\"client\",\"metadata\":null,\"user_uuid\":\"4"])</script><script>self.__next_f.push([1,"49fc11c-764c-4de2-980c-87eaef6e9481\",\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":\"npx @playwright/mcp@latest\",\"server_params\":\"[object Object]\",\"server_config\":\"\",\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":7854,\"uuid\":\"142054c8-d0df-4081-8315-c7983170b8ce\",\"name\":\"ollama-mcp-client\",\"title\":\"Ollama MCP (Model Context Protocol)\",\"description\":\"MCP client for local ollama models\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/77615094?v=4\",\"created_at\":\"$D2025-04-10T13:58:12.889Z\",\"updated_at\":\"$D2025-04-10T14:00:55.358Z\",\"status\":\"created\",\"author_name\":\"mihirrd\",\"author_avatar_url\":\"https://avatars.githubusercontent.com/u/77615094?v=4\",\"tags\":\"[]\",\"category\":\"developer-tools\",\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/mihirrd/ollama-mcp-client\",\"target\":\"_self\",\"content\":\"$50\",\"summary\":\"$51\",\"img_url\":null,\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"45\\\",\\\"license\\\":\\\"MIT license\\\",\\\"language\\\":\\\"Python\\\",\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-06-03 18:16:42\\\"}\",\"user_uuid\":null,\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":null,\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":13894,\"uuid\":\"2df71351-d93e-4268-ae50-c9a34c56f750\",\"name\":\"DrssionPageMCP\",\"title\":\"DrissionPage MCP Server -- 骚神出品\",\"description\":\"基于DrissionPage和FastMCP的浏览器自动化MCP服务器，提供丰富的浏览器操作API供AI调用\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/21326329?v=4\",\"created_at\":\"$D2025-06-01T02:04:46.939Z\",\"updated_at\":\"$D2025-06-01T03:53:01.058Z\",\"status\":\"created\",\"author_name\":\"wxhzhwxhzh\",\"author_avatar_url\":\"https://avatars.githubusercontent.com/u/21326329?v=4\",\"tags\":\"drissionpage,mcp-server\",\"category\":\"browser-automation\",\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/wxhzhwxhzh/drssionpagemcp\",\"target\":\"_self\",\"content\":\"$52\",\"summary\":\"$53\",\"img_url\":\"https://github.com/wxhzhwxhzh/DrssionPageMCP/raw/master/img/DrissionPageMCP-logo.png\",\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"17\\\",\\\"license\\\":\\\"\\\",\\\"language\\\":\\\"Python\\\",\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-05-27 15:18:59\\\"}\",\"user_uuid\":null,\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":null,\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":13914,\"uuid\":\"53206a60-23f6-4e64-b98f-6d4da40f1cd6\",\"name\":\"tinyagents\",\"title\":\"tinyagents\",\"description\":\"Tiny Agents: LLM + MCP Tools\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/8515462?v=4\",\"created_at\":\"$D2025-06-01T02:33:09.896Z\",\"updated_at\":\"$D2025-06-01T02:33:31.132Z\",\"status\":\"created\",\"author_name\":\"albertvillanova\",\"author_avatar_url\":\"https://avatars.githubusercontent.com/u/8515462?v=4\",\"tags\":\"[]\",\"category\":\"research-and-data\",\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/albertvillanova/tinyagents\",\"target\":\"_self\",\"content\":\"$54\",\"summary\":\"$55\",\"img_url\":null,\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"50\\\",\\\"license\\\":\\\"Apache-2.0 license\\\",\\\"language\\\":\\\"Python\\\",\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-05-19 22:16:13\\\"}\",\"user_uuid\":null,\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":null,\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":3352,\"uuid\":\"4bd43778-7b25-4ab4-9616-320e114dfbbf\",\"name\":\"n8n-coolify-mcp-tools\",\"title\":\"Coolify MCP Workflow\",\"description\":\"This workflow leverages the Community n8n MCP Client and my new Coolify MCP Server to interact with your Coolify infrastructure using MCP (Model Context Protocol).\",\"avatar_url\":\"https://avatars.githubusercontent"])</script><script>self.__next_f.push([1,".com/u/24812768?v=4\",\"created_at\":\"$D2025-03-19T02:37:57.216Z\",\"updated_at\":\"$D2025-03-19T02:41:35.048Z\",\"status\":\"created\",\"author_name\":\"wrediam\",\"author_avatar_url\":\"https://avatars.githubusercontent.com/u/24812768?v=4\",\"tags\":\"[]\",\"category\":\"developer-tools\",\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/wrediam/n8n-coolify-mcp-tools\",\"target\":\"_self\",\"content\":\"$56\",\"summary\":\"$57\",\"img_url\":null,\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"9\\\",\\\"license\\\":\\\"MIT license\\\",\\\"language\\\":null,\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-03-18 20:55:49\\\"}\",\"user_uuid\":null,\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":null,\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":13893,\"uuid\":\"bc4e35d5-ca2d-44b8-92d9-609dd4d0d4d8\",\"name\":\"proteus-ai\",\"title\":\"Proteus Workflow Engine\",\"description\":\"一个强大的、可扩展的多智能体工作流引擎，支持Multi-Agent系统、auto-workflow、MCP-SERVER接入等功能，支持多种工具和资源，提供智能代理和自动化服务执行。\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/3463296?v=4\",\"created_at\":\"$D2025-06-01T02:02:46.355Z\",\"updated_at\":\"$D2025-06-01T02:34:22.436Z\",\"status\":\"created\",\"author_name\":\"alishangtian\",\"author_avatar_url\":\"https://avatars.githubusercontent.com/u/3463296?v=4\",\"tags\":\"agent,workflow,multi-agent,llm\",\"category\":\"research-and-data\",\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/alishangtian/proteus-ai\",\"target\":\"_self\",\"content\":\"$58\",\"summary\":\"$59\",\"img_url\":null,\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"2\\\",\\\"license\\\":\\\"MIT license\\\",\\\"language\\\":\\\"Python\\\",\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-06-02 16:40:48\\\"}\",\"user_uuid\":null,\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":null,\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":2736,\"uuid\":\"f0cbfd22-4251-4565-ae18-ef095aeb410e\",\"name\":\"mattermost-mcp-host\",\"title\":\"Mattermost MCP Host\",\"description\":\"A Mattermost integration that connects to Model Context Protocol (MCP) servers, leveraging a LangGraph-based Agent.\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/30863630?v=4\",\"created_at\":\"$D2025-03-14T00:47:18.733Z\",\"updated_at\":\"$D2025-03-14T02:03:15.253Z\",\"status\":\"created\",\"author_name\":\"jagan-shanmugam\",\"author_avatar_url\":\"https://avatars.githubusercontent.com/u/30863630?v=4\",\"tags\":\"mcp,mattermost,llm,langgraph,mcp-clients\",\"category\":\"developer-tools\",\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/jagan-shanmugam/mattermost-mcp-host\",\"target\":\"_self\",\"content\":\"$5a\",\"summary\":\"$5b\",\"img_url\":\"https://camo.githubusercontent.com/ac892e3817eb6a9c9d3342be0cde3db8be518803e934d938046faece231ece53/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f76657273696f6e2d302e312e302d626c7565\",\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"16\\\",\\\"license\\\":\\\"MIT license\\\",\\\"language\\\":\\\"Python\\\",\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-04-07 21:56:47\\\"}\",\"user_uuid\":null,\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":null,\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":5127,\"uuid\":\"6c81ff9c-989d-4acc-bed6-f3fcef2e85cc\",\"name\":\"codemcp\",\"title\":\"codemcp\",\"description\":\"Coding assistant MCP for Claude Desktop\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/13564?v=4\",\"created_at\":\"$D2025-04-01T13:43:23.267Z\",\"updated_at\":\"$D2025-04-01T14:03:47.817Z\",\"status\":\"created\",\"author_name\":\"ezyang\",\"author_avatar_url\":\"https://avatars.githubusercontent.com/u/13564?v=4\",\"tags\":\"[]\",\"category\":\"developer-tools\",\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/ezyang/codemcp\",\"target\":\"_self\",\"content\":\"$5c\",\"summary\":\"$"])</script><script>self.__next_f.push([1,"5d\",\"img_url\":\"https://github.com/ezyang/codemcp/raw/main/static/screenshot.png?raw=true\",\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"1386\\\",\\\"license\\\":\\\"Apache-2.0 license\\\",\\\"language\\\":\\\"Python\\\",\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-06-03 21:37:30\\\"}\",\"user_uuid\":null,\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":null,\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":4428,\"uuid\":\"bd97f602-3ae1-4baa-9d3b-51e44639c393\",\"name\":\"Axiom\",\"title\":\"Axiom - A Docs Expert Agent\",\"description\":\"A documentation AI Agent built with LangGraph, MCP Docs, and Chainlit, designed to help users create different projects using natural language.\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/146746287?v=4\",\"created_at\":\"$D2025-03-24T17:06:37.788Z\",\"updated_at\":\"$D2025-03-24T17:32:29.176Z\",\"status\":\"created\",\"author_name\":\"aasherkamal216\",\"author_avatar_url\":\"https://avatars.githubusercontent.com/u/146746287?v=4\",\"tags\":\"agent,docker,mcp,gemini,chainlit,openrouter,deepseek,langgraph\",\"category\":\"research-and-data\",\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/aasherkamal216/axiom\",\"target\":\"_self\",\"content\":\"---\\ntitle: Axiom Agent\\nemoji: 🏢\\ncolorFrom: blue\\ncolorTo: green\\nsdk: docker\\npinned: false\\n---\\n\\nCheck out the configuration reference at https://huggingface.co/docs/hub/spaces-config-reference\",\"summary\":\"$5e\",\"img_url\":\"https://github.com/aasherkamal216/Axiom/raw/main/public/axiom.png\",\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"5\\\",\\\"license\\\":\\\"MIT license\\\",\\\"language\\\":\\\"Python\\\",\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-03-26 15:21:12\\\"}\",\"user_uuid\":null,\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":null,\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":13899,\"uuid\":\"86c5e90c-2164-44bb-9251-5c91006931d0\",\"name\":\"chat\",\"title\":\"Anirudh's Chat\",\"description\":\"A simple NextJS MCP client with sensible keybindings\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/7597423?v=4\",\"created_at\":\"$D2025-06-01T02:09:21.769Z\",\"updated_at\":\"$D2025-06-02T14:16:43.296Z\",\"status\":\"created\",\"author_name\":\"kamath\",\"author_avatar_url\":\"https://avatars.githubusercontent.com/u/7597423?v=4\",\"tags\":\"[]\",\"category\":\"developer-tools\",\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/kamath/chat\",\"target\":\"_self\",\"content\":\"$5f\",\"summary\":\"$60\",\"img_url\":\"https://github.com/kamath/chat/raw/main/public/demo.png\",\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"28\\\",\\\"license\\\":\\\"\\\",\\\"language\\\":\\\"TypeScript\\\",\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-05-16 09:55:27\\\"}\",\"user_uuid\":null,\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":null,\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":13895,\"uuid\":\"b845f67c-9825-4022-90b1-516c0736ca97\",\"name\":\"LLM-Amap-mcp\",\"title\":\"高德地址解析服务\",\"description\":\"大模型搭配高德mcp的地址服务\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/25945597?v=4\",\"created_at\":\"$D2025-06-01T02:06:49.691Z\",\"updated_at\":\"$D2025-06-02T00:13:37.479Z\",\"status\":\"created\",\"author_name\":\"blacsheep\",\"author_avatar_url\":\"https://avatars.githubusercontent.com/u/25945597?v=4\",\"tags\":\"[]\",\"category\":\"location-services\",\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/blacsheep/llm-amap-mcp\",\"target\":\"_self\",\"content\":\"$61\",\"summary\":\"$62\",\"img_url\":null,\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"2\\\",\\\"license\\\":\\\"\\\",\\\"language\\\":\\\"Python\\\",\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-05-28 16:15:01\\\"}\",\"user_uuid\":null,\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_confi"])</script><script>self.__next_f.push([1,"g\":null,\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":14894,\"uuid\":\"42c678c8-703e-45b3-86a4-82619b26676d\",\"name\":\"install-mcp\",\"title\":\"Install MCP CLI\",\"description\":\"A simple CLI to install MCP servers into any client\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/171979587?v=4\",\"created_at\":\"$D2025-06-02T17:10:34.276Z\",\"updated_at\":\"$D2025-06-02T17:32:06.185Z\",\"status\":\"created\",\"author_name\":\"supermemoryai\",\"author_avatar_url\":\"https://avatars.githubusercontent.com/u/171979587?v=4\",\"tags\":\"[]\",\"category\":\"developer-tools\",\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/supermemoryai/install-mcp\",\"target\":\"_self\",\"content\":\"# Install MCP CLI\\n\\n### A CLI tool to install and manage MCP servers.\\n\\nInstalling MCPs is a huge pain, so I made a CLI tool to make it easier.\\n\\n## Usage\\n\\nJust run\\n`npx install-mcp '\u003ccommand\u003e' --client \u003cclient\u003e`\\n\\nAlso works with SSE URLs\\n`npx install-mcp '\u003curl\u003e' --client \u003cclient\u003e`\\n\\nwhere `\u003cclient\u003e` is one of the following:\\n\\n- `claude`\\n- `cline`\\n- `roo-cline`\\n- `windsurf`\\n- `witsy`\\n- `enconvo`\\n- `cursor`\\n\\n## License\\n\\nMIT\",\"summary\":\"$63\",\"img_url\":null,\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"55\\\",\\\"license\\\":\\\"MIT license\\\",\\\"language\\\":\\\"TypeScript\\\",\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-06-05 14:10:05\\\"}\",\"user_uuid\":null,\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":null,\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":10088,\"uuid\":\"986ff7c1-bc90-451e-ad19-d2e4fc7b08d4\",\"name\":\"airbnb_mcp\",\"title\":\"airbnb_mcp\",\"description\":\"airbnb_mcp\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/17763961?v=4\",\"created_at\":\"$D2025-04-21T17:17:29.005Z\",\"updated_at\":\"$D2025-04-21T17:21:10.226Z\",\"status\":\"created\",\"author_name\":\"Sakil786\",\"author_avatar_url\":\"https://avatars.githubusercontent.com/u/17763961?v=4\",\"tags\":\"[]\",\"category\":\"research-and-data\",\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/sakil786/airbnb_mcp\",\"target\":\"_self\",\"content\":\"# airbnb_mcp\\nairbnb_mcp\",\"summary\":\"## what is airbnb_mcp? \\nairbnb_mcp is a project aimed at providing a client-side solution for interacting with Airbnb's services.\\n\\n## how to use airbnb_mcp? \\nTo use airbnb_mcp, clone the repository from GitHub and follow the setup instructions provided in the README file.\\n\\n## key features of airbnb_mcp? \\n- Client-side implementation for Airbnb services\\n- Easy integration with existing applications\\n- Open-source under GPL-3.0 license\\n\\n## use cases of airbnb_mcp? \\n1. Building applications that require Airbnb data access.\\n2. Creating custom tools for managing Airbnb listings.\\n3. Integrating Airbnb functionalities into other platforms.\\n\\n## FAQ from airbnb_mcp? \\n- Is airbnb_mcp free to use?  \\n\u003e Yes! airbnb_mcp is open-source and free to use under the GPL-3.0 license.\\n\\n- What programming language is used in airbnb_mcp?  \\n\u003e The project is developed in Python.\\n\\n- How can I contribute to airbnb_mcp?  \\n\u003e You can contribute by submitting issues or pull requests on the GitHub repository.\",\"img_url\":null,\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"2\\\",\\\"license\\\":\\\"GPL-3.0 license\\\",\\\"language\\\":\\\"Python\\\",\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-04-20 20:28:00\\\"}\",\"user_uuid\":null,\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":null,\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":1579,\"uuid\":\"6bc81040-dfe7-4e40-a8a6-a4607b094a52\",\"name\":\"oneshot\",\"title\":\"oneshot\",\"description\":\"Anthropic MCP client for macOS\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/4247901?v=4\",\"created_at\":\"$D2025-02-23T09:39:31.452Z\",\"updated_at\":\"$D2025-02-23T09:48:49.211Z\",\"status\":\"created\",\"author_name\":\"Destiner\",\"author_avatar_url\":\"https://avatars.gith"])</script><script>self.__next_f.push([1,"ubusercontent.com/u/4247901?v=4\",\"tags\":\"[]\",\"category\":\"developer-tools\",\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/destiner/oneshot\",\"target\":\"_self\",\"content\":\"# oneshot\\n\\nAnthropic's MCP client for macOS.\\n\\n\u003e [!WARNING]\\n\u003e This is an early prototype. Expect the chat history and the tool configuration to be lost with future updates.\\n\\n\u003cimg width=\\\"990\\\" alt=\\\"Screenshot 2025-01-05 at 15 01 14\\\" src=\\\"https://github.com/user-attachments/assets/299840b7-e3c5-484d-8178-c13889a01df8\\\" /\u003e\\n\\n\\n## Features\\n\\n- Bring your own API key\\n- Built-in tool discovery\\n- One-click tool installation\\n\\n## Set up\\n\\nTo install dependencies:\\n\\n```bash\\nbun install\\n```\\n\\n\u003e [!NOTE]\\n\u003e You also need `bun` and Python's `uv` installed locally. Later versions of the app will bundle those dependencies automatically.\\n\\nTo run:\\n\\n1. Run the API server:\\n\\n```bash\\nbun run server:dev\\n```\\n\\n2. Run the UI:\\n\\n```bash\\nbun run app:dev\\n```\",\"summary\":\"$64\",\"img_url\":\"https://private-user-images.githubusercontent.com/4247901/400208865-299840b7-e3c5-484d-8178-c13889a01df8.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDkwNjA0NjAsIm5iZiI6MTc0OTA2MDE2MCwicGF0aCI6Ii80MjQ3OTAxLzQwMDIwODg2NS0yOTk4NDBiNy1lM2M1LTQ4NGQtODE3OC1jMTM4ODlhMDFkZjgucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDYwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA2MDRUMTgwMjQwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YzY3ZWE5ODVlYjFiZGFiZDYxYTcyNWZjNTY5NDk4OWU0ZjIwNmE4OTRiNzJmM2QyMjRhMThkNTBjYzM1MDMxNyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.ahnCA-mgWlMlQf0V66WgDXNEsyLv2lZxiAnC6-3gWYw\",\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"13\\\",\\\"license\\\":\\\"\\\",\\\"language\\\":\\\"Vue\\\",\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-01-05 17:42:27\\\"}\",\"user_uuid\":null,\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":null,\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false},{\"id\":169,\"uuid\":\"b15b563b-1d82-41a5-9e7b-bb7f1ddda4ac\",\"name\":\"mcphost\",\"title\":\"MCPHost 🤖\",\"description\":\"A CLI host application that enables Large Language Models (LLMs) to interact with external tools through the Model Context Protocol (MCP).\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/17607124?v=4\",\"created_at\":\"$D2024-12-13T09:12:13.176Z\",\"updated_at\":\"$D2024-12-13T12:28:12.786Z\",\"status\":\"created\",\"author_name\":\"mark3labs\",\"author_avatar_url\":\"https://avatars.githubusercontent.com/u/17607124?v=4\",\"tags\":\"[]\",\"category\":\"developer-tools\",\"is_featured\":false,\"sort\":1,\"url\":\"https://github.com/mark3labs/mcphost\",\"target\":\"_self\",\"content\":\"$65\",\"summary\":\"$66\",\"img_url\":null,\"type\":\"client\",\"metadata\":\"{\\\"star\\\":\\\"974\\\",\\\"license\\\":\\\"MIT license\\\",\\\"language\\\":\\\"Go\\\",\\\"is_official\\\":false,\\\"latest_commit_time\\\":\\\"2025-05-13 23:22:59\\\"}\",\"user_uuid\":null,\"tools\":null,\"sse_url\":null,\"sse_provider\":null,\"sse_params\":null,\"is_official\":false,\"server_command\":null,\"server_params\":null,\"server_config\":null,\"allow_call\":false,\"is_innovation\":false,\"is_dxt\":false,\"dxt_manifest\":null,\"dxt_file_url\":null,\"is_audit\":false}]}],[\"$\",\"$L67\",null,{\"totalPages\":8,\"itemsPerPage\":60,\"currentPage\":5}]]\n"])</script></body></html>